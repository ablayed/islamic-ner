"""Gold test-set preparation and validation workflow.

This utility helps convert a noisy held-out silver split into a manually
corrected gold test split:

1) prepare: sample records and create editable CSV sheets.
2) build: rebuild gold_test.json from edited CSV labels.
3) validate: run schema + BIO checks on the gold file.

Examples:
    python scripts/gold_annotation.py prepare
    python scripts/gold_annotation.py build
    python scripts/gold_annotation.py validate
"""

from __future__ import annotations

import argparse
import csv
import json
import random
import sys
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Sequence


PROJECT_ROOT = Path(__file__).resolve().parents[1]

EXPECTED_ENTITY_TYPES = ["SCHOLAR", "BOOK", "CONCEPT", "PLACE", "HADITH_REF"]
TRUTHY_VALUES = {"1", "true", "yes", "y", "done"}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Gold annotation workflow tools.")
    subparsers = parser.add_subparsers(dest="command", required=True)

    prepare = subparsers.add_parser("prepare", help="Sample held-out silver data for manual gold annotation.")
    prepare.add_argument(
        "--input-path",
        type=Path,
        default=PROJECT_ROOT / "data" / "silver" / "test_held_out.json",
        help="Input held-out silver JSON path.",
    )
    prepare.add_argument(
        "--sample-size",
        type=int,
        default=200,
        help="Number of records to sample.",
    )
    prepare.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for deterministic sampling.",
    )
    prepare.add_argument(
        "--output-json",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test.json",
        help="Output path for sampled gold JSON template.",
    )
    prepare.add_argument(
        "--tokens-csv",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test_tokens.csv",
        help="Token-level CSV sheet to edit gold tags quickly.",
    )
    prepare.add_argument(
        "--meta-csv",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test_sentence_meta.csv",
        help="Sentence-level metadata sheet (confidence / notes / reviewed).",
    )
    prepare.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing output files.",
    )

    build = subparsers.add_parser("build", help="Build final gold JSON from edited CSV files.")
    build.add_argument(
        "--template-json",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test.json",
        help="Template JSON generated by prepare.",
    )
    build.add_argument(
        "--tokens-csv",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test_tokens.csv",
        help="Edited token-level CSV containing gold_tag column.",
    )
    build.add_argument(
        "--meta-csv",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test_sentence_meta.csv",
        help="Optional sentence-level annotation metadata CSV.",
    )
    build.add_argument(
        "--output-json",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test.json",
        help="Output path for rebuilt gold JSON.",
    )
    build.add_argument(
        "--output-meta-json",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test_annotation_meta.json",
        help="Output path for annotation metadata JSON.",
    )
    build.add_argument(
        "--strict-reviewed",
        action="store_true",
        help="Fail if any sentence has reviewed flag != true-ish in meta CSV.",
    )
    build.add_argument(
        "--allow-empty-gold-tag",
        action="store_true",
        help="Allow empty gold_tag and fall back to silver_tag for those rows.",
    )

    validate = subparsers.add_parser("validate", help="Validate gold JSON format and BIO consistency.")
    validate.add_argument(
        "--input-json",
        type=Path,
        default=PROJECT_ROOT / "data" / "gold" / "gold_test.json",
        help="Gold JSON to validate.",
    )

    return parser.parse_args()


def read_json_records(path: Path) -> List[Dict]:
    if not path.exists():
        raise FileNotFoundError(f"JSON file not found: {path}")
    payload = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(payload, list):
        raise ValueError(f"Expected top-level JSON list in {path}")
    return payload


def write_json(path: Path, payload: object) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def is_valid_bio_tag(tag: str) -> bool:
    if tag == "O":
        return True
    if "-" not in tag:
        return False
    prefix, entity_type = tag.split("-", 1)
    return prefix in {"B", "I"} and entity_type in EXPECTED_ENTITY_TYPES


def validate_bio_sequence(tags: Sequence[str]) -> List[str]:
    errors: List[str] = []
    prev_type = ""
    prev_prefix = "O"
    for idx, tag in enumerate(tags):
        if not is_valid_bio_tag(tag):
            errors.append(f"invalid tag '{tag}' at token_index={idx}")
            prev_type = ""
            prev_prefix = "O"
            continue

        if tag == "O":
            prev_type = ""
            prev_prefix = "O"
            continue

        prefix, entity_type = tag.split("-", 1)
        if prefix == "I":
            if prev_prefix not in {"B", "I"} or prev_type != entity_type:
                errors.append(
                    f"invalid I- continuation '{tag}' at token_index={idx}; "
                    f"previous tag was '{tags[idx - 1] if idx > 0 else 'START'}'"
                )
        prev_type = entity_type
        prev_prefix = prefix

    return errors


def validate_record_shape(record: Dict, record_index: int) -> List[str]:
    errors: List[str] = []
    tokens = record.get("tokens")
    tags = record.get("ner_tags")
    if not isinstance(tokens, list):
        errors.append(f"record[{record_index}] missing/invalid 'tokens' list")
        return errors
    if not isinstance(tags, list):
        errors.append(f"record[{record_index}] missing/invalid 'ner_tags' list")
        return errors
    if len(tokens) != len(tags):
        errors.append(
            f"record[{record_index}] token/tag length mismatch: "
            f"{len(tokens)} tokens vs {len(tags)} tags"
        )
        return errors
    tag_errors = validate_bio_sequence([str(tag) for tag in tags])
    for err in tag_errors:
        errors.append(f"record[{record_index}] {err}")
    return errors


def ensure_writable(paths: Sequence[Path], force: bool) -> None:
    if force:
        return
    existing = [str(path) for path in paths if path.exists()]
    if existing:
        raise FileExistsError(
            "Refusing to overwrite existing files without --force:\n"
            + "\n".join(f"- {path}" for path in existing)
        )


def parse_int(name: str, raw_value: str) -> int:
    value = str(raw_value).strip()
    try:
        return int(value)
    except ValueError as exc:
        raise ValueError(f"Invalid integer for '{name}': '{raw_value}'") from exc


def parse_boolish(raw_value: str) -> bool:
    return str(raw_value).strip().lower() in TRUTHY_VALUES


def collect_entity_counts(records: Sequence[Dict]) -> Dict[str, int]:
    counts = {entity_type: 0 for entity_type in EXPECTED_ENTITY_TYPES}
    for record in records:
        for label in record.get("ner_tags", []):
            if isinstance(label, str) and label.startswith("B-"):
                entity_type = label[2:]
                counts[entity_type] = counts.get(entity_type, 0) + 1
    return counts


def command_prepare(args: argparse.Namespace) -> None:
    records = read_json_records(args.input_path)
    shape_errors: List[str] = []
    for idx, record in enumerate(records):
        shape_errors.extend(validate_record_shape(record, idx))
    if shape_errors:
        preview = "\n".join(f"- {err}" for err in shape_errors[:20])
        raise ValueError(
            f"Input file has {len(shape_errors)} validation error(s). First errors:\n{preview}"
        )

    sample_size = int(args.sample_size)
    if sample_size <= 0:
        raise ValueError("--sample-size must be > 0")
    if sample_size > len(records):
        raise ValueError(
            f"--sample-size ({sample_size}) exceeds held-out size ({len(records)})."
        )

    ensure_writable([args.output_json, args.tokens_csv, args.meta_csv], force=args.force)
    rng = random.Random(args.seed)
    sampled_records = [records[idx] for idx in rng.sample(range(len(records)), sample_size)]

    args.output_json.parent.mkdir(parents=True, exist_ok=True)
    write_json(args.output_json, sampled_records)

    token_fields = [
        "sample_index",
        "id",
        "token_index",
        "token",
        "silver_tag",
        "gold_tag",
        "entity_hint",
        "source",
        "book",
    ]
    with args.tokens_csv.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=token_fields)
        writer.writeheader()
        for sample_idx, record in enumerate(sampled_records):
            rec_id = str(record.get("id", f"sample_{sample_idx}"))
            source = str(record.get("source", ""))
            book = str(record.get("book", ""))
            tokens = list(record["tokens"])
            tags = list(record["ner_tags"])
            for token_idx, (token, silver_tag) in enumerate(zip(tokens, tags)):
                entity_hint = "O"
                if isinstance(silver_tag, str) and "-" in silver_tag:
                    _, entity_hint = silver_tag.split("-", 1)
                writer.writerow(
                    {
                        "sample_index": sample_idx,
                        "id": rec_id,
                        "token_index": token_idx,
                        "token": token,
                        "silver_tag": silver_tag,
                        "gold_tag": silver_tag,
                        "entity_hint": entity_hint,
                        "source": source,
                        "book": book,
                    }
                )

    meta_fields = ["sample_index", "id", "source", "book", "confidence", "note", "reviewed"]
    with args.meta_csv.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=meta_fields)
        writer.writeheader()
        for sample_idx, record in enumerate(sampled_records):
            writer.writerow(
                {
                    "sample_index": sample_idx,
                    "id": str(record.get("id", f"sample_{sample_idx}")),
                    "source": str(record.get("source", "")),
                    "book": str(record.get("book", "")),
                    "confidence": "1.0",
                    "note": "",
                    "reviewed": "0",
                }
            )

    print(f"Prepared {len(sampled_records)} records from {args.input_path}")
    print(f"- Gold template JSON: {args.output_json}")
    print(f"- Token annotation sheet: {args.tokens_csv}")
    print(f"- Sentence meta sheet: {args.meta_csv}")
    print("Next: edit 'gold_tag' in token sheet, then run: python scripts/gold_annotation.py build")


def command_build(args: argparse.Namespace) -> None:
    template_records = read_json_records(args.template_json)
    template_errors: List[str] = []
    for idx, record in enumerate(template_records):
        template_errors.extend(validate_record_shape(record, idx))
    if template_errors:
        preview = "\n".join(f"- {err}" for err in template_errors[:20])
        raise ValueError(
            f"Template JSON has {len(template_errors)} validation error(s). First errors:\n{preview}"
        )

    if not args.tokens_csv.exists():
        raise FileNotFoundError(f"Token CSV not found: {args.tokens_csv}")

    with args.tokens_csv.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        required_columns = {"sample_index", "id", "token_index", "token", "silver_tag", "gold_tag"}
        if reader.fieldnames is None or not required_columns.issubset(set(reader.fieldnames)):
            raise ValueError(
                "Token CSV is missing required columns. Required: "
                + ", ".join(sorted(required_columns))
            )

        rows_by_key: Dict[tuple[int, str], Dict[int, Dict[str, str]]] = defaultdict(dict)
        for row_idx, row in enumerate(reader):
            rec_id = str(row.get("id", "")).strip()
            if not rec_id:
                raise ValueError(f"Token CSV row {row_idx + 2} missing id.")
            sample_index = parse_int("sample_index", row.get("sample_index", ""))
            if sample_index < 0:
                raise ValueError(f"Token CSV row {row_idx + 2} has negative sample_index.")
            token_index = parse_int("token_index", row.get("token_index", ""))
            if token_index in rows_by_key[(sample_index, rec_id)]:
                raise ValueError(
                    f"Duplicate token row for sample_index={sample_index}, "
                    f"id='{rec_id}', token_index={token_index} in CSV."
                )
            rows_by_key[(sample_index, rec_id)][token_index] = row

    template_key_set: set[tuple[int, str]] = set()
    rebuilt_records: List[Dict] = []
    record_errors: List[str] = []
    changed_token_count = 0
    changed_sentence_count = 0

    for record_idx, record in enumerate(template_records):
        rec_id = str(record.get("id", f"sample_{record_idx}"))
        record_key = (record_idx, rec_id)
        template_key_set.add(record_key)
        rows_for_id = rows_by_key.get(record_key)
        if rows_for_id is None:
            record_errors.append(
                f"sample_index={record_idx}, id='{rec_id}': no rows found in token CSV"
            )
            continue

        tokens = list(record["tokens"])
        silver_tags = [str(tag) for tag in record["ner_tags"]]
        if len(rows_for_id) != len(tokens):
            record_errors.append(
                f"sample_index={record_idx}, id='{rec_id}': token count mismatch; "
                f"template={len(tokens)} csv={len(rows_for_id)}"
            )
            continue

        gold_tags: List[str] = []
        local_changes = 0
        for token_index, template_token in enumerate(tokens):
            row = rows_for_id.get(token_index)
            if row is None:
                record_errors.append(
                    f"sample_index={record_idx}, id='{rec_id}': missing token_index={token_index}"
                )
                continue

            csv_token = str(row.get("token", ""))
            if csv_token != template_token:
                record_errors.append(
                    f"sample_index={record_idx}, id='{rec_id}' token_index={token_index}: token mismatch "
                    f"(template='{template_token}' csv='{csv_token}')"
                )
                continue

            silver_tag = str(row.get("silver_tag", "")).strip()
            gold_tag = str(row.get("gold_tag", "")).strip()
            if not gold_tag:
                if args.allow_empty_gold_tag:
                    gold_tag = silver_tag
                else:
                    record_errors.append(
                        f"sample_index={record_idx}, id='{rec_id}' token_index={token_index}: empty gold_tag"
                    )
                    continue

            if not is_valid_bio_tag(gold_tag):
                record_errors.append(
                    f"sample_index={record_idx}, id='{rec_id}' token_index={token_index}: "
                    f"invalid gold_tag '{gold_tag}'"
                )
                continue

            gold_tags.append(gold_tag)
            if gold_tag != silver_tag:
                local_changes += 1

        if len(gold_tags) != len(tokens):
            continue

        bio_errors = validate_bio_sequence(gold_tags)
        for bio_err in bio_errors:
            record_errors.append(f"sample_index={record_idx}, id='{rec_id}': {bio_err}")
        if bio_errors:
            continue

        updated = dict(record)
        updated["ner_tags"] = gold_tags
        rebuilt_records.append(updated)

        if local_changes > 0:
            changed_sentence_count += 1
            changed_token_count += local_changes

    extra_keys = sorted(set(rows_by_key) - template_key_set)
    if extra_keys:
        preview_keys = [f"(sample_index={idx}, id='{rec_id}')" for idx, rec_id in extra_keys[:5]]
        record_errors.append(
            f"Token CSV has {len(extra_keys)} record key(s) not found in template JSON "
            f"(examples: {preview_keys})"
        )

    if record_errors:
        preview = "\n".join(f"- {err}" for err in record_errors[:40])
        raise ValueError(f"Build failed with {len(record_errors)} error(s):\n{preview}")

    write_json(args.output_json, rebuilt_records)

    if args.meta_csv.exists():
        with args.meta_csv.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.DictReader(handle)
            required_meta_cols = {"id", "confidence", "note", "reviewed"}
            if reader.fieldnames is None or not required_meta_cols.issubset(set(reader.fieldnames)):
                raise ValueError(
                    "Meta CSV is missing required columns. Required: "
                    + ", ".join(sorted(required_meta_cols))
                )
            meta_rows = list(reader)

        meta_by_key: Dict[tuple[int, str], Dict[str, object]] = {}
        for row in meta_rows:
            rec_id = str(row.get("id", "")).strip()
            sample_index = parse_int("sample_index", row.get("sample_index", ""))
            if not rec_id:
                continue
            confidence_raw = str(row.get("confidence", "")).strip()
            confidence = None
            if confidence_raw:
                try:
                    confidence = float(confidence_raw)
                except ValueError as exc:
                    raise ValueError(
                        f"Invalid confidence value '{confidence_raw}' for id='{rec_id}' in meta CSV."
                    ) from exc
            meta_by_key[(sample_index, rec_id)] = {
                "id": rec_id,
                "sample_index": sample_index,
                "confidence": confidence,
                "note": str(row.get("note", "")).strip(),
                "reviewed": parse_boolish(str(row.get("reviewed", ""))),
            }

        meta_payload: List[Dict[str, object]] = []
        unreviewed_ids: List[str] = []
        for idx, record in enumerate(rebuilt_records):
            rec_id = str(record.get("id", f"sample_{idx}"))
            entry = meta_by_key.get(
                (idx, rec_id),
                {
                    "id": rec_id,
                    "sample_index": idx,
                    "confidence": None,
                    "note": "",
                    "reviewed": False,
                },
            )
            if not bool(entry["reviewed"]):
                unreviewed_ids.append(f"{idx}:{rec_id}")
            meta_payload.append(entry)

        if args.strict_reviewed and unreviewed_ids:
            raise ValueError(
                f"{len(unreviewed_ids)} sentence(s) are not marked reviewed in meta CSV "
                f"(examples: {unreviewed_ids[:5]})."
            )
        write_json(args.output_meta_json, meta_payload)

    entity_counts = collect_entity_counts(rebuilt_records)
    print(f"Built gold JSON: {args.output_json}")
    print(f"- Records: {len(rebuilt_records)}")
    print(f"- Sentences with any tag edits vs silver: {changed_sentence_count}")
    print(f"- Tokens edited vs silver: {changed_token_count}")
    print(f"- Entity counts (B-tags): {entity_counts}")
    if args.meta_csv.exists():
        print(f"- Annotation metadata JSON: {args.output_meta_json}")


def command_validate(args: argparse.Namespace) -> None:
    records = read_json_records(args.input_json)
    errors: List[str] = []
    total_tokens = 0
    for idx, record in enumerate(records):
        total_tokens += len(record.get("tokens", [])) if isinstance(record.get("tokens"), list) else 0
        errors.extend(validate_record_shape(record, idx))

    if errors:
        preview = "\n".join(f"- {err}" for err in errors[:40])
        raise ValueError(f"Validation failed with {len(errors)} error(s):\n{preview}")

    counts = collect_entity_counts(records)
    label_counter: Counter = Counter()
    for record in records:
        label_counter.update(str(label) for label in record.get("ner_tags", []))

    print(f"Validation passed: {args.input_json}")
    print(f"- Records: {len(records)}")
    print(f"- Tokens: {total_tokens}")
    print(f"- Entity counts (B-tags): {counts}")
    print(f"- Label distribution: {dict(sorted(label_counter.items()))}")


def main() -> int:
    args = parse_args()
    try:
        if args.command == "prepare":
            command_prepare(args)
        elif args.command == "build":
            command_build(args)
        elif args.command == "validate":
            command_validate(args)
        else:
            raise ValueError(f"Unknown command: {args.command}")
    except Exception as exc:  # pragma: no cover
        print(f"ERROR: {exc}", file=sys.stderr)
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
