Hallucinated entities are the dominant failure mode, accounting for 47.5% of all errors. These are false positives where non-entity text is labeled as an entity, most often SCHOLAR, and the confusion matrix shows this clearly with 110 O-tokens predicted as SCHOLAR. The root cause is dataset bias: silver training data is isnad-heavy, so the model overgeneralizes name-dense narration patterns into contexts where entities are actually sparse. A practical fix is to add more matn-focused negative examples, apply hard-negative mining for SCHOLAR/PLACE (training on cases the model is most likely to get wrong), and enforce a confidence threshold so low-confidence spans are suppressed at inference time.

Boundary errors are the second largest category at 31.2%, where the model detects an entity but chooses the wrong span limits. In practice this appears as over-extension (including honorifics or trailing words) and under-extension (cutting patronymic chains too early), such as predicting "محمد بن سهل بن ابي حثمه" when gold ends at "محمد بن سهل بن ابي". The root cause is Arabic name morphology and variable nasab length, especially when honorific phrases appear immediately after names, and this is a well-documented challenge in Arabic NER literature rather than a project-specific anomaly. This can be improved with targeted boundary supervision, post-processing rules that strip honorific tails, and additional gold examples of long multi-token scholar names.

Missed entities (false negatives) make up 20.6% of errors, where a gold entity is present but the model predicts O. These misses are concentrated in less frequent names such as "النعمان بن بشير" and "علي بن حسين," indicating weak recall on the long tail. The underlying issue is sparse exposure in silver data, consistent with the earlier finding that 70.3% of scholar names were singletons. To reduce this, expand rare-name coverage via lexicon-guided augmentation (e.g., inserting known rare names into sentence templates to generate synthetic training examples), add semantically diverse contexts, and include more manually corrected gold examples for low-frequency entities; this is the same core idea as the Phase 2 book-boost strategy applied to scholar recall.

Type confusion is minimal at 0.7%, with only one observed case where "بدرا" was confused between SCHOLAR and PLACE. In practical terms, this is just 1 confusion out of 141 total errors, which strongly validates the entity schema design choices made in Phase 1. The likely cause in that single error is local context ambiguity rather than systematic label collapse. A lightweight fix is to add a few targeted disambiguation examples and context rules for ambiguous historical terms that can denote person, place, or event.

Morphological errors were not observed (0.0%), so Arabic-specific tokenization/normalization appears to be working as intended on this gold set. In other words, agglutination and diacritic-variant handling did not emerge as active failure drivers in the current evaluation sample. This validates the design decision to normalize text before model input rather than relying on the model alone to learn diacritic invariance. The right action is to preserve the current normalization pipeline and add regression tests so this strength is maintained as data and models evolve.
