{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ef22d2",
   "metadata": {},
   "source": [
    "# 06 - CAMeLBERT-CA Ablation (Clean Rebuild)\n",
    "\n",
    "This notebook is a fresh, self-contained ablation run that mirrors notebook 05 training settings while swapping the backbone to `CAMeL-Lab/bert-base-arabic-camelbert-ca`.\n",
    "\n",
    "Goals:\n",
    "1. Train/evaluate CAMeLBERT-CA on the same silver train/dev splits.\n",
    "2. Compare against AraBERT (standard) and AraBERT (weighted).\n",
    "3. Export a report-ready comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611771d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import inspect\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59916e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: c:\\Users\\diaab\\islamic-ner\n",
      "Torch device: cpu\n",
      "CAMeLBERT model: CAMeL-Lab/bert-base-arabic-camelbert-ca\n"
     ]
    }
   ],
   "source": [
    "# --------- Paths and run configuration ---------\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"data\").exists() and (candidate / \"models\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SILVER_TRAIN_PATH = ROOT / \"data\" / \"silver\" / \"train.json\"\n",
    "SILVER_DEV_PATH = ROOT / \"data\" / \"silver\" / \"dev.json\"\n",
    "\n",
    "MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\"\n",
    "MAX_SEQ_LENGTH = 128\n",
    "CHECKPOINT_STEPS = 50\n",
    "FORCE_RETRAIN_CAMELBERT = False\n",
    "AUTO_DELETE_INCOMPLETE_CHECKPOINTS = True\n",
    "\n",
    "ARABERT_STANDARD_RUN_DIR = ROOT / \"models\" / \"islamic_ner_standard\"\n",
    "ARABERT_WEIGHTED_RUN_DIR = ROOT / \"models\" / \"islamic_ner_weighted\"\n",
    "ARABERT_STANDARD_FINAL = ARABERT_STANDARD_RUN_DIR / \"final_model\"\n",
    "ARABERT_WEIGHTED_FINAL = ARABERT_WEIGHTED_RUN_DIR / \"final_model\"\n",
    "\n",
    "CAMEL_RUN_DIR = ROOT / \"models\" / \"islamic_ner_camelbert_ca\"\n",
    "CAMEL_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels = [\n",
    "    \"O\",\n",
    "    \"B-SCHOLAR\", \"I-SCHOLAR\",\n",
    "    \"B-BOOK\", \"I-BOOK\",\n",
    "    \"B-CONCEPT\", \"I-CONCEPT\",\n",
    "    \"B-PLACE\", \"I-PLACE\",\n",
    "    \"B-HADITH_REF\", \"I-HADITH_REF\",\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "seqeval_metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"Torch device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print(f\"CAMeLBERT model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bf9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Data loading and metrics helpers ---------\n",
    "\n",
    "def load_silver_split(path: Path) -> Dataset:\n",
    "    records = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    cleaned = []\n",
    "    for i, record in enumerate(records):\n",
    "        tokens = record.get(\"tokens\") or []\n",
    "        tags = record.get(\"ner_tags\") or []\n",
    "        if not isinstance(tokens, list) or not isinstance(tags, list):\n",
    "            continue\n",
    "        n = min(len(tokens), len(tags))\n",
    "        if n == 0:\n",
    "            continue\n",
    "        cleaned.append(\n",
    "            {\n",
    "                \"id\": record.get(\"id\", f\"{path.stem}_{i}\"),\n",
    "                \"tokens\": tokens[:n],\n",
    "                \"ner_tags\": tags[:n],\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(cleaned)\n",
    "\n",
    "\n",
    "def load_dataset_dict() -> DatasetDict:\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": load_silver_split(SILVER_TRAIN_PATH),\n",
    "            \"dev\": load_silver_split(SILVER_DEV_PATH),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def count_labels(split_dataset: Dataset) -> Counter:\n",
    "        counts = Counter()\n",
    "        for row in split_dataset:\n",
    "            counts.update(row[\"ner_tags\"])\n",
    "        return counts\n",
    "\n",
    "    train_counts = count_labels(dataset_dict[\"train\"])\n",
    "    dev_counts = count_labels(dataset_dict[\"dev\"])\n",
    "    unknown_train = sorted(set(train_counts) - set(labels))\n",
    "    unknown_dev = sorted(set(dev_counts) - set(labels))\n",
    "    assert not unknown_train, f\"Unknown labels in train: {unknown_train}\"\n",
    "    assert not unknown_dev, f\"Unknown labels in dev: {unknown_dev}\"\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels_for_tokenizer(examples: Dict[str, List[List[str]]], tokenizer):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, word_level_tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[word_level_tags[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def decode_predictions(pred_ids: np.ndarray, label_ids: np.ndarray) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    y_true: List[List[str]] = []\n",
    "    y_pred: List[List[str]] = []\n",
    "\n",
    "    for pred_row, label_row in zip(pred_ids, label_ids):\n",
    "        row_true: List[str] = []\n",
    "        row_pred: List[str] = []\n",
    "        for pred_id, label_id in zip(pred_row, label_row):\n",
    "            if int(label_id) == -100:\n",
    "                continue\n",
    "            row_true.append(id2label[int(label_id)])\n",
    "            row_pred.append(id2label[int(pred_id)])\n",
    "        y_true.append(row_true)\n",
    "        y_pred.append(row_pred)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    logits, labels_arr = eval_pred\n",
    "    pred_ids = np.argmax(logits, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, labels_arr)\n",
    "\n",
    "    scores = seqeval_metric.compute(predictions=y_pred, references=y_true)\n",
    "    return {\n",
    "        \"precision\": float(scores[\"overall_precision\"]),\n",
    "        \"recall\": float(scores[\"overall_recall\"]),\n",
    "        \"f1\": float(scores[\"overall_f1\"]),\n",
    "        \"accuracy\": float(scores[\"overall_accuracy\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_and_report(trainer: Trainer, dataset, run_name: str, verbose: bool = True):\n",
    "    pred_output = trainer.predict(dataset)\n",
    "    pred_ids = np.argmax(pred_output.predictions, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, pred_output.label_ids)\n",
    "\n",
    "    overall = {\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "    report_text = classification_report(y_true, y_pred, digits=4, zero_division=0)\n",
    "    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[{run_name}] Overall Precision: {overall['precision']:.4f}\")\n",
    "        print(f\"[{run_name}] Overall Recall:    {overall['recall']:.4f}\")\n",
    "        print(f\"[{run_name}] Overall F1:        {overall['f1']:.4f}\")\n",
    "        print(report_text)\n",
    "\n",
    "    return overall, report_dict, pred_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb653a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Model/checkpoint helpers ---------\n",
    "\n",
    "REQUIRED_CKPT_FILES = [\n",
    "    \"model.safetensors\",\n",
    "    \"trainer_state.json\",\n",
    "    \"optimizer.pt\",\n",
    "    \"scheduler.pt\",\n",
    "    \"training_args.bin\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_tokenizer(source: str, prefer_local: bool = True):\n",
    "    if prefer_local:\n",
    "        try:\n",
    "            return AutoTokenizer.from_pretrained(source, local_files_only=True)\n",
    "        except Exception:\n",
    "            print(f\"Local tokenizer cache missing for: {source}. Retrying with online lookup...\")\n",
    "    return AutoTokenizer.from_pretrained(source)\n",
    "\n",
    "\n",
    "def init_model(model_path_or_name: str):\n",
    "    kwargs = {}\n",
    "    if model_path_or_name == MODEL_NAME:\n",
    "        try:\n",
    "            return AutoModelForTokenClassification.from_pretrained(\n",
    "                model_path_or_name,\n",
    "                num_labels=len(labels),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "        except Exception:\n",
    "            print(f\"Local model cache missing for: {model_path_or_name}. Retrying with online lookup...\")\n",
    "\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path_or_name,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "\n",
    "def is_checkpoint_complete(path: Path) -> bool:\n",
    "    return all((path / fname).exists() for fname in REQUIRED_CKPT_FILES)\n",
    "\n",
    "\n",
    "def find_latest_complete_checkpoint(run_dir: Path) -> str | None:\n",
    "    if not run_dir.exists():\n",
    "        return None\n",
    "\n",
    "    candidates = []\n",
    "    for path in run_dir.glob(\"checkpoint-*\"):\n",
    "        try:\n",
    "            step = int(path.name.split(\"-\")[-1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if is_checkpoint_complete(path):\n",
    "            candidates.append((step, path))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    return str(candidates[-1][1])\n",
    "\n",
    "\n",
    "def find_incomplete_checkpoints(run_dir: Path) -> List[Path]:\n",
    "    bad = []\n",
    "    for path in run_dir.glob(\"checkpoint-*\"):\n",
    "        if not is_checkpoint_complete(path):\n",
    "            bad.append(path)\n",
    "    return sorted(bad)\n",
    "\n",
    "\n",
    "def delete_incomplete_checkpoints(run_dir: Path):\n",
    "    removed = []\n",
    "    for bad_dir in find_incomplete_checkpoints(run_dir):\n",
    "        for child in sorted(bad_dir.rglob(\"*\"), reverse=True):\n",
    "            if child.is_file() or child.is_symlink():\n",
    "                child.unlink(missing_ok=True)\n",
    "            elif child.is_dir():\n",
    "                child.rmdir()\n",
    "        bad_dir.rmdir()\n",
    "        removed.append(bad_dir)\n",
    "    return removed\n",
    "\n",
    "\n",
    "def make_training_args(output_dir: Path) -> TrainingArguments:\n",
    "    kwargs = {\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": CHECKPOINT_STEPS,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_steps\": 25,\n",
    "        \"seed\": SEED,\n",
    "        \"report_to\": \"none\",\n",
    "    }\n",
    "\n",
    "    signature = inspect.signature(TrainingArguments.__init__)\n",
    "    if \"eval_strategy\" in signature.parameters:\n",
    "        kwargs[\"eval_strategy\"] = \"steps\"\n",
    "        kwargs[\"eval_steps\"] = CHECKPOINT_STEPS\n",
    "    else:\n",
    "        kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "        kwargs[\"eval_steps\"] = CHECKPOINT_STEPS\n",
    "\n",
    "    return TrainingArguments(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272071e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 2969\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 360\n",
      "    })\n",
      "})\n",
      "Train samples: 2969\n",
      "Dev samples: 360\n"
     ]
    }
   ],
   "source": [
    "# --------- Load dataset ---------\n",
    "\n",
    "dataset_dict = load_dataset_dict()\n",
    "print(dataset_dict)\n",
    "print(\"Train samples:\", len(dataset_dict[\"train\"]))\n",
    "print(\"Dev samples:\", len(dataset_dict[\"dev\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94a794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint/final model found; using base tokenizer: CAMeL-Lab/bert-base-arabic-camelbert-ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af27a59097d4a7891048cffc45075e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels (CAMeLBERT-CA):   0%|          | 0/2969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad9c0081f4429a80ae3db0b11a0023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels (CAMeLBERT-CA):   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2969\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------- Prepare CAMeLBERT tokenizer + tokenized splits ---------\n",
    "\n",
    "if AUTO_DELETE_INCOMPLETE_CHECKPOINTS:\n",
    "    removed = delete_incomplete_checkpoints(CAMEL_RUN_DIR)\n",
    "    if removed:\n",
    "        print(\"Removed incomplete checkpoints:\")\n",
    "        for p in removed:\n",
    "            print(\" -\", p)\n",
    "\n",
    "resume_ckpt = find_latest_complete_checkpoint(CAMEL_RUN_DIR)\n",
    "final_model_dir = CAMEL_RUN_DIR / \"final_model\"\n",
    "has_final_weights = (final_model_dir / \"model.safetensors\").exists()\n",
    "\n",
    "if resume_ckpt:\n",
    "    tokenizer_source = resume_ckpt\n",
    "    print(\"Found resumable CAMeLBERT checkpoint:\", resume_ckpt)\n",
    "elif has_final_weights:\n",
    "    tokenizer_source = str(final_model_dir)\n",
    "    print(\"No checkpoint found; using tokenizer from final model:\", final_model_dir)\n",
    "else:\n",
    "    tokenizer_source = MODEL_NAME\n",
    "    print(\"No checkpoint/final model found; using base tokenizer:\", MODEL_NAME)\n",
    "\n",
    "camel_tokenizer = load_tokenizer(tokenizer_source, prefer_local=True)\n",
    "camel_data_collator = DataCollatorForTokenClassification(tokenizer=camel_tokenizer)\n",
    "\n",
    "tokenized_camel = dataset_dict.map(\n",
    "    lambda examples: tokenize_and_align_labels_for_tokenizer(examples, camel_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    desc=\"Tokenize + align labels (CAMeLBERT-CA)\",\n",
    ")\n",
    "\n",
    "tokenized_camel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31139d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CAMeLBERT source: CAMeL-Lab/bert-base-arabic-camelbert-ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CAMeLBERT training from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b30caa09734de9b3866fa575cce814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7375, 'grad_norm': 1.733098030090332, 'learning_rate': 2.9193548387096776e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2735, 'grad_norm': 1.6799211502075195, 'learning_rate': 2.8387096774193552e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d023e289794944d280eb0d9d545f7d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21093282103538513, 'eval_precision': 0.7527733755942948, 'eval_recall': 0.7515822784810127, 'eval_f1': 0.7521773555027712, 'eval_accuracy': 0.9388170619973532, 'eval_runtime': 20.4438, 'eval_samples_per_second': 17.609, 'eval_steps_per_second': 1.125, 'epoch': 0.27}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting CAMeLBERT training from scratch.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     train_result_camel = \u001b[43mtrainer_camel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m eval_result_camel = trainer_camel.evaluate(tokenized_camel[\u001b[33m\"\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCAMeLBERT train metrics:\u001b[39m\u001b[33m\"\u001b[39m, train_result_camel.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2356\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2353\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2358\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2807\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2804\u001b[39m     metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m   2806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m2807\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2808\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2886\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial, metrics)\u001b[39m\n\u001b[32m   2884\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   2885\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m2886\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m   2890\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_optimizer_and_scheduler(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3454\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3451\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3453\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3525\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3523\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   3524\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3525\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3530\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:2759\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   2754\u001b[39m     gc.collect()\n\u001b[32m   2756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   2757\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   2758\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2759\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2761\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:307\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    277\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    278\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    279\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    280\u001b[39m ):\n\u001b[32m    281\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:547\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    536\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    540\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    541\u001b[39m     )\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    544\u001b[39m     k: {\n\u001b[32m    545\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    546\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    548\u001b[39m     }\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    550\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:456\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    450\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a sparse tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which this library does not support.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make a much larger file than needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor.is_contiguous():\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    457\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m     )\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    463\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m    464\u001b[39m     tensor = tensor.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."
     ]
    }
   ],
   "source": [
    "# --------- Train or reuse CAMeLBERT, then evaluate + save artifacts ---------\n",
    "\n",
    "train_result_camel = None\n",
    "eval_result_camel = None\n",
    "\n",
    "if has_final_weights and not FORCE_RETRAIN_CAMELBERT:\n",
    "    print(\"Using existing final CAMeLBERT weights (set FORCE_RETRAIN_CAMELBERT=True to retrain).\")\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=str(CAMEL_RUN_DIR / \"_tmp_eval\"),\n",
    "        per_device_eval_batch_size=16,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "    )\n",
    "    trainer_camel = Trainer(\n",
    "        model=init_model(str(final_model_dir)),\n",
    "        args=eval_args,\n",
    "        eval_dataset=tokenized_camel[\"dev\"],\n",
    "        tokenizer=camel_tokenizer,\n",
    "        data_collator=camel_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "else:\n",
    "    model_source = resume_ckpt or MODEL_NAME\n",
    "    print(\"Training CAMeLBERT source:\", model_source)\n",
    "\n",
    "    trainer_camel = Trainer(\n",
    "        model=init_model(model_source),\n",
    "        args=make_training_args(CAMEL_RUN_DIR),\n",
    "        train_dataset=tokenized_camel[\"train\"],\n",
    "        eval_dataset=tokenized_camel[\"dev\"],\n",
    "        tokenizer=camel_tokenizer,\n",
    "        data_collator=camel_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if resume_ckpt:\n",
    "        print(\"Resuming training from:\", resume_ckpt)\n",
    "        train_result_camel = trainer_camel.train(resume_from_checkpoint=resume_ckpt)\n",
    "    else:\n",
    "        print(\"Starting CAMeLBERT training from scratch.\")\n",
    "        train_result_camel = trainer_camel.train()\n",
    "\n",
    "    eval_result_camel = trainer_camel.evaluate(tokenized_camel[\"dev\"])\n",
    "    print(\"CAMeLBERT train metrics:\", train_result_camel.metrics)\n",
    "    print(\"CAMeLBERT eval metrics:\", eval_result_camel)\n",
    "\n",
    "overall_camel, report_camel, pred_output_camel = evaluate_and_report(\n",
    "    trainer_camel,\n",
    "    tokenized_camel[\"dev\"],\n",
    "    \"CAMeLBERT-CA\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "camel_final_dir = CAMEL_RUN_DIR / \"final_model\"\n",
    "camel_final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_camel.save_model(str(camel_final_dir))\n",
    "camel_tokenizer.save_pretrained(str(camel_final_dir))\n",
    "\n",
    "(CAMEL_RUN_DIR / \"dev_overall_metrics.json\").write_text(\n",
    "    json.dumps(overall_camel, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(CAMEL_RUN_DIR / \"dev_classification_report.json\").write_text(\n",
    "    json.dumps(report_camel, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "if train_result_camel is not None:\n",
    "    (CAMEL_RUN_DIR / \"run_camel_train_metrics.json\").write_text(\n",
    "        json.dumps(train_result_camel.metrics, indent=2, ensure_ascii=False),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "if eval_result_camel is not None:\n",
    "    (CAMEL_RUN_DIR / \"run_camel_eval_metrics.json\").write_text(\n",
    "        json.dumps(eval_result_camel, indent=2, ensure_ascii=False),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "print(f\"Saved CAMeLBERT artifacts to: {CAMEL_RUN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Evaluate AraBERT (standard + weighted) for fair per-class comparison ---------\n",
    "\n",
    "def evaluate_saved_model(model_dir: Path, run_name: str):\n",
    "    if not model_dir.exists() or not (model_dir / \"model.safetensors\").exists():\n",
    "        print(f\"[{run_name}] missing model weights at {model_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    model_tokenizer = load_tokenizer(str(model_dir), prefer_local=True)\n",
    "    tokenized_dev = dataset_dict[\"dev\"].map(\n",
    "        lambda examples: tokenize_and_align_labels_for_tokenizer(examples, model_tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset_dict[\"dev\"].column_names,\n",
    "        desc=f\"Tokenize dev ({run_name})\",\n",
    "    )\n",
    "\n",
    "    eval_trainer = Trainer(\n",
    "        model=init_model(str(model_dir)),\n",
    "        args=TrainingArguments(\n",
    "            output_dir=str(ROOT / \"models\" / \"_tmp_eval\" / run_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")),\n",
    "            per_device_eval_batch_size=16,\n",
    "            report_to=\"none\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        eval_dataset=tokenized_dev,\n",
    "        tokenizer=model_tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer=model_tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    overall, report, _ = evaluate_and_report(eval_trainer, tokenized_dev, run_name, verbose=False)\n",
    "    print(f\"[{run_name}] Overall F1: {overall['f1']:.4f}\")\n",
    "    return overall, report\n",
    "\n",
    "\n",
    "arabert_standard_overall, arabert_standard_report = evaluate_saved_model(\n",
    "    ARABERT_STANDARD_FINAL,\n",
    "    \"AraBERT (standard)\",\n",
    ")\n",
    "arabert_weighted_overall, arabert_weighted_report = evaluate_saved_model(\n",
    "    ARABERT_WEIGHTED_FINAL,\n",
    "    \"AraBERT (weighted)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca757e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Build required comparison table ---------\n",
    "\n",
    "def extract_overall_f1(overall: Dict | None) -> float:\n",
    "    if not overall:\n",
    "        return float(\"nan\")\n",
    "    if \"f1\" in overall:\n",
    "        return float(overall[\"f1\"])\n",
    "    if \"eval_f1\" in overall:\n",
    "        return float(overall[\"eval_f1\"])\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "def extract_entity_f1(report: Dict | None, entity_name: str) -> float:\n",
    "    if not report:\n",
    "        return float(\"nan\")\n",
    "    entity_metrics = report.get(entity_name, {})\n",
    "    if isinstance(entity_metrics, dict) and \"f1-score\" in entity_metrics:\n",
    "        return float(entity_metrics[\"f1-score\"])\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "def extract_macro_f1(report: Dict | None) -> float:\n",
    "    if not report:\n",
    "        return float(\"nan\")\n",
    "    macro = report.get(\"macro avg\", {})\n",
    "    if isinstance(macro, dict) and \"f1-score\" in macro:\n",
    "        return float(macro[\"f1-score\"])\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "comparison_rows = [\n",
    "    {\n",
    "        \"Metric\": \"Overall F1\",\n",
    "        \"AraBERT (standard)\": extract_overall_f1(arabert_standard_overall),\n",
    "        \"AraBERT (weighted)\": extract_overall_f1(arabert_weighted_overall),\n",
    "        \"CAMeLBERT-CA\": extract_overall_f1(overall_camel),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"SCHOLAR F1\",\n",
    "        \"AraBERT (standard)\": extract_entity_f1(arabert_standard_report, \"SCHOLAR\"),\n",
    "        \"AraBERT (weighted)\": extract_entity_f1(arabert_weighted_report, \"SCHOLAR\"),\n",
    "        \"CAMeLBERT-CA\": extract_entity_f1(report_camel, \"SCHOLAR\"),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"BOOK F1\",\n",
    "        \"AraBERT (standard)\": extract_entity_f1(arabert_standard_report, \"BOOK\"),\n",
    "        \"AraBERT (weighted)\": extract_entity_f1(arabert_weighted_report, \"BOOK\"),\n",
    "        \"CAMeLBERT-CA\": extract_entity_f1(report_camel, \"BOOK\"),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"CONCEPT F1\",\n",
    "        \"AraBERT (standard)\": extract_entity_f1(arabert_standard_report, \"CONCEPT\"),\n",
    "        \"AraBERT (weighted)\": extract_entity_f1(arabert_weighted_report, \"CONCEPT\"),\n",
    "        \"CAMeLBERT-CA\": extract_entity_f1(report_camel, \"CONCEPT\"),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"PLACE F1\",\n",
    "        \"AraBERT (standard)\": extract_entity_f1(arabert_standard_report, \"PLACE\"),\n",
    "        \"AraBERT (weighted)\": extract_entity_f1(arabert_weighted_report, \"PLACE\"),\n",
    "        \"CAMeLBERT-CA\": extract_entity_f1(report_camel, \"PLACE\"),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"HADITH_REF F1\",\n",
    "        \"AraBERT (standard)\": extract_entity_f1(arabert_standard_report, \"HADITH_REF\"),\n",
    "        \"AraBERT (weighted)\": extract_entity_f1(arabert_weighted_report, \"HADITH_REF\"),\n",
    "        \"CAMeLBERT-CA\": extract_entity_f1(report_camel, \"HADITH_REF\"),\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Macro F1\",\n",
    "        \"AraBERT (standard)\": extract_macro_f1(arabert_standard_report),\n",
    "        \"AraBERT (weighted)\": extract_macro_f1(arabert_weighted_report),\n",
    "        \"CAMeLBERT-CA\": extract_macro_f1(report_camel),\n",
    "    },\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "for col in [\"AraBERT (standard)\", \"AraBERT (weighted)\", \"CAMeLBERT-CA\"]:\n",
    "    comparison_df[col] = pd.to_numeric(comparison_df[col], errors=\"coerce\").round(4)\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5aa782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Save comparison outputs for README/report ---------\n",
    "\n",
    "comparison_csv_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison_with_camelbert.csv\"\n",
    "comparison_json_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison_with_camelbert.json\"\n",
    "\n",
    "comparison_payload = {\n",
    "    \"camelbert_model_name\": MODEL_NAME,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"arabert_standard_run_dir\": str(ARABERT_STANDARD_RUN_DIR),\n",
    "    \"arabert_weighted_run_dir\": str(ARABERT_WEIGHTED_RUN_DIR),\n",
    "    \"camelbert_run_dir\": str(CAMEL_RUN_DIR),\n",
    "    \"table\": comparison_rows,\n",
    "}\n",
    "\n",
    "comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "comparison_json_path.write_text(\n",
    "    json.dumps(comparison_payload, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Comparison table saved to:\", comparison_csv_path)\n",
    "print(\"Comparison summary saved to:\", comparison_json_path)\n",
    "\n",
    "try:\n",
    "    print(\"\n",
    "README table (markdown):\")\n",
    "    print(comparison_df.to_markdown(index=False))\n",
    "except Exception:\n",
    "    print(\"to_markdown unavailable (install tabulate). Use CSV output instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f72ca",
   "metadata": {},
   "source": [
    "## Analysis Questions\n",
    "\n",
    "Fill these after running all cells:\n",
    "\n",
    "- Which model wins overall?\n",
    "- Which model wins on rare classes?\n",
    "- Does Classical Arabic pre-training help?\n",
    "- What does this suggest about domain-specific pre-training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Auto-generated analysis draft ---------\n",
    "\n",
    "def winner_for(metric_name: str) -> str:\n",
    "    row = comparison_df[comparison_df[\"Metric\"] == metric_name]\n",
    "    if row.empty:\n",
    "        return \"N/A\"\n",
    "    series = pd.to_numeric(\n",
    "        row.iloc[0][[\"AraBERT (standard)\", \"AraBERT (weighted)\", \"CAMeLBERT-CA\"]],\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    if series.dropna().empty:\n",
    "        return \"N/A\"\n",
    "    return str(series.idxmax())\n",
    "\n",
    "\n",
    "def mean_metric(model_name: str, metric_names: List[str]) -> float:\n",
    "    vals = []\n",
    "    for m in metric_names:\n",
    "        row = comparison_df[comparison_df[\"Metric\"] == m]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        vals.append(float(pd.to_numeric(row.iloc[0][model_name], errors=\"coerce\")))\n",
    "    vals = [v for v in vals if not np.isnan(v)]\n",
    "    if not vals:\n",
    "        return float(\"nan\")\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "\n",
    "overall_winner = winner_for(\"Overall F1\")\n",
    "scholar_winner = winner_for(\"SCHOLAR F1\")\n",
    "concept_winner = winner_for(\"CONCEPT F1\")\n",
    "macro_winner = winner_for(\"Macro F1\")\n",
    "\n",
    "rare_metrics = [\"BOOK F1\", \"PLACE F1\", \"HADITH_REF F1\"]\n",
    "rare_scores = {\n",
    "    \"AraBERT (standard)\": mean_metric(\"AraBERT (standard)\", rare_metrics),\n",
    "    \"AraBERT (weighted)\": mean_metric(\"AraBERT (weighted)\", rare_metrics),\n",
    "    \"CAMeLBERT-CA\": mean_metric(\"CAMeLBERT-CA\", rare_metrics),\n",
    "}\n",
    "rare_winner = max(\n",
    "    rare_scores,\n",
    "    key=lambda k: (-np.inf if np.isnan(rare_scores[k]) else rare_scores[k]),\n",
    ")\n",
    "\n",
    "print(\"Overall winner:\", overall_winner)\n",
    "print(\"Rare-class winner (BOOK/PLACE/HADITH_REF mean):\", rare_winner)\n",
    "print(\"SCHOLAR winner:\", scholar_winner)\n",
    "print(\"CONCEPT winner:\", concept_winner)\n",
    "print(\"Macro-F1 winner:\", macro_winner)\n",
    "\n",
    "print(\"\n",
    "Draft interpretation:\")\n",
    "print(\"1. Model that wins Overall F1 is the best default for deployment on current dev distribution.\")\n",
    "print(\"2. Rare-class winner indicates which backbone handles sparse entity supervision better.\")\n",
    "print(\"3. If CAMeLBERT-CA wins SCHOLAR or overall, that supports value of Classical Arabic pre-training.\")\n",
    "print(\"4. If CAMeLBERT-CA loses CONCEPT while winning SCHOLAR, domain pre-training helps classical entities but may trade off modern/abstract terms.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
