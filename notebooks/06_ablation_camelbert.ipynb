{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - CAMeLBERT Ablation (Clean Rewrite)\n",
        "\n",
        "This notebook compares AraBERT (standard and weighted) with CAMeLBERT-CA on the silver dev split. It uses saved AraBERT metrics, simplifies CAMeLBERT training logic, and saves a reproducible comparison table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import random\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"data\").exists() and (candidate / \"models\").exists():\n",
        "            return candidate\n",
        "    return start\n",
        "\n",
        "\n",
        "ROOT = find_project_root(Path.cwd())\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "labels = [\n",
        "    \"O\",\n",
        "    \"B-SCHOLAR\",\n",
        "    \"I-SCHOLAR\",\n",
        "    \"B-BOOK\",\n",
        "    \"I-BOOK\",\n",
        "    \"B-CONCEPT\",\n",
        "    \"I-CONCEPT\",\n",
        "    \"B-PLACE\",\n",
        "    \"I-PLACE\",\n",
        "    \"B-HADITH_REF\",\n",
        "    \"I-HADITH_REF\",\n",
        "]\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "DATA_DIR = ROOT / \"data\" / \"silver\"\n",
        "TRAIN_PATH = DATA_DIR / \"train.json\"\n",
        "DEV_PATH = DATA_DIR / \"dev.json\"\n",
        "\n",
        "MODELS_DIR = ROOT / \"models\"\n",
        "RUN_A_DIR = MODELS_DIR / \"islamic_ner_standard\"\n",
        "RUN_B_DIR = MODELS_DIR / \"islamic_ner_weighted\"\n",
        "CAMELBERT_DIR = MODELS_DIR / \"islamic_ner_camelbert_ca\"\n",
        "CAMELBERT_FINAL_DIR = CAMELBERT_DIR / \"final_model\"\n",
        "\n",
        "print(f\"ROOT: {ROOT}\")\n",
        "print(f\"Train: {TRAIN_PATH}\")\n",
        "print(f\"Dev: {DEV_PATH}\")\n",
        "print(f\"Models: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = json.loads(TRAIN_PATH.read_text(encoding=\"utf-8\"))\n",
        "dev_data = json.loads(DEV_PATH.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def clean_records(records: list[dict]) -> list[dict]:\n",
        "    cleaned = []\n",
        "    for record in records:\n",
        "        tokens = record.get(\"tokens\", [])\n",
        "        ner_tags = record.get(\"ner_tags\", [])\n",
        "        if not isinstance(tokens, list) or not isinstance(ner_tags, list):\n",
        "            continue\n",
        "        if len(tokens) != len(ner_tags):\n",
        "            continue\n",
        "        if any(tag not in label2id for tag in ner_tags):\n",
        "            continue\n",
        "        cleaned.append(\n",
        "            {\n",
        "                \"id\": record.get(\"id\"),\n",
        "                \"tokens\": tokens,\n",
        "                \"ner_tags\": ner_tags,\n",
        "            }\n",
        "        )\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "train_records = clean_records(train_data)\n",
        "dev_records = clean_records(dev_data)\n",
        "\n",
        "print(f\"Raw train records: {len(train_data)}\")\n",
        "print(f\"Raw dev records: {len(dev_data)}\")\n",
        "print(f\"Filtered train records: {len(train_records)}\")\n",
        "print(f\"Filtered dev records: {len(dev_records)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, tokenizer, max_length: int = 128):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for batch_index, label_sequence in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
        "        label_ids = []\n",
        "        previous_word_id = None\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_id != previous_word_id:\n",
        "                label_ids.append(label2id[label_sequence[word_id]])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_id = word_id\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "def to_tag_sequences(predictions, label_ids):\n",
        "    predicted_ids = np.argmax(predictions, axis=2)\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for pred_row, label_row in zip(predicted_ids, label_ids):\n",
        "        pred_tags = []\n",
        "        label_tags = []\n",
        "        for pred_id, label_id in zip(pred_row, label_row):\n",
        "            if label_id == -100:\n",
        "                continue\n",
        "            pred_tags.append(id2label[int(pred_id)])\n",
        "            label_tags.append(id2label[int(label_id)])\n",
        "        true_predictions.append(pred_tags)\n",
        "        true_labels.append(label_tags)\n",
        "\n",
        "    return true_predictions, true_labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, label_ids = eval_pred\n",
        "    true_predictions, true_labels = to_tag_sequences(predictions, label_ids)\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_predictions),\n",
        "        \"recall\": recall_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_classification_report(predictions, label_ids):\n",
        "    true_predictions, true_labels = to_tag_sequences(predictions, label_ids)\n",
        "    return classification_report(\n",
        "        true_labels,\n",
        "        true_predictions,\n",
        "        output_dict=True,\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def to_builtin(value):\n",
        "    if isinstance(value, dict):\n",
        "        return {k: to_builtin(v) for k, v in value.items()}\n",
        "    if isinstance(value, list):\n",
        "        return [to_builtin(v) for v in value]\n",
        "    if isinstance(value, tuple):\n",
        "        return [to_builtin(v) for v in value]\n",
        "    if isinstance(value, np.generic):\n",
        "        return value.item()\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_A_METRICS_PATH = RUN_A_DIR / \"run_a_eval_metrics.json\"\n",
        "RUN_B_METRICS_PATH = RUN_B_DIR / \"run_b_eval_metrics.json\"\n",
        "\n",
        "RUN_A_REPORT_PATH = RUN_A_DIR / \"dev_classification_report.json\"\n",
        "RUN_B_REPORT_PATH = RUN_B_DIR / \"dev_classification_report.json\"\n",
        "\n",
        "KNOWN_RUN_A = {\"f1\": 0.9370, \"precision\": 0.9241, \"recall\": 0.9504}\n",
        "KNOWN_RUN_B = {\"f1\": 0.8720, \"precision\": 0.8242, \"recall\": 0.9256}\n",
        "\n",
        "\n",
        "def load_saved_or_known_metrics(path: Path, known_values: dict, run_name: str) -> dict:\n",
        "    if path.exists():\n",
        "        raw = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "        metrics = {\n",
        "            \"f1\": float(raw.get(\"eval_f1\", raw.get(\"f1\", known_values[\"f1\"]))),\n",
        "            \"precision\": float(raw.get(\"eval_precision\", raw.get(\"precision\", known_values[\"precision\"]))),\n",
        "            \"recall\": float(raw.get(\"eval_recall\", raw.get(\"recall\", known_values[\"recall\"]))),\n",
        "        }\n",
        "        print(f\"{run_name}: loaded metrics from {path}\")\n",
        "        return metrics\n",
        "\n",
        "    print(f\"WARNING: {run_name} metrics file not found at {path}. Using known hardcoded values.\")\n",
        "    return known_values.copy()\n",
        "\n",
        "\n",
        "def load_report_if_exists(path: Path, run_name: str):\n",
        "    if path.exists():\n",
        "        print(f\"{run_name}: loaded classification report from {path}\")\n",
        "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "    print(f\"WARNING: {run_name} classification report missing at {path}. Per-entity values will be NaN.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "arabert_standard_overall = load_saved_or_known_metrics(RUN_A_METRICS_PATH, KNOWN_RUN_A, \"AraBERT Run A\")\n",
        "arabert_weighted_overall = load_saved_or_known_metrics(RUN_B_METRICS_PATH, KNOWN_RUN_B, \"AraBERT Run B\")\n",
        "\n",
        "arabert_standard_report = load_report_if_exists(RUN_A_REPORT_PATH, \"AraBERT Run A\")\n",
        "arabert_weighted_report = load_report_if_exists(RUN_B_REPORT_PATH, \"AraBERT Run B\")\n",
        "\n",
        "print(\"AraBERT Run A overall:\", arabert_standard_overall)\n",
        "print(\"AraBERT Run B overall:\", arabert_weighted_overall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\"\n",
        "CAMELBERT_MODEL_PATH = CAMELBERT_FINAL_DIR / \"model.safetensors\"\n",
        "\n",
        "camelbert_overall = {\"f1\": np.nan, \"precision\": np.nan, \"recall\": np.nan}\n",
        "camelbert_report = None\n",
        "\n",
        "\n",
        "try:\n",
        "    if CAMELBERT_MODEL_PATH.exists():\n",
        "        print(f\"Found existing CAMeLBERT model at {CAMELBERT_MODEL_PATH}. Skipping training.\")\n",
        "        tokenizer_source = CAMELBERT_FINAL_DIR if (CAMELBERT_FINAL_DIR / \"tokenizer_config.json\").exists() else MODEL_NAME\n",
        "        camelbert_tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_source))\n",
        "        camelbert_model = AutoModelForTokenClassification.from_pretrained(str(CAMELBERT_FINAL_DIR))\n",
        "\n",
        "        dev_dataset = Dataset.from_list(dev_records)\n",
        "        tokenized_dev = dev_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=dev_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        eval_kwargs = {\n",
        "            \"output_dir\": str(CAMELBERT_DIR / \"eval_runs\"),\n",
        "            \"per_device_eval_batch_size\": 16,\n",
        "            \"seed\": SEED,\n",
        "            \"report_to\": \"none\",\n",
        "            \"save_strategy\": \"no\",\n",
        "        }\n",
        "        eval_signature = inspect.signature(TrainingArguments.__init__).parameters\n",
        "        if \"eval_strategy\" in eval_signature:\n",
        "            eval_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            eval_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "\n",
        "        eval_args = TrainingArguments(**eval_kwargs)\n",
        "        data_collator = DataCollatorForTokenClassification(tokenizer=camelbert_tokenizer)\n",
        "        trainer = Trainer(\n",
        "            model=camelbert_model,\n",
        "            args=eval_args,\n",
        "            eval_dataset=tokenized_dev,\n",
        "            tokenizer=camelbert_tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"No saved final CAMeLBERT model found. Training from scratch.\")\n",
        "        CAMELBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        camelbert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        camelbert_model = AutoModelForTokenClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=len(labels),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "        )\n",
        "\n",
        "        train_dataset = Dataset.from_list(train_records)\n",
        "        dev_dataset = Dataset.from_list(dev_records)\n",
        "\n",
        "        tokenized_train = train_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=train_dataset.column_names,\n",
        "        )\n",
        "        tokenized_dev = dev_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=dev_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        training_kwargs = {\n",
        "            \"output_dir\": str(CAMELBERT_DIR),\n",
        "            \"num_train_epochs\": 5,\n",
        "            \"per_device_train_batch_size\": 16,\n",
        "            \"per_device_eval_batch_size\": 16,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"weight_decay\": 0.01,\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"load_best_model_at_end\": True,\n",
        "            \"metric_for_best_model\": \"f1\",\n",
        "            \"greater_is_better\": True,\n",
        "            \"seed\": SEED,\n",
        "            \"report_to\": \"none\",\n",
        "        }\n",
        "        training_signature = inspect.signature(TrainingArguments.__init__).parameters\n",
        "        if \"eval_strategy\" in training_signature:\n",
        "            training_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            training_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "\n",
        "        training_args = TrainingArguments(**training_kwargs)\n",
        "        data_collator = DataCollatorForTokenClassification(tokenizer=camelbert_tokenizer)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=camelbert_model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train,\n",
        "            eval_dataset=tokenized_dev,\n",
        "            tokenizer=camelbert_tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        CAMELBERT_FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        trainer.save_model(str(CAMELBERT_FINAL_DIR))\n",
        "        camelbert_tokenizer.save_pretrained(str(CAMELBERT_FINAL_DIR))\n",
        "        print(f\"Saved CAMeLBERT final model to {CAMELBERT_FINAL_DIR}\")\n",
        "\n",
        "    camelbert_eval_metrics = trainer.evaluate(eval_dataset=tokenized_dev)\n",
        "    camelbert_predictions = trainer.predict(tokenized_dev)\n",
        "    camelbert_report = build_classification_report(camelbert_predictions.predictions, camelbert_predictions.label_ids)\n",
        "\n",
        "    camelbert_overall = {\n",
        "        \"f1\": float(camelbert_eval_metrics.get(\"eval_f1\", camelbert_eval_metrics.get(\"f1\", np.nan))),\n",
        "        \"precision\": float(camelbert_eval_metrics.get(\"eval_precision\", camelbert_eval_metrics.get(\"precision\", np.nan))),\n",
        "        \"recall\": float(camelbert_eval_metrics.get(\"eval_recall\", camelbert_eval_metrics.get(\"recall\", np.nan))),\n",
        "    }\n",
        "\n",
        "    CAMELBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    (CAMELBERT_DIR / \"camelbert_eval_metrics.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_eval_metrics), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "    (CAMELBERT_DIR / \"dev_overall_metrics.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_overall), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "    (CAMELBERT_DIR / \"dev_classification_report.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_report), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "    print(\"CAMeLBERT overall:\", camelbert_overall)\n",
        "\n",
        "except Exception:\n",
        "    print(\"CAMeLBERT training/evaluation failed. Full traceback:\")\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def overall_f1(metrics: dict) -> float:\n",
        "    if not isinstance(metrics, dict):\n",
        "        return np.nan\n",
        "    if \"f1\" in metrics:\n",
        "        return float(metrics[\"f1\"])\n",
        "    if \"eval_f1\" in metrics:\n",
        "        return float(metrics[\"eval_f1\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def entity_f1(report: dict | None, entity_name: str) -> float:\n",
        "    if not isinstance(report, dict):\n",
        "        return np.nan\n",
        "    entity_stats = report.get(entity_name)\n",
        "    if isinstance(entity_stats, dict) and \"f1-score\" in entity_stats:\n",
        "        return float(entity_stats[\"f1-score\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def macro_f1(report: dict | None) -> float:\n",
        "    if not isinstance(report, dict):\n",
        "        return np.nan\n",
        "    macro_stats = report.get(\"macro avg\")\n",
        "    if isinstance(macro_stats, dict) and \"f1-score\" in macro_stats:\n",
        "        return float(macro_stats[\"f1-score\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "comparison_df = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"Metric\": \"Overall F1\",\n",
        "            \"AraBERT (standard)\": overall_f1(arabert_standard_overall),\n",
        "            \"AraBERT (weighted)\": overall_f1(arabert_weighted_overall),\n",
        "            \"CAMeLBERT-CA\": overall_f1(camelbert_overall),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"SCHOLAR F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"SCHOLAR\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"SCHOLAR\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"SCHOLAR\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"BOOK F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"BOOK\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"BOOK\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"BOOK\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"CONCEPT F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"CONCEPT\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"CONCEPT\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"CONCEPT\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"PLACE F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"PLACE\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"PLACE\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"PLACE\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"HADITH_REF F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"HADITH_REF\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"HADITH_REF\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"HADITH_REF\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"Macro F1\",\n",
        "            \"AraBERT (standard)\": macro_f1(arabert_standard_report),\n",
        "            \"AraBERT (weighted)\": macro_f1(arabert_weighted_report),\n",
        "            \"CAMeLBERT-CA\": macro_f1(camelbert_report),\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_csv_path = MODELS_DIR / \"islamic_ner_ablation_comparison.csv\"\n",
        "comparison_json_path = MODELS_DIR / \"islamic_ner_ablation_comparison.json\"\n",
        "\n",
        "comparison_df.to_csv(comparison_csv_path, index=False, encoding=\"utf-8\")\n",
        "comparison_json_path.write_text(\n",
        "    json.dumps(to_builtin(comparison_df.to_dict(orient=\"records\")), ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "\n",
        "def dataframe_to_markdown(df: pd.DataFrame) -> str:\n",
        "    headers = list(df.columns)\n",
        "    lines = [\n",
        "        \"| \" + \" | \".join(headers) + \" |\",\n",
        "        \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\",\n",
        "    ]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        values = []\n",
        "        for value in row.values:\n",
        "            if isinstance(value, (float, np.floating)) and np.isnan(value):\n",
        "                values.append(\"NaN\")\n",
        "            elif isinstance(value, (float, np.floating)):\n",
        "                values.append(f\"{float(value):.4f}\")\n",
        "            else:\n",
        "                values.append(str(value))\n",
        "        lines.append(\"| \" + \" | \".join(values) + \" |\")\n",
        "\n",
        "    return \"\n",
        "\".join(lines)\n",
        "\n",
        "\n",
        "print(f\"Saved CSV: {comparison_csv_path}\")\n",
        "print(f\"Saved JSON: {comparison_json_path}\")\n",
        "print(\"\n",
        "Markdown table for README:\n",
        "\")\n",
        "print(dataframe_to_markdown(comparison_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Questions\n",
        "\n",
        "1. Which model wins overall?\n",
        "2. Which model wins on rare classes?\n",
        "3. Does Classical Arabic pre-training help?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}