{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - CAMeLBERT Ablation (Clean Rewrite)\n",
        "\n",
        "This notebook compares AraBERT (standard and weighted) with CAMeLBERT-CA on the silver dev split. It uses saved AraBERT metrics, simplifies CAMeLBERT training logic, and saves a reproducible comparison table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROOT: c:\\Users\\diaab\\islamic-ner\n",
            "Train: c:\\Users\\diaab\\islamic-ner\\data\\silver\\train.json\n",
            "Dev: c:\\Users\\diaab\\islamic-ner\\data\\silver\\dev.json\n",
            "Models: c:\\Users\\diaab\\islamic-ner\\models\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import random\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"data\").exists() and (candidate / \"models\").exists():\n",
        "            return candidate\n",
        "    return start\n",
        "\n",
        "\n",
        "ROOT = find_project_root(Path.cwd())\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "labels = [\n",
        "    \"O\",\n",
        "    \"B-SCHOLAR\",\n",
        "    \"I-SCHOLAR\",\n",
        "    \"B-BOOK\",\n",
        "    \"I-BOOK\",\n",
        "    \"B-CONCEPT\",\n",
        "    \"I-CONCEPT\",\n",
        "    \"B-PLACE\",\n",
        "    \"I-PLACE\",\n",
        "    \"B-HADITH_REF\",\n",
        "    \"I-HADITH_REF\",\n",
        "]\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "DATA_DIR = ROOT / \"data\" / \"silver\"\n",
        "TRAIN_PATH = DATA_DIR / \"train.json\"\n",
        "DEV_PATH = DATA_DIR / \"dev.json\"\n",
        "\n",
        "MODELS_DIR = ROOT / \"models\"\n",
        "RUN_A_DIR = MODELS_DIR / \"islamic_ner_standard\"\n",
        "RUN_B_DIR = MODELS_DIR / \"islamic_ner_weighted\"\n",
        "CAMELBERT_DIR = MODELS_DIR / \"islamic_ner_camelbert_ca\"\n",
        "CAMELBERT_FINAL_DIR = CAMELBERT_DIR / \"final_model\"\n",
        "\n",
        "print(f\"ROOT: {ROOT}\")\n",
        "print(f\"Train: {TRAIN_PATH}\")\n",
        "print(f\"Dev: {DEV_PATH}\")\n",
        "print(f\"Models: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw train records: 2969\n",
            "Raw dev records: 360\n",
            "Filtered train records: 2969\n",
            "Filtered dev records: 360\n"
          ]
        }
      ],
      "source": [
        "train_data = json.loads(TRAIN_PATH.read_text(encoding=\"utf-8\"))\n",
        "dev_data = json.loads(DEV_PATH.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def clean_records(records: list[dict]) -> list[dict]:\n",
        "    cleaned = []\n",
        "    for record in records:\n",
        "        tokens = record.get(\"tokens\", [])\n",
        "        ner_tags = record.get(\"ner_tags\", [])\n",
        "        if not isinstance(tokens, list) or not isinstance(ner_tags, list):\n",
        "            continue\n",
        "        if len(tokens) != len(ner_tags):\n",
        "            continue\n",
        "        if any(tag not in label2id for tag in ner_tags):\n",
        "            continue\n",
        "        cleaned.append(\n",
        "            {\n",
        "                \"id\": record.get(\"id\"),\n",
        "                \"tokens\": tokens,\n",
        "                \"ner_tags\": ner_tags,\n",
        "            }\n",
        "        )\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "train_records = clean_records(train_data)\n",
        "dev_records = clean_records(dev_data)\n",
        "\n",
        "print(f\"Raw train records: {len(train_data)}\")\n",
        "print(f\"Raw dev records: {len(dev_data)}\")\n",
        "print(f\"Filtered train records: {len(train_records)}\")\n",
        "print(f\"Filtered dev records: {len(dev_records)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, tokenizer, max_length: int = 128):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for batch_index, label_sequence in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
        "        label_ids = []\n",
        "        previous_word_id = None\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_id != previous_word_id:\n",
        "                label_ids.append(label2id[label_sequence[word_id]])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_id = word_id\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "def to_tag_sequences(predictions, label_ids):\n",
        "    predicted_ids = np.argmax(predictions, axis=2)\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for pred_row, label_row in zip(predicted_ids, label_ids):\n",
        "        pred_tags = []\n",
        "        label_tags = []\n",
        "        for pred_id, label_id in zip(pred_row, label_row):\n",
        "            if label_id == -100:\n",
        "                continue\n",
        "            pred_tags.append(id2label[int(pred_id)])\n",
        "            label_tags.append(id2label[int(label_id)])\n",
        "        true_predictions.append(pred_tags)\n",
        "        true_labels.append(label_tags)\n",
        "\n",
        "    return true_predictions, true_labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, label_ids = eval_pred\n",
        "    true_predictions, true_labels = to_tag_sequences(predictions, label_ids)\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_predictions),\n",
        "        \"recall\": recall_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_classification_report(predictions, label_ids):\n",
        "    true_predictions, true_labels = to_tag_sequences(predictions, label_ids)\n",
        "    return classification_report(\n",
        "        true_labels,\n",
        "        true_predictions,\n",
        "        output_dict=True,\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def to_builtin(value):\n",
        "    if isinstance(value, dict):\n",
        "        return {k: to_builtin(v) for k, v in value.items()}\n",
        "    if isinstance(value, list):\n",
        "        return [to_builtin(v) for v in value]\n",
        "    if isinstance(value, tuple):\n",
        "        return [to_builtin(v) for v in value]\n",
        "    if isinstance(value, np.generic):\n",
        "        return value.item()\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AraBERT Run A: loaded metrics from c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_standard\\run_a_eval_metrics.json\n",
            "WARNING: AraBERT Run B metrics file not found at c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_weighted\\run_b_eval_metrics.json. Using known hardcoded values.\n",
            "WARNING: AraBERT Run A classification report missing at c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_standard\\dev_classification_report.json. Per-entity values will be NaN.\n",
            "WARNING: AraBERT Run B classification report missing at c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_weighted\\dev_classification_report.json. Per-entity values will be NaN.\n",
            "AraBERT Run A overall: {'f1': 0.9370447450572321, 'precision': 0.9240636223704464, 'recall': 0.950395778364116}\n",
            "AraBERT Run B overall: {'f1': 0.872, 'precision': 0.8242, 'recall': 0.9256}\n"
          ]
        }
      ],
      "source": [
        "RUN_A_METRICS_PATH = RUN_A_DIR / \"run_a_eval_metrics.json\"\n",
        "RUN_B_METRICS_PATH = RUN_B_DIR / \"run_b_eval_metrics.json\"\n",
        "\n",
        "RUN_A_REPORT_PATH = RUN_A_DIR / \"dev_classification_report.json\"\n",
        "RUN_B_REPORT_PATH = RUN_B_DIR / \"dev_classification_report.json\"\n",
        "\n",
        "KNOWN_RUN_A = {\"f1\": 0.9370, \"precision\": 0.9241, \"recall\": 0.9504}\n",
        "KNOWN_RUN_B = {\"f1\": 0.8720, \"precision\": 0.8242, \"recall\": 0.9256}\n",
        "\n",
        "\n",
        "def load_saved_or_known_metrics(path: Path, known_values: dict, run_name: str) -> dict:\n",
        "    if path.exists():\n",
        "        raw = json.loads(path.read_text(encoding=\"utf-8-sig\"))\n",
        "        metrics = {\n",
        "            \"f1\": float(raw.get(\"eval_f1\", raw.get(\"f1\", known_values[\"f1\"]))),\n",
        "            \"precision\": float(raw.get(\"eval_precision\", raw.get(\"precision\", known_values[\"precision\"]))),\n",
        "            \"recall\": float(raw.get(\"eval_recall\", raw.get(\"recall\", known_values[\"recall\"]))),\n",
        "        }\n",
        "        print(f\"{run_name}: loaded metrics from {path}\")\n",
        "        return metrics\n",
        "\n",
        "    print(f\"WARNING: {run_name} metrics file not found at {path}. Using known hardcoded values.\")\n",
        "    return known_values.copy()\n",
        "\n",
        "\n",
        "def load_report_if_exists(path: Path, run_name: str):\n",
        "    if path.exists():\n",
        "        print(f\"{run_name}: loaded classification report from {path}\")\n",
        "        return json.loads(path.read_text(encoding=\"utf-8-sig\"))\n",
        "\n",
        "    print(f\"WARNING: {run_name} classification report missing at {path}. Per-entity values will be NaN.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "arabert_standard_overall = load_saved_or_known_metrics(RUN_A_METRICS_PATH, KNOWN_RUN_A, \"AraBERT Run A\")\n",
        "arabert_weighted_overall = load_saved_or_known_metrics(RUN_B_METRICS_PATH, KNOWN_RUN_B, \"AraBERT Run B\")\n",
        "\n",
        "arabert_standard_report = load_report_if_exists(RUN_A_REPORT_PATH, \"AraBERT Run A\")\n",
        "arabert_weighted_report = load_report_if_exists(RUN_B_REPORT_PATH, \"AraBERT Run B\")\n",
        "\n",
        "print(\"AraBERT Run A overall:\", arabert_standard_overall)\n",
        "print(\"AraBERT Run B overall:\", arabert_weighted_overall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No saved final CAMeLBERT model found. Training from scratch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fc7388cfa1645f485d1c13466fc4cc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2969 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b61a66f4c190456d927a0be9e598961d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/360 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5324d4214100427585c78f681290b2b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/930 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "431df29aebd946af8283c1d786e0dd1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.09633850306272507, 'eval_precision': 0.861228813559322, 'eval_recall': 0.8575949367088608, 'eval_f1': 0.8594080338266386, 'eval_runtime': 29.7828, 'eval_samples_per_second': 12.088, 'eval_steps_per_second': 0.772, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "801ce54c0f694a9fa5ef73e4d2c58e74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.06424366682767868, 'eval_precision': 0.8891739353514623, 'eval_recall': 0.9140295358649789, 'eval_f1': 0.9014304291287386, 'eval_runtime': 23.3574, 'eval_samples_per_second': 15.413, 'eval_steps_per_second': 0.985, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1313, 'grad_norm': 0.551958441734314, 'learning_rate': 1.3870967741935484e-05, 'epoch': 2.69}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99ac65dc912c49b8beea164e298f21ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.052211347967386246, 'eval_precision': 0.9159707724425887, 'eval_recall': 0.9256329113924051, 'eval_f1': 0.9207764952780694, 'eval_runtime': 24.4717, 'eval_samples_per_second': 14.711, 'eval_steps_per_second': 0.94, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b04d7daca2c4f928edb33cb4cb4cd95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.05023102089762688, 'eval_precision': 0.921313183949974, 'eval_recall': 0.9324894514767933, 'eval_f1': 0.926867627785059, 'eval_runtime': 23.6457, 'eval_samples_per_second': 15.225, 'eval_steps_per_second': 0.973, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "119e5bba7bd54bc698384b2933363111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.05576692149043083, 'eval_precision': 0.9202485758674263, 'eval_recall': 0.9372362869198312, 'eval_f1': 0.9286647504572771, 'eval_runtime': 23.8944, 'eval_samples_per_second': 15.066, 'eval_steps_per_second': 0.963, 'epoch': 5.0}\n",
            "{'train_runtime': 4489.4399, 'train_samples_per_second': 3.307, 'train_steps_per_second': 0.207, 'train_loss': 0.08120005412768293, 'epoch': 5.0}\n",
            "Saved CAMeLBERT final model to c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_camelbert_ca\\final_model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c08450028a24de6b0446175ab7f05f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8625c422e3264c88bda1f63485fe510e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAMeLBERT overall: {'f1': 0.9286647504572771, 'precision': 0.9202485758674263, 'recall': 0.9372362869198312}\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\"\n",
        "CAMELBERT_MODEL_PATH = CAMELBERT_FINAL_DIR / \"model.safetensors\"\n",
        "\n",
        "camelbert_overall = {\"f1\": np.nan, \"precision\": np.nan, \"recall\": np.nan}\n",
        "camelbert_report = None\n",
        "\n",
        "\n",
        "try:\n",
        "    if CAMELBERT_MODEL_PATH.exists():\n",
        "        print(f\"Found existing CAMeLBERT model at {CAMELBERT_MODEL_PATH}. Skipping training.\")\n",
        "        tokenizer_source = CAMELBERT_FINAL_DIR if (CAMELBERT_FINAL_DIR / \"tokenizer_config.json\").exists() else MODEL_NAME\n",
        "        camelbert_tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_source))\n",
        "        camelbert_model = AutoModelForTokenClassification.from_pretrained(str(CAMELBERT_FINAL_DIR))\n",
        "\n",
        "        dev_dataset = Dataset.from_list(dev_records)\n",
        "        tokenized_dev = dev_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=dev_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        eval_kwargs = {\n",
        "            \"output_dir\": str(CAMELBERT_DIR / \"eval_runs\"),\n",
        "            \"per_device_eval_batch_size\": 16,\n",
        "            \"seed\": SEED,\n",
        "            \"report_to\": \"none\",\n",
        "            \"save_strategy\": \"no\",\n",
        "        }\n",
        "        eval_signature = inspect.signature(TrainingArguments.__init__).parameters\n",
        "        if \"eval_strategy\" in eval_signature:\n",
        "            eval_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            eval_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "\n",
        "        eval_args = TrainingArguments(**eval_kwargs)\n",
        "        data_collator = DataCollatorForTokenClassification(tokenizer=camelbert_tokenizer)\n",
        "        trainer = Trainer(\n",
        "            model=camelbert_model,\n",
        "            args=eval_args,\n",
        "            eval_dataset=tokenized_dev,\n",
        "            tokenizer=camelbert_tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"No saved final CAMeLBERT model found. Training from scratch.\")\n",
        "        CAMELBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        camelbert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        camelbert_model = AutoModelForTokenClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=len(labels),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "        )\n",
        "\n",
        "        train_dataset = Dataset.from_list(train_records)\n",
        "        dev_dataset = Dataset.from_list(dev_records)\n",
        "\n",
        "        tokenized_train = train_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=train_dataset.column_names,\n",
        "        )\n",
        "        tokenized_dev = dev_dataset.map(\n",
        "            lambda batch: tokenize_and_align_labels(batch, camelbert_tokenizer, max_length=128),\n",
        "            batched=True,\n",
        "            remove_columns=dev_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        training_kwargs = {\n",
        "            \"output_dir\": str(CAMELBERT_DIR),\n",
        "            \"num_train_epochs\": 5,\n",
        "            \"per_device_train_batch_size\": 16,\n",
        "            \"per_device_eval_batch_size\": 16,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"weight_decay\": 0.01,\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"load_best_model_at_end\": True,\n",
        "            \"metric_for_best_model\": \"f1\",\n",
        "            \"greater_is_better\": True,\n",
        "            \"seed\": SEED,\n",
        "            \"report_to\": \"none\",\n",
        "            \"save_safetensors\": False,\n",
        "        }\n",
        "        training_signature = inspect.signature(TrainingArguments.__init__).parameters\n",
        "        if \"eval_strategy\" in training_signature:\n",
        "            training_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            training_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "\n",
        "        training_args = TrainingArguments(**training_kwargs)\n",
        "        data_collator = DataCollatorForTokenClassification(tokenizer=camelbert_tokenizer)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=camelbert_model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train,\n",
        "            eval_dataset=tokenized_dev,\n",
        "            tokenizer=camelbert_tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        CAMELBERT_FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        trainer.save_model(str(CAMELBERT_FINAL_DIR))\n",
        "        camelbert_tokenizer.save_pretrained(str(CAMELBERT_FINAL_DIR))\n",
        "        print(f\"Saved CAMeLBERT final model to {CAMELBERT_FINAL_DIR}\")\n",
        "\n",
        "    camelbert_eval_metrics = trainer.evaluate(eval_dataset=tokenized_dev)\n",
        "    camelbert_predictions = trainer.predict(tokenized_dev)\n",
        "    camelbert_report = build_classification_report(camelbert_predictions.predictions, camelbert_predictions.label_ids)\n",
        "\n",
        "    camelbert_overall = {\n",
        "        \"f1\": float(camelbert_eval_metrics.get(\"eval_f1\", camelbert_eval_metrics.get(\"f1\", np.nan))),\n",
        "        \"precision\": float(camelbert_eval_metrics.get(\"eval_precision\", camelbert_eval_metrics.get(\"precision\", np.nan))),\n",
        "        \"recall\": float(camelbert_eval_metrics.get(\"eval_recall\", camelbert_eval_metrics.get(\"recall\", np.nan))),\n",
        "    }\n",
        "\n",
        "    CAMELBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    (CAMELBERT_DIR / \"camelbert_eval_metrics.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_eval_metrics), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "    (CAMELBERT_DIR / \"dev_overall_metrics.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_overall), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "    (CAMELBERT_DIR / \"dev_classification_report.json\").write_text(\n",
        "        json.dumps(to_builtin(camelbert_report), ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "    print(\"CAMeLBERT overall:\", camelbert_overall)\n",
        "\n",
        "except Exception:\n",
        "    print(\"CAMeLBERT training/evaluation failed. Full traceback:\")\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>AraBERT (standard)</th>\n",
              "      <th>AraBERT (weighted)</th>\n",
              "      <th>CAMeLBERT-CA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overall F1</td>\n",
              "      <td>0.937045</td>\n",
              "      <td>0.872</td>\n",
              "      <td>0.928665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SCHOLAR F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.930889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BOOK F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CONCEPT F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PLACE F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HADITH_REF F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Macro F1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.681055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Metric  AraBERT (standard)  AraBERT (weighted)  CAMeLBERT-CA\n",
              "0     Overall F1            0.937045               0.872      0.928665\n",
              "1     SCHOLAR F1                 NaN                 NaN      0.930889\n",
              "2        BOOK F1                 NaN                 NaN           NaN\n",
              "3     CONCEPT F1                 NaN                 NaN      0.933333\n",
              "4       PLACE F1                 NaN                 NaN      0.860000\n",
              "5  HADITH_REF F1                 NaN                 NaN      0.000000\n",
              "6       Macro F1                 NaN                 NaN      0.681055"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def overall_f1(metrics: dict) -> float:\n",
        "    if not isinstance(metrics, dict):\n",
        "        return np.nan\n",
        "    if \"f1\" in metrics:\n",
        "        return float(metrics[\"f1\"])\n",
        "    if \"eval_f1\" in metrics:\n",
        "        return float(metrics[\"eval_f1\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def entity_f1(report: dict | None, entity_name: str) -> float:\n",
        "    if not isinstance(report, dict):\n",
        "        return np.nan\n",
        "    entity_stats = report.get(entity_name)\n",
        "    if isinstance(entity_stats, dict) and \"f1-score\" in entity_stats:\n",
        "        return float(entity_stats[\"f1-score\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def macro_f1(report: dict | None) -> float:\n",
        "    if not isinstance(report, dict):\n",
        "        return np.nan\n",
        "    macro_stats = report.get(\"macro avg\")\n",
        "    if isinstance(macro_stats, dict) and \"f1-score\" in macro_stats:\n",
        "        return float(macro_stats[\"f1-score\"])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "comparison_df = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"Metric\": \"Overall F1\",\n",
        "            \"AraBERT (standard)\": overall_f1(arabert_standard_overall),\n",
        "            \"AraBERT (weighted)\": overall_f1(arabert_weighted_overall),\n",
        "            \"CAMeLBERT-CA\": overall_f1(camelbert_overall),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"SCHOLAR F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"SCHOLAR\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"SCHOLAR\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"SCHOLAR\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"BOOK F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"BOOK\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"BOOK\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"BOOK\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"CONCEPT F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"CONCEPT\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"CONCEPT\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"CONCEPT\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"PLACE F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"PLACE\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"PLACE\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"PLACE\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"HADITH_REF F1\",\n",
        "            \"AraBERT (standard)\": entity_f1(arabert_standard_report, \"HADITH_REF\"),\n",
        "            \"AraBERT (weighted)\": entity_f1(arabert_weighted_report, \"HADITH_REF\"),\n",
        "            \"CAMeLBERT-CA\": entity_f1(camelbert_report, \"HADITH_REF\"),\n",
        "        },\n",
        "        {\n",
        "            \"Metric\": \"Macro F1\",\n",
        "            \"AraBERT (standard)\": macro_f1(arabert_standard_report),\n",
        "            \"AraBERT (weighted)\": macro_f1(arabert_weighted_report),\n",
        "            \"CAMeLBERT-CA\": macro_f1(camelbert_report),\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved CSV: c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_ablation_comparison.csv\n",
            "Saved JSON: c:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_ablation_comparison.json\n",
            "Markdown table for README:\n",
            "| Metric | AraBERT (standard) | AraBERT (weighted) | CAMeLBERT-CA |\n",
            "| --- | --- | --- | --- |\n",
            "| Overall F1 | 0.9370 | 0.8720 | 0.9287 |\n",
            "| SCHOLAR F1 | NaN | NaN | 0.9309 |\n",
            "| BOOK F1 | NaN | NaN | NaN |\n",
            "| CONCEPT F1 | NaN | NaN | 0.9333 |\n",
            "| PLACE F1 | NaN | NaN | 0.8600 |\n",
            "| HADITH_REF F1 | NaN | NaN | 0.0000 |\n",
            "| Macro F1 | NaN | NaN | 0.6811 |\n"
          ]
        }
      ],
      "source": [
        "comparison_csv_path = MODELS_DIR / \"islamic_ner_ablation_comparison.csv\"\n",
        "comparison_json_path = MODELS_DIR / \"islamic_ner_ablation_comparison.json\"\n",
        "\n",
        "comparison_df.to_csv(comparison_csv_path, index=False, encoding=\"utf-8\")\n",
        "comparison_json_path.write_text(\n",
        "    json.dumps(to_builtin(comparison_df.to_dict(orient=\"records\")), ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "\n",
        "def dataframe_to_markdown(df: pd.DataFrame) -> str:\n",
        "    headers = list(df.columns)\n",
        "    lines = [\n",
        "        \"| \" + \" | \".join(headers) + \" |\",\n",
        "        \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\",\n",
        "    ]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        values = []\n",
        "        for value in row.values:\n",
        "            if isinstance(value, (float, np.floating)) and np.isnan(value):\n",
        "                values.append(\"NaN\")\n",
        "            elif isinstance(value, (float, np.floating)):\n",
        "                values.append(f\"{float(value):.4f}\")\n",
        "            else:\n",
        "                values.append(str(value))\n",
        "        lines.append(\"| \" + \" | \".join(values) + \" |\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "print(f\"Saved CSV: {comparison_csv_path}\")\n",
        "print(f\"Saved JSON: {comparison_json_path}\")\n",
        "print(\"Markdown table for README:\")\n",
        "print(dataframe_to_markdown(comparison_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Questions\n",
        "\n",
        "1. Which model wins overall?\n",
        "2. Which model wins on rare classes?\n",
        "3. Does Classical Arabic pre-training help?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
