{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Islamic NER Training (Silver Data, AraBERT v02)\n",
    "\n",
    "This notebook trains and compares two token-classification setups on the silver dataset:\n",
    "- **Run A**: standard cross-entropy\n",
    "- **Run B**: class-weighted cross-entropy\n",
    "\n",
    "Both runs use the same tokenizer/model backbone and the same subword alignment strategy as the ANERCorp baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Two Training Runs? (Ablation Study)\n",
    "\n",
    "We run two experiments to isolate the impact of **class weighting** under strong label imbalance.\n",
    "\n",
    "- Run A gives the reference behavior with no weighting.\n",
    "- Run B increases the loss contribution of rare classes (BOOK, HADITH_REF, some I- tags).\n",
    "\n",
    "Class weights can improve recall for rare entities by penalizing mistakes on those classes more heavily.\n",
    "Given this silver distribution, expect **SCHOLAR** to be strongest, while **BOOK/HADITH_REF** may stay weaker due to very low support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\diaab\\islamic-ner\n",
      "Torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def ensure_package(import_name: str, pip_name: str | None = None) -> None:\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        return\n",
    "    except ImportError:\n",
    "        pkg = pip_name or import_name\n",
    "        print(f\"Installing missing package: {pkg} (python: {sys.executable})\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "\n",
    "# Install into the active notebook kernel only if missing.\n",
    "ensure_package(\"evaluate\")\n",
    "ensure_package(\"seqeval\")\n",
    "ensure_package(\"datasets\")\n",
    "ensure_package(\"transformers\")\n",
    "ensure_package(\"accelerate\")\n",
    "ensure_package(\"matplotlib\")\n",
    "ensure_package(\"pandas\")\n",
    "ensure_package(\"numpy\")\n",
    "ensure_package(\"torch\")\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"data\").exists() and (candidate / \"notebooks\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_project_root(Path.cwd().resolve())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Torch device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: C:\\Users\\diaab\\islamic-ner\\data\\silver\\train.json\n",
      "Dev path: C:\\Users\\diaab\\islamic-ner\\data\\silver\\dev.json\n",
      "Model: aubmindlab/bert-base-arabertv02\n",
      "MAX_SEQ_LENGTH: 128\n",
      "Label mapping: {'O': 0, 'B-SCHOLAR': 1, 'I-SCHOLAR': 2, 'B-BOOK': 3, 'I-BOOK': 4, 'B-CONCEPT': 5, 'I-CONCEPT': 6, 'B-PLACE': 7, 'I-PLACE': 8, 'B-HADITH_REF': 9, 'I-HADITH_REF': 10}\n"
     ]
    }
   ],
   "source": [
    "SILVER_TRAIN_PATH = ROOT / \"data\" / \"silver\" / \"train.json\"\n",
    "SILVER_DEV_PATH = ROOT / \"data\" / \"silver\" / \"dev.json\"\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "labels = [\n",
    "    \"O\",\n",
    "    \"B-SCHOLAR\", \"I-SCHOLAR\",\n",
    "    \"B-BOOK\", \"I-BOOK\",\n",
    "    \"B-CONCEPT\", \"I-CONCEPT\",\n",
    "    \"B-PLACE\", \"I-PLACE\",\n",
    "    \"B-HADITH_REF\", \"I-HADITH_REF\",\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Train path:\", SILVER_TRAIN_PATH)\n",
    "print(\"Dev path:\", SILVER_DEV_PATH)\n",
    "print(\"Model:\", model_name)\n",
    "print(\"MAX_SEQ_LENGTH:\", MAX_SEQ_LENGTH)\n",
    "print(\"Label mapping:\", label2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Note (AraBERT Preprocessing)\n",
    "\n",
    "AraBERT v02 can benefit from `ArabertPreprocessor` pre-segmentation.\n",
    "For this notebook, we intentionally **skip preprocessing** and use the raw tokenizer input so we can measure it later as a separate ablation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 2969\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 360\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_count</th>\n",
       "      <th>dev_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>125158</td>\n",
       "      <td>15583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-SCHOLAR</td>\n",
       "      <td>14053</td>\n",
       "      <td>1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-SCHOLAR</td>\n",
       "      <td>20228</td>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-BOOK</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-BOOK</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-CONCEPT</td>\n",
       "      <td>767</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-CONCEPT</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PLACE</td>\n",
       "      <td>356</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PLACE</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-HADITH_REF</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-HADITH_REF</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  train_count  dev_count\n",
       "0              O       125158      15583\n",
       "1      B-SCHOLAR        14053       1763\n",
       "2      I-SCHOLAR        20228       2411\n",
       "3         B-BOOK           50          0\n",
       "4         I-BOOK           51          0\n",
       "5      B-CONCEPT          767         92\n",
       "6      I-CONCEPT            6          0\n",
       "7        B-PLACE          356         55\n",
       "8        I-PLACE            8          6\n",
       "9   B-HADITH_REF           75          0\n",
       "10  I-HADITH_REF          182          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_silver_split(path: Path) -> Dataset:\n",
    "    records = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    cleaned = []\n",
    "    for i, record in enumerate(records):\n",
    "        tokens = record.get(\"tokens\") or []\n",
    "        tags = record.get(\"ner_tags\") or []\n",
    "        if not isinstance(tokens, list) or not isinstance(tags, list):\n",
    "            continue\n",
    "        n = min(len(tokens), len(tags))\n",
    "        if n == 0:\n",
    "            continue\n",
    "        cleaned.append(\n",
    "            {\n",
    "                \"id\": record.get(\"id\", f\"{path.stem}_{i}\"),\n",
    "                \"tokens\": tokens[:n],\n",
    "                \"ner_tags\": tags[:n],\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(cleaned)\n",
    "\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": load_silver_split(SILVER_TRAIN_PATH),\n",
    "        \"dev\": load_silver_split(SILVER_DEV_PATH),\n",
    "    }\n",
    ")\n",
    "print(dataset_dict)\n",
    "\n",
    "\n",
    "def count_labels(split_dataset: Dataset) -> Counter:\n",
    "    counts = Counter()\n",
    "    for row in split_dataset:\n",
    "        counts.update(row[\"ner_tags\"])\n",
    "    return counts\n",
    "\n",
    "\n",
    "train_counts = count_labels(dataset_dict[\"train\"])\n",
    "dev_counts = count_labels(dataset_dict[\"dev\"])\n",
    "\n",
    "unknown_train = sorted(set(train_counts) - set(labels))\n",
    "unknown_dev = sorted(set(dev_counts) - set(labels))\n",
    "assert not unknown_train, f\"Unknown labels in train: {unknown_train}\"\n",
    "assert not unknown_dev, f\"Unknown labels in dev: {unknown_dev}\"\n",
    "\n",
    "dist_df = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"train_count\": [train_counts.get(lbl, 0) for lbl in labels],\n",
    "    \"dev_count\": [dev_counts.get(lbl, 0) for lbl in labels],\n",
    "})\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f041c01c3d6c49e0afd92d5acbda709e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels:   0%|          | 0/2969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d58ec7f68a4932af589c18f180c908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels:   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2969\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples: Dict[str, List[List[str]]]) -> Dict[str, List[List[int]]]:\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, word_level_tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword keeps the original word label.\n",
    "                label_ids.append(label2id[word_level_tags[word_idx]])\n",
    "            else:\n",
    "                # Non-first subwords are ignored in the loss/metrics.\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    desc=\"Tokenize + align labels\",\n",
    ")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m seqeval_metric = \u001b[43mevaluate\u001b[49m.load(\u001b[33m\"\u001b[39m\u001b[33mseqeval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_predictions\u001b[39m(pred_ids: np.ndarray, label_ids: np.ndarray) -> Tuple[List[List[\u001b[38;5;28mstr\u001b[39m]], List[List[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m      5\u001b[39m     y_true: List[List[\u001b[38;5;28mstr\u001b[39m]] = []\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "seqeval_metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def decode_predictions(pred_ids: np.ndarray, label_ids: np.ndarray) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    y_true: List[List[str]] = []\n",
    "    y_pred: List[List[str]] = []\n",
    "\n",
    "    for pred_row, label_row in zip(pred_ids, label_ids):\n",
    "        row_true: List[str] = []\n",
    "        row_pred: List[str] = []\n",
    "        for pred_id, label_id in zip(pred_row, label_row):\n",
    "            if int(label_id) == -100:\n",
    "                continue\n",
    "            row_true.append(id2label[int(label_id)])\n",
    "            row_pred.append(id2label[int(pred_id)])\n",
    "        y_true.append(row_true)\n",
    "        y_pred.append(row_pred)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    logits, labels_arr = eval_pred\n",
    "    pred_ids = np.argmax(logits, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, labels_arr)\n",
    "\n",
    "    scores = seqeval_metric.compute(predictions=y_pred, references=y_true)\n",
    "    return {\n",
    "        \"precision\": float(scores[\"overall_precision\"]),\n",
    "        \"recall\": float(scores[\"overall_recall\"]),\n",
    "        \"f1\": float(scores[\"overall_f1\"]),\n",
    "        \"accuracy\": float(scores[\"overall_accuracy\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_and_print(trainer: Trainer, dataset, run_name: str):\n",
    "    pred_output = trainer.predict(dataset)\n",
    "    pred_ids = np.argmax(pred_output.predictions, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, pred_output.label_ids)\n",
    "\n",
    "    overall = {\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "    }\n",
    "    report_text = classification_report(y_true, y_pred, digits=4)\n",
    "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    print(f\"[{run_name}] Overall Precision: {overall['precision']:.4f}\")\n",
    "    print(f\"[{run_name}] Overall Recall:    {overall['recall']:.4f}\")\n",
    "    print(f\"[{run_name}] Overall F1:        {overall['f1']:.4f}\\n\")\n",
    "    print(report_text)\n",
    "\n",
    "    return overall, report_dict, pred_output\n",
    "\n",
    "\n",
    "def plot_training_loss(trainer: Trainer, run_name: str) -> pd.DataFrame:\n",
    "    loss_rows = []\n",
    "    for row in trainer.state.log_history:\n",
    "        if \"loss\" in row and \"eval_loss\" not in row and \"epoch\" in row:\n",
    "            loss_rows.append({\"epoch\": float(row[\"epoch\"]), \"loss\": float(row[\"loss\"])})\n",
    "\n",
    "    if not loss_rows:\n",
    "        print(f\"No training loss logs found for {run_name}.\")\n",
    "        return pd.DataFrame(columns=[\"epoch\", \"loss\"])\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_rows).sort_values(\"epoch\")\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(loss_df[\"epoch\"], loss_df[\"loss\"], marker=\"o\")\n",
    "    plt.title(f\"Training Loss Curve - {run_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return loss_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_A_DIR = ROOT / \"models\" / \"islamic_ner_standard\"\n",
    "RUN_B_DIR = ROOT / \"models\" / \"islamic_ner_weighted\"\n",
    "RUN_A_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUN_B_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_training_args(output_dir: Path) -> TrainingArguments:\n",
    "    kwargs = {\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_steps\": 25,\n",
    "        \"seed\": SEED,\n",
    "        \"report_to\": \"none\",\n",
    "    }\n",
    "\n",
    "    # Transformers renamed evaluation_strategy -> eval_strategy in newer releases.\n",
    "    signature = inspect.signature(TrainingArguments.__init__)\n",
    "    if \"eval_strategy\" in signature.parameters:\n",
    "        kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "    else:\n",
    "        kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "\n",
    "    return TrainingArguments(**kwargs)\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "\n",
    "For Run B, class weights are computed from train-token frequencies:\n",
    "\n",
    "`weight_i = total_tokens / (num_classes * count_i)`\n",
    "\n",
    "Then all weights are normalized so the `O` class is near `1.0`.\n",
    "This keeps weighting interpretable while upweighting rare entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "      <th>raw_weight</th>\n",
       "      <th>normalized_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>125158</td>\n",
       "      <td>0.116895</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-SCHOLAR</td>\n",
       "      <td>14053</td>\n",
       "      <td>1.041085</td>\n",
       "      <td>8.906141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-SCHOLAR</td>\n",
       "      <td>20228</td>\n",
       "      <td>0.723273</td>\n",
       "      <td>6.187364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-BOOK</td>\n",
       "      <td>50</td>\n",
       "      <td>292.607273</td>\n",
       "      <td>2503.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-BOOK</td>\n",
       "      <td>51</td>\n",
       "      <td>286.869875</td>\n",
       "      <td>2454.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-CONCEPT</td>\n",
       "      <td>767</td>\n",
       "      <td>19.074790</td>\n",
       "      <td>163.178618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-CONCEPT</td>\n",
       "      <td>6</td>\n",
       "      <td>2438.393939</td>\n",
       "      <td>20859.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PLACE</td>\n",
       "      <td>356</td>\n",
       "      <td>41.096527</td>\n",
       "      <td>351.567416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PLACE</td>\n",
       "      <td>8</td>\n",
       "      <td>1828.795455</td>\n",
       "      <td>15644.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-HADITH_REF</td>\n",
       "      <td>75</td>\n",
       "      <td>195.071515</td>\n",
       "      <td>1668.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-HADITH_REF</td>\n",
       "      <td>182</td>\n",
       "      <td>80.386613</td>\n",
       "      <td>687.681319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label   count   raw_weight  normalized_weight\n",
       "0              O  125158     0.116895           1.000000\n",
       "1      B-SCHOLAR   14053     1.041085           8.906141\n",
       "2      I-SCHOLAR   20228     0.723273           6.187364\n",
       "3         B-BOOK      50   292.607273        2503.160000\n",
       "4         I-BOOK      51   286.869875        2454.078431\n",
       "5      B-CONCEPT     767    19.074790         163.178618\n",
       "6      I-CONCEPT       6  2438.393939       20859.666667\n",
       "7        B-PLACE     356    41.096527         351.567416\n",
       "8        I-PLACE       8  1828.795455       15644.750000\n",
       "9   B-HADITH_REF      75   195.071515        1668.773333\n",
       "10  I-HADITH_REF     182    80.386613         687.681319"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_counts = Counter()\n",
    "for row in dataset_dict[\"train\"]:\n",
    "    train_label_counts.update(row[\"ner_tags\"])\n",
    "\n",
    "total_tokens = sum(train_label_counts.get(label, 0) for label in labels)\n",
    "num_classes = len(labels)\n",
    "\n",
    "raw_weights = {}\n",
    "for label in labels:\n",
    "    count_i = train_label_counts.get(label, 0)\n",
    "    if count_i == 0:\n",
    "        raw_weights[label] = 0.0\n",
    "    else:\n",
    "        raw_weights[label] = total_tokens / (num_classes * count_i)\n",
    "\n",
    "o_weight = raw_weights.get(\"O\", 1.0) or 1.0\n",
    "normalized_weights = {\n",
    "    label: (weight / o_weight if weight > 0 else 0.0)\n",
    "    for label, weight in raw_weights.items()\n",
    "}\n",
    "\n",
    "class_weights_tensor = torch.tensor([normalized_weights[label] for label in labels], dtype=torch.float)\n",
    "\n",
    "weights_df = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": labels,\n",
    "        \"count\": [train_label_counts.get(label, 0) for label in labels],\n",
    "        \"raw_weight\": [raw_weights[label] for label in labels],\n",
    "        \"normalized_weight\": [normalized_weights[label] for label in labels],\n",
    "    }\n",
    ")\n",
    "weights_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run A: Standard Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0984030bea4d8b97e8232e4e370293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6401, 'grad_norm': 1.6450728178024292, 'learning_rate': 2.9193548387096776e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2674, 'grad_norm': 0.9909935593605042, 'learning_rate': 2.8387096774193552e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1835, 'grad_norm': 0.8548858761787415, 'learning_rate': 2.758064516129032e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1505, 'grad_norm': 1.2996970415115356, 'learning_rate': 2.6774193548387097e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1336, 'grad_norm': 0.900173008441925, 'learning_rate': 2.5967741935483872e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1119, 'grad_norm': 1.9526371955871582, 'learning_rate': 2.5161290322580648e-05, 'epoch': 0.81}\n",
      "{'loss': 0.096, 'grad_norm': 1.576730728149414, 'learning_rate': 2.4354838709677417e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22588df6a7914435b742a2f741072b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07621356099843979, 'eval_precision': 0.8334965719882468, 'eval_recall': 0.8981530343007915, 'eval_f1': 0.8646177292354584, 'eval_accuracy': 0.9718761124955501, 'eval_runtime': 24.9136, 'eval_samples_per_second': 14.45, 'eval_steps_per_second': 0.923, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0741, 'grad_norm': 0.672429084777832, 'learning_rate': 2.3548387096774193e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0612, 'grad_norm': 0.9741367697715759, 'learning_rate': 2.274193548387097e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0626, 'grad_norm': 1.2146960496902466, 'learning_rate': 2.1935483870967744e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0647, 'grad_norm': 0.7410274147987366, 'learning_rate': 2.1129032258064516e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0613, 'grad_norm': 1.7313599586486816, 'learning_rate': 2.032258064516129e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0594, 'grad_norm': 1.0290260314941406, 'learning_rate': 1.9516129032258064e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0564, 'grad_norm': 1.0620189905166626, 'learning_rate': 1.870967741935484e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649ab05e1ab24715832432aee2d54229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05845726281404495, 'eval_precision': 0.8822637695805963, 'eval_recall': 0.9213720316622691, 'eval_f1': 0.9013939081053175, 'eval_accuracy': 0.978741799318517, 'eval_runtime': 24.7011, 'eval_samples_per_second': 14.574, 'eval_steps_per_second': 0.931, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0474, 'grad_norm': 1.2661596536636353, 'learning_rate': 1.7903225806451616e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0378, 'grad_norm': 0.7674102783203125, 'learning_rate': 1.7096774193548388e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0459, 'grad_norm': 1.1610219478607178, 'learning_rate': 1.629032258064516e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0418, 'grad_norm': 0.5561832785606384, 'learning_rate': 1.5483870967741936e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0367, 'grad_norm': 0.8121139407157898, 'learning_rate': 1.467741935483871e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0318, 'grad_norm': 0.5557832717895508, 'learning_rate': 1.3870967741935484e-05, 'epoch': 2.69}\n",
      "{'loss': 0.0382, 'grad_norm': 0.8272638320922852, 'learning_rate': 1.3064516129032258e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0354, 'grad_norm': 0.5930027961730957, 'learning_rate': 1.2258064516129034e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2c52cebe14d8a9a53b28e781d3fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04503537341952324, 'eval_precision': 0.9161021365294424, 'eval_recall': 0.9277044854881267, 'eval_f1': 0.9218668065023597, 'eval_accuracy': 0.9844886334740375, 'eval_runtime': 23.5961, 'eval_samples_per_second': 15.257, 'eval_steps_per_second': 0.975, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0269, 'grad_norm': 0.961778461933136, 'learning_rate': 1.1451612903225806e-05, 'epoch': 3.09}\n",
      "{'loss': 0.028, 'grad_norm': 0.7221362590789795, 'learning_rate': 1.0645161290322582e-05, 'epoch': 3.23}\n",
      "{'loss': 0.0275, 'grad_norm': 0.7667438983917236, 'learning_rate': 9.838709677419354e-06, 'epoch': 3.36}\n",
      "{'loss': 0.0234, 'grad_norm': 0.503824770450592, 'learning_rate': 9.03225806451613e-06, 'epoch': 3.49}\n",
      "{'loss': 0.0245, 'grad_norm': 0.6172646880149841, 'learning_rate': 8.225806451612904e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0308, 'grad_norm': 0.8951314687728882, 'learning_rate': 7.419354838709678e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0286, 'grad_norm': 0.5362250208854675, 'learning_rate': 6.612903225806452e-06, 'epoch': 3.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e2830a4d7f4d8d959976a3bce4a626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04456828534603119, 'eval_precision': 0.9195696721311475, 'eval_recall': 0.9472295514511874, 'eval_f1': 0.9331946971666233, 'eval_accuracy': 0.9850989167471902, 'eval_runtime': 23.7427, 'eval_samples_per_second': 15.163, 'eval_steps_per_second': 0.969, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0226, 'grad_norm': 0.4139533042907715, 'learning_rate': 5.8064516129032256e-06, 'epoch': 4.03}\n",
      "{'loss': 0.0209, 'grad_norm': 0.5850458145141602, 'learning_rate': 4.9999999999999996e-06, 'epoch': 4.17}\n",
      "{'loss': 0.0214, 'grad_norm': 0.8732544779777527, 'learning_rate': 4.1935483870967736e-06, 'epoch': 4.3}\n",
      "{'loss': 0.021, 'grad_norm': 0.6469407081604004, 'learning_rate': 3.3870967741935484e-06, 'epoch': 4.44}\n",
      "{'loss': 0.0191, 'grad_norm': 0.8116809725761414, 'learning_rate': 2.580645161290323e-06, 'epoch': 4.57}\n",
      "{'loss': 0.0194, 'grad_norm': 0.6761155724525452, 'learning_rate': 1.774193548387097e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0186, 'grad_norm': 0.6528463363647461, 'learning_rate': 9.67741935483871e-07, 'epoch': 4.84}\n",
      "{'loss': 0.0208, 'grad_norm': 0.6678882837295532, 'learning_rate': 1.6129032258064518e-07, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ac1eda2e1841d28f28c9fbd29c3adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.045910559594631195, 'eval_precision': 0.9240636223704464, 'eval_recall': 0.950395778364116, 'eval_f1': 0.9370447450572321, 'eval_accuracy': 0.9858617708386309, 'eval_runtime': 25.2566, 'eval_samples_per_second': 14.254, 'eval_steps_per_second': 0.911, 'epoch': 5.0}\n",
      "{'train_runtime': 4496.0966, 'train_samples_per_second': 3.302, 'train_steps_per_second': 0.207, 'train_loss': 0.07244265638692404, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e708830307094477b640974fc7e3b02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run A train metrics: {'train_runtime': 4496.0966, 'train_samples_per_second': 3.302, 'train_steps_per_second': 0.207, 'total_flos': 929459193068352.0, 'train_loss': 0.07244265638692404, 'epoch': 5.0}\n",
      "Run A eval metrics: {'eval_loss': 0.045910559594631195, 'eval_precision': 0.9240636223704464, 'eval_recall': 0.950395778364116, 'eval_f1': 0.9370447450572321, 'eval_accuracy': 0.9858617708386309, 'eval_runtime': 24.6871, 'eval_samples_per_second': 14.583, 'eval_steps_per_second': 0.932, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "model_a = init_model()\n",
    "training_args_a = make_training_args(RUN_A_DIR)\n",
    "\n",
    "trainer_a = Trainer(\n",
    "    model=model_a,\n",
    "    args=training_args_a,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result_a = trainer_a.train()\n",
    "eval_result_a = trainer_a.evaluate(tokenized_datasets[\"dev\"])\n",
    "print(\"Run A train metrics:\", train_result_a.metrics)\n",
    "print(\"Run A eval metrics:\", eval_result_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_and_print' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m overall_a, report_a, pred_output_a = \u001b[43mevaluate_and_print\u001b[49m(\n\u001b[32m      2\u001b[39m     trainer_a, tokenized_datasets[\u001b[33m\"\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRun A - Standard\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m loss_curve_a = plot_training_loss(trainer_a, \u001b[33m\"\u001b[39m\u001b[33mRun A - Standard\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m run_a_final_dir = RUN_A_DIR / \u001b[33m\"\u001b[39m\u001b[33mfinal_model\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate_and_print' is not defined"
     ]
    }
   ],
   "source": [
    "overall_a, report_a, pred_output_a = evaluate_and_print(\n",
    "    trainer_a, tokenized_datasets[\"dev\"], \"Run A - Standard\"\n",
    ")\n",
    "loss_curve_a = plot_training_loss(trainer_a, \"Run A - Standard\")\n",
    "\n",
    "run_a_final_dir = RUN_A_DIR / \"final_model\"\n",
    "run_a_final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_a.save_model(str(run_a_final_dir))\n",
    "tokenizer.save_pretrained(str(run_a_final_dir))\n",
    "\n",
    "(RUN_A_DIR / \"dev_overall_metrics.json\").write_text(\n",
    "    json.dumps(overall_a, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(RUN_A_DIR / \"dev_classification_report.json\").write_text(\n",
    "    json.dumps(report_a, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "if not loss_curve_a.empty:\n",
    "    loss_curve_a.to_csv(RUN_A_DIR / \"training_loss_curve.csv\", index=False)\n",
    "\n",
    "print(f\"Saved Run A model and metrics under: {RUN_A_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run B: Class-Weighted Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels_tensor = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights.to(logits.device),\n",
    "            ignore_index=-100,\n",
    "        )\n",
    "        loss = loss_fn(logits.view(-1, model.config.num_labels), labels_tensor.view(-1))\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "model_b = init_model()\n",
    "training_args_b = make_training_args(RUN_B_DIR)\n",
    "\n",
    "trainer_b = WeightedTrainer(\n",
    "    model=model_b,\n",
    "    args=training_args_b,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "train_result_b = trainer_b.train()\n",
    "eval_result_b = trainer_b.evaluate(tokenized_datasets[\"dev\"])\n",
    "print(\"Run B train metrics:\", train_result_b.metrics)\n",
    "print(\"Run B eval metrics:\", eval_result_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_b, report_b, pred_output_b = evaluate_and_print(\n",
    "    trainer_b, tokenized_datasets[\"dev\"], \"Run B - Weighted\"\n",
    ")\n",
    "loss_curve_b = plot_training_loss(trainer_b, \"Run B - Weighted\")\n",
    "\n",
    "run_b_final_dir = RUN_B_DIR / \"final_model\"\n",
    "run_b_final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_b.save_model(str(run_b_final_dir))\n",
    "tokenizer.save_pretrained(str(run_b_final_dir))\n",
    "\n",
    "(RUN_B_DIR / \"dev_overall_metrics.json\").write_text(\n",
    "    json.dumps(overall_b, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(RUN_B_DIR / \"dev_classification_report.json\").write_text(\n",
    "    json.dumps(report_b, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "if not loss_curve_b.empty:\n",
    "    loss_curve_b.to_csv(RUN_B_DIR / \"training_loss_curve.csv\", index=False)\n",
    "\n",
    "print(f\"Saved Run B model and metrics under: {RUN_B_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Per-Entity Metrics\n",
    "\n",
    "Use per-entity precision/recall/F1 with support counts in mind:\n",
    "- High-support classes (especially SCHOLAR) are expected to be more stable.\n",
    "- Very low-support classes (BOOK, HADITH_REF, some I- tags) can show volatile scores.\n",
    "- If weighted loss helps, you usually see recall gains on rare classes, sometimes with a precision tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Run A (standard)\": overall_a,\n",
    "        \"Run B (weighted)\": overall_b,\n",
    "    }\n",
    ")\n",
    "comparison_df\n",
    "\n",
    "comparison_csv_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison.csv\"\n",
    "comparison_json_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison.json\"\n",
    "\n",
    "comparison_payload = {\n",
    "    \"model_name\": model_name,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"run_a_output_dir\": str(RUN_A_DIR),\n",
    "    \"run_b_output_dir\": str(RUN_B_DIR),\n",
    "    \"run_a_overall\": overall_a,\n",
    "    \"run_b_overall\": overall_b,\n",
    "    \"class_weights_normalized\": normalized_weights,\n",
    "}\n",
    "\n",
    "comparison_df.to_csv(comparison_csv_path)\n",
    "comparison_json_path.write_text(\n",
    "    json.dumps(comparison_payload, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Comparison table saved to:\", comparison_csv_path)\n",
    "print(\"Comparison summary saved to:\", comparison_json_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
