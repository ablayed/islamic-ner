{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Islamic NER Training (Silver Data, AraBERT v02)\n",
    "\n",
    "This notebook trains and compares two token-classification setups on the silver dataset:\n",
    "- **Run A**: standard cross-entropy\n",
    "- **Run B**: class-weighted cross-entropy\n",
    "\n",
    "Both runs use the same tokenizer/model backbone and the same subword alignment strategy as the ANERCorp baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Two Training Runs? (Ablation Study)\n",
    "\n",
    "We run two experiments to isolate the impact of **class weighting** under strong label imbalance.\n",
    "\n",
    "- Run A gives the reference behavior with no weighting.\n",
    "- Run B increases the loss contribution of rare classes (BOOK, HADITH_REF, some I- tags).\n",
    "\n",
    "Class weights can improve recall for rare entities by penalizing mistakes on those classes more heavily.\n",
    "Given this silver distribution, expect **SCHOLAR** to be strongest, while **BOOK/HADITH_REF** may stay weaker due to very low support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fd16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\diaab\\islamic-ner\n",
      "Torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def ensure_package(import_name: str, pip_name: str | None = None) -> None:\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        return\n",
    "    except ImportError:\n",
    "        pkg = pip_name or import_name\n",
    "        print(f\"Installing missing package: {pkg} (python: {sys.executable})\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "\n",
    "# Install into the active notebook kernel only if missing.\n",
    "ensure_package(\"evaluate\")\n",
    "ensure_package(\"seqeval\")\n",
    "ensure_package(\"datasets\")\n",
    "ensure_package(\"transformers\")\n",
    "ensure_package(\"accelerate\")\n",
    "ensure_package(\"matplotlib\")\n",
    "ensure_package(\"pandas\")\n",
    "ensure_package(\"numpy\")\n",
    "ensure_package(\"torch\")\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"data\").exists() and (candidate / \"notebooks\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_project_root(Path.cwd().resolve())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Torch device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6165eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: C:\\Users\\diaab\\islamic-ner\\data\\silver\\train.json\n",
      "Dev path: C:\\Users\\diaab\\islamic-ner\\data\\silver\\dev.json\n",
      "Model: aubmindlab/bert-base-arabertv02\n",
      "MAX_SEQ_LENGTH: 128\n",
      "Label mapping: {'O': 0, 'B-SCHOLAR': 1, 'I-SCHOLAR': 2, 'B-BOOK': 3, 'I-BOOK': 4, 'B-CONCEPT': 5, 'I-CONCEPT': 6, 'B-PLACE': 7, 'I-PLACE': 8, 'B-HADITH_REF': 9, 'I-HADITH_REF': 10}\n"
     ]
    }
   ],
   "source": [
    "SILVER_TRAIN_PATH = ROOT / \"data\" / \"silver\" / \"train.json\"\n",
    "SILVER_DEV_PATH = ROOT / \"data\" / \"silver\" / \"dev.json\"\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "labels = [\n",
    "    \"O\",\n",
    "    \"B-SCHOLAR\", \"I-SCHOLAR\",\n",
    "    \"B-BOOK\", \"I-BOOK\",\n",
    "    \"B-CONCEPT\", \"I-CONCEPT\",\n",
    "    \"B-PLACE\", \"I-PLACE\",\n",
    "    \"B-HADITH_REF\", \"I-HADITH_REF\",\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Train path:\", SILVER_TRAIN_PATH)\n",
    "print(\"Dev path:\", SILVER_DEV_PATH)\n",
    "print(\"Model:\", model_name)\n",
    "print(\"MAX_SEQ_LENGTH:\", MAX_SEQ_LENGTH)\n",
    "print(\"Label mapping:\", label2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad7b11",
   "metadata": {},
   "source": [
    "## Tokenization Note (AraBERT Preprocessing)\n",
    "\n",
    "AraBERT v02 can benefit from `ArabertPreprocessor` pre-segmentation.\n",
    "For this notebook, we intentionally **skip preprocessing** and use the raw tokenizer input so we can measure it later as a separate ablation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949bbba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 2969\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 360\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_count</th>\n",
       "      <th>dev_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>125158</td>\n",
       "      <td>15583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-SCHOLAR</td>\n",
       "      <td>14053</td>\n",
       "      <td>1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-SCHOLAR</td>\n",
       "      <td>20228</td>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-BOOK</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-BOOK</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-CONCEPT</td>\n",
       "      <td>767</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-CONCEPT</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PLACE</td>\n",
       "      <td>356</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PLACE</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-HADITH_REF</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-HADITH_REF</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  train_count  dev_count\n",
       "0              O       125158      15583\n",
       "1      B-SCHOLAR        14053       1763\n",
       "2      I-SCHOLAR        20228       2411\n",
       "3         B-BOOK           50          0\n",
       "4         I-BOOK           51          0\n",
       "5      B-CONCEPT          767         92\n",
       "6      I-CONCEPT            6          0\n",
       "7        B-PLACE          356         55\n",
       "8        I-PLACE            8          6\n",
       "9   B-HADITH_REF           75          0\n",
       "10  I-HADITH_REF          182          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_silver_split(path: Path) -> Dataset:\n",
    "    records = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    cleaned = []\n",
    "    for i, record in enumerate(records):\n",
    "        tokens = record.get(\"tokens\") or []\n",
    "        tags = record.get(\"ner_tags\") or []\n",
    "        if not isinstance(tokens, list) or not isinstance(tags, list):\n",
    "            continue\n",
    "        n = min(len(tokens), len(tags))\n",
    "        if n == 0:\n",
    "            continue\n",
    "        cleaned.append(\n",
    "            {\n",
    "                \"id\": record.get(\"id\", f\"{path.stem}_{i}\"),\n",
    "                \"tokens\": tokens[:n],\n",
    "                \"ner_tags\": tags[:n],\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(cleaned)\n",
    "\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": load_silver_split(SILVER_TRAIN_PATH),\n",
    "        \"dev\": load_silver_split(SILVER_DEV_PATH),\n",
    "    }\n",
    ")\n",
    "print(dataset_dict)\n",
    "\n",
    "\n",
    "def count_labels(split_dataset: Dataset) -> Counter:\n",
    "    counts = Counter()\n",
    "    for row in split_dataset:\n",
    "        counts.update(row[\"ner_tags\"])\n",
    "    return counts\n",
    "\n",
    "\n",
    "train_counts = count_labels(dataset_dict[\"train\"])\n",
    "dev_counts = count_labels(dataset_dict[\"dev\"])\n",
    "\n",
    "unknown_train = sorted(set(train_counts) - set(labels))\n",
    "unknown_dev = sorted(set(dev_counts) - set(labels))\n",
    "assert not unknown_train, f\"Unknown labels in train: {unknown_train}\"\n",
    "assert not unknown_dev, f\"Unknown labels in dev: {unknown_dev}\"\n",
    "\n",
    "dist_df = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"train_count\": [train_counts.get(lbl, 0) for lbl in labels],\n",
    "    \"dev_count\": [dev_counts.get(lbl, 0) for lbl in labels],\n",
    "})\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e279677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c322fcf1958454b801b8552d23723a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels:   0%|          | 0/2969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f087f34d5174cab84bffac22cd007ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize + align labels:   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2969\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples: Dict[str, List[List[str]]]) -> Dict[str, List[List[int]]]:\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, word_level_tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword keeps the original word label.\n",
    "                label_ids.append(label2id[word_level_tags[word_idx]])\n",
    "            else:\n",
    "                # Non-first subwords are ignored in the loss/metrics.\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    desc=\"Tokenize + align labels\",\n",
    ")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcaf90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval_metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def decode_predictions(pred_ids: np.ndarray, label_ids: np.ndarray) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    y_true: List[List[str]] = []\n",
    "    y_pred: List[List[str]] = []\n",
    "\n",
    "    for pred_row, label_row in zip(pred_ids, label_ids):\n",
    "        row_true: List[str] = []\n",
    "        row_pred: List[str] = []\n",
    "        for pred_id, label_id in zip(pred_row, label_row):\n",
    "            if int(label_id) == -100:\n",
    "                continue\n",
    "            row_true.append(id2label[int(label_id)])\n",
    "            row_pred.append(id2label[int(pred_id)])\n",
    "        y_true.append(row_true)\n",
    "        y_pred.append(row_pred)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    logits, labels_arr = eval_pred\n",
    "    pred_ids = np.argmax(logits, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, labels_arr)\n",
    "\n",
    "    scores = seqeval_metric.compute(predictions=y_pred, references=y_true)\n",
    "    return {\n",
    "        \"precision\": float(scores[\"overall_precision\"]),\n",
    "        \"recall\": float(scores[\"overall_recall\"]),\n",
    "        \"f1\": float(scores[\"overall_f1\"]),\n",
    "        \"accuracy\": float(scores[\"overall_accuracy\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_and_print(trainer: Trainer, dataset, run_name: str):\n",
    "    pred_output = trainer.predict(dataset)\n",
    "    pred_ids = np.argmax(pred_output.predictions, axis=2)\n",
    "    y_true, y_pred = decode_predictions(pred_ids, pred_output.label_ids)\n",
    "\n",
    "    overall = {\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "    }\n",
    "    report_text = classification_report(y_true, y_pred, digits=4)\n",
    "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    print(f\"[{run_name}] Overall Precision: {overall['precision']:.4f}\")\n",
    "    print(f\"[{run_name}] Overall Recall:    {overall['recall']:.4f}\")\n",
    "    print(f\"[{run_name}] Overall F1:        {overall['f1']:.4f}\\n\")\n",
    "    print(report_text)\n",
    "\n",
    "    return overall, report_dict, pred_output\n",
    "\n",
    "\n",
    "def plot_training_loss(trainer: Trainer, run_name: str) -> pd.DataFrame:\n",
    "    loss_rows = []\n",
    "    for row in trainer.state.log_history:\n",
    "        if \"loss\" in row and \"eval_loss\" not in row and \"epoch\" in row:\n",
    "            loss_rows.append({\"epoch\": float(row[\"epoch\"]), \"loss\": float(row[\"loss\"])})\n",
    "\n",
    "    if not loss_rows:\n",
    "        print(f\"No training loss logs found for {run_name}.\")\n",
    "        return pd.DataFrame(columns=[\"epoch\", \"loss\"])\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_rows).sort_values(\"epoch\")\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(loss_df[\"epoch\"], loss_df[\"loss\"], marker=\"o\")\n",
    "    plt.title(f\"Training Loss Curve - {run_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return loss_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fdb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_A_DIR = ROOT / \"models\" / \"islamic_ner_standard\"\n",
    "RUN_B_DIR = ROOT / \"models\" / \"islamic_ner_weighted\"\n",
    "RUN_A_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUN_B_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_training_args(output_dir: Path) -> TrainingArguments:\n",
    "    kwargs = {\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_steps\": 25,\n",
    "        \"seed\": SEED,\n",
    "        \"report_to\": \"none\",\n",
    "    }\n",
    "\n",
    "    # Transformers renamed evaluation_strategy -> eval_strategy in newer releases.\n",
    "    signature = inspect.signature(TrainingArguments.__init__)\n",
    "    if \"eval_strategy\" in signature.parameters:\n",
    "        kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "    else:\n",
    "        kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "\n",
    "    return TrainingArguments(**kwargs)\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbab8f",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "\n",
    "For Run B, class weights are computed from train-token frequencies:\n",
    "\n",
    "`weight_i = total_tokens / (num_classes * count_i)`\n",
    "\n",
    "Then all weights are normalized so the `O` class is near `1.0`.\n",
    "This keeps weighting interpretable while upweighting rare entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c077a287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "      <th>raw_weight</th>\n",
       "      <th>normalized_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>125158</td>\n",
       "      <td>0.116895</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-SCHOLAR</td>\n",
       "      <td>14053</td>\n",
       "      <td>1.041085</td>\n",
       "      <td>8.906141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-SCHOLAR</td>\n",
       "      <td>20228</td>\n",
       "      <td>0.723273</td>\n",
       "      <td>6.187364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-BOOK</td>\n",
       "      <td>50</td>\n",
       "      <td>292.607273</td>\n",
       "      <td>2503.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-BOOK</td>\n",
       "      <td>51</td>\n",
       "      <td>286.869875</td>\n",
       "      <td>2454.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-CONCEPT</td>\n",
       "      <td>767</td>\n",
       "      <td>19.074790</td>\n",
       "      <td>163.178618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-CONCEPT</td>\n",
       "      <td>6</td>\n",
       "      <td>2438.393939</td>\n",
       "      <td>20859.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PLACE</td>\n",
       "      <td>356</td>\n",
       "      <td>41.096527</td>\n",
       "      <td>351.567416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PLACE</td>\n",
       "      <td>8</td>\n",
       "      <td>1828.795455</td>\n",
       "      <td>15644.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-HADITH_REF</td>\n",
       "      <td>75</td>\n",
       "      <td>195.071515</td>\n",
       "      <td>1668.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-HADITH_REF</td>\n",
       "      <td>182</td>\n",
       "      <td>80.386613</td>\n",
       "      <td>687.681319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label   count   raw_weight  normalized_weight\n",
       "0              O  125158     0.116895           1.000000\n",
       "1      B-SCHOLAR   14053     1.041085           8.906141\n",
       "2      I-SCHOLAR   20228     0.723273           6.187364\n",
       "3         B-BOOK      50   292.607273        2503.160000\n",
       "4         I-BOOK      51   286.869875        2454.078431\n",
       "5      B-CONCEPT     767    19.074790         163.178618\n",
       "6      I-CONCEPT       6  2438.393939       20859.666667\n",
       "7        B-PLACE     356    41.096527         351.567416\n",
       "8        I-PLACE       8  1828.795455       15644.750000\n",
       "9   B-HADITH_REF      75   195.071515        1668.773333\n",
       "10  I-HADITH_REF     182    80.386613         687.681319"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_counts = Counter()\n",
    "for row in dataset_dict[\"train\"]:\n",
    "    train_label_counts.update(row[\"ner_tags\"])\n",
    "\n",
    "total_tokens = sum(train_label_counts.get(label, 0) for label in labels)\n",
    "num_classes = len(labels)\n",
    "\n",
    "raw_weights = {}\n",
    "for label in labels:\n",
    "    count_i = train_label_counts.get(label, 0)\n",
    "    if count_i == 0:\n",
    "        raw_weights[label] = 0.0\n",
    "    else:\n",
    "        raw_weights[label] = total_tokens / (num_classes * count_i)\n",
    "\n",
    "o_weight = raw_weights.get(\"O\", 1.0) or 1.0\n",
    "normalized_weights = {\n",
    "    label: (weight / o_weight if weight > 0 else 0.0)\n",
    "    for label, weight in raw_weights.items()\n",
    "}\n",
    "\n",
    "class_weights_tensor = torch.tensor([normalized_weights[label] for label in labels], dtype=torch.float)\n",
    "\n",
    "weights_df = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": labels,\n",
    "        \"count\": [train_label_counts.get(label, 0) for label in labels],\n",
    "        \"raw_weight\": [raw_weights[label] for label in labels],\n",
    "        \"normalized_weight\": [normalized_weights[label] for label in labels],\n",
    "    }\n",
    ")\n",
    "weights_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d5a89",
   "metadata": {},
   "source": [
    "## Run A: Standard Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145e6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d253b2fad053487e8eb49e940f0dd456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      2\u001b[39m training_args_a = make_training_args(RUN_A_DIR)\n\u001b[32m      4\u001b[39m trainer_a = Trainer(\n\u001b[32m      5\u001b[39m     model=model_a,\n\u001b[32m      6\u001b[39m     args=training_args_a,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_result_a = \u001b[43mtrainer_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m eval_result_a = trainer_a.evaluate(tokenized_datasets[\u001b[33m\"\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRun A train metrics:\u001b[39m\u001b[33m\"\u001b[39m, train_result_a.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3318\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3322\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3323\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3324\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3363\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   3361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3362\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3363\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3364\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3365\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1889\u001b[39m, in \u001b[36mBertForTokenClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1883\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1885\u001b[39m \u001b[33;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[32m   1886\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1887\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1895\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1896\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1897\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1901\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1903\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.dropout(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1139\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1154\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    683\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    684\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    685\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m         output_attentions,\n\u001b[32m    692\u001b[39m     )\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    623\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    624\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pytorch_utils.py:238\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    638\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_a = init_model()\n",
    "training_args_a = make_training_args(RUN_A_DIR)\n",
    "\n",
    "trainer_a = Trainer(\n",
    "    model=model_a,\n",
    "    args=training_args_a,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result_a = trainer_a.train()\n",
    "eval_result_a = trainer_a.evaluate(tokenized_datasets[\"dev\"])\n",
    "print(\"Run A train metrics:\", train_result_a.metrics)\n",
    "print(\"Run A eval metrics:\", eval_result_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793c16cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4715ff04e574da1bde27693650b7d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run A - Standard] Overall Precision: 0.0014\n",
      "[Run A - Standard] Overall Recall:    0.0058\n",
      "[Run A - Standard] Overall F1:        0.0022\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BOOK     0.0000    0.0000    0.0000         0\n",
      "     CONCEPT     0.0011    0.0326    0.0021        92\n",
      "  HADITH_REF     0.0000    0.0000    0.0000         0\n",
      "       PLACE     0.0014    0.0556    0.0027        54\n",
      "     SCHOLAR     0.0051    0.0029    0.0037      1749\n",
      "\n",
      "   micro avg     0.0014    0.0058    0.0022      1895\n",
      "   macro avg     0.0015    0.0182    0.0017      1895\n",
      "weighted avg     0.0048    0.0058    0.0036      1895\n",
      "\n",
      "No training loss logs found for Run A - Standard.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m tokenizer.save_pretrained(\u001b[38;5;28mstr\u001b[39m(run_a_final_dir))\n\u001b[32m     11\u001b[39m (RUN_A_DIR / \u001b[33m\"\u001b[39m\u001b[33mdev_overall_metrics.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(\n\u001b[32m     12\u001b[39m     json.dumps(overall_a, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     13\u001b[39m     encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m (RUN_A_DIR / \u001b[33m\"\u001b[39m\u001b[33mdev_classification_report.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m     17\u001b[39m     encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss_curve_a.empty:\n\u001b[32m     20\u001b[39m     loss_curve_a.to_csv(RUN_A_DIR / \u001b[33m\"\u001b[39m\u001b[33mtraining_loss_curve.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:202\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    200\u001b[39m chunks = \u001b[38;5;28mself\u001b[39m.iterencode(o, _one_shot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     chunks = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type int32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "overall_a, report_a, pred_output_a = evaluate_and_print(\n",
    "    trainer_a, tokenized_datasets[\"dev\"], \"Run A - Standard\"\n",
    ")\n",
    "loss_curve_a = plot_training_loss(trainer_a, \"Run A - Standard\")\n",
    "\n",
    "run_a_final_dir = RUN_A_DIR / \"final_model\"\n",
    "run_a_final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_a.save_model(str(run_a_final_dir))\n",
    "tokenizer.save_pretrained(str(run_a_final_dir))\n",
    "\n",
    "(RUN_A_DIR / \"dev_overall_metrics.json\").write_text(\n",
    "    json.dumps(overall_a, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(RUN_A_DIR / \"dev_classification_report.json\").write_text(\n",
    "    json.dumps(report_a, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "if not loss_curve_a.empty:\n",
    "    loss_curve_a.to_csv(RUN_A_DIR / \"training_loss_curve.csv\", index=False)\n",
    "\n",
    "print(f\"Saved Run A model and metrics under: {RUN_A_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91946d",
   "metadata": {},
   "source": [
    "## Run B: Class-Weighted Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56f246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7484b6c7c54feeb0695ceba3e475f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5913, 'grad_norm': 4.050981044769287, 'learning_rate': 2.9193548387096776e-05, 'epoch': 0.13}\n",
      "{'loss': 1.2358, 'grad_norm': 3.8853511810302734, 'learning_rate': 2.8387096774193552e-05, 'epoch': 0.27}\n",
      "{'loss': 0.9294, 'grad_norm': 5.556384086608887, 'learning_rate': 2.758064516129032e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6153, 'grad_norm': 7.997486114501953, 'learning_rate': 2.6774193548387097e-05, 'epoch': 0.54}\n",
      "{'loss': 0.581, 'grad_norm': 10.126968383789062, 'learning_rate': 2.5967741935483872e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6282, 'grad_norm': 8.523314476013184, 'learning_rate': 2.5161290322580648e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2682, 'grad_norm': 1.8890446424484253, 'learning_rate': 2.4354838709677417e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e3c5630b3643ce9c2b7ec1dc453fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12917840480804443, 'eval_precision': 0.6237809452363091, 'eval_recall': 0.8775725593667546, 'eval_f1': 0.7292260469195353, 'eval_accuracy': 0.917204902608961, 'eval_runtime': 32.0661, 'eval_samples_per_second': 11.227, 'eval_steps_per_second': 0.717, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2164, 'grad_norm': 11.489849090576172, 'learning_rate': 2.3548387096774193e-05, 'epoch': 1.08}\n",
      "{'loss': 0.1272, 'grad_norm': 1.0407789945602417, 'learning_rate': 2.274193548387097e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2498, 'grad_norm': 1.3018063306808472, 'learning_rate': 2.1935483870967744e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3541, 'grad_norm': 1.7459803819656372, 'learning_rate': 2.1129032258064516e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1344, 'grad_norm': 4.631195545196533, 'learning_rate': 2.032258064516129e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0633, 'grad_norm': 2.5732500553131104, 'learning_rate': 1.9516129032258064e-05, 'epoch': 1.75}\n",
      "{'loss': 0.2345, 'grad_norm': 0.9725518822669983, 'learning_rate': 1.870967741935484e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34061600768c4023891ba6270e21d31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07385812699794769, 'eval_precision': 0.7029580936729664, 'eval_recall': 0.9029023746701847, 'eval_f1': 0.7904827904827905, 'eval_accuracy': 0.9415653765956364, 'eval_runtime': 42.1563, 'eval_samples_per_second': 8.54, 'eval_steps_per_second': 0.546, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.376, 'grad_norm': 0.3282621204853058, 'learning_rate': 1.7903225806451616e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1313, 'grad_norm': 0.3646169602870941, 'learning_rate': 1.7096774193548388e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0882, 'grad_norm': 2.698906898498535, 'learning_rate': 1.629032258064516e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0785, 'grad_norm': 1.114035725593567, 'learning_rate': 1.5483870967741936e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0463, 'grad_norm': 0.7898567914962769, 'learning_rate': 1.467741935483871e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0398, 'grad_norm': 0.4270433187484741, 'learning_rate': 1.3870967741935484e-05, 'epoch': 2.69}\n",
      "{'loss': 0.0462, 'grad_norm': 11.269672393798828, 'learning_rate': 1.3064516129032258e-05, 'epoch': 2.82}\n",
      "{'loss': 0.1242, 'grad_norm': 0.25987058877944946, 'learning_rate': 1.2258064516129034e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a83adbb561c47618a97e5a37e98688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05719820782542229, 'eval_precision': 0.8046620046620047, 'eval_recall': 0.9108179419525066, 'eval_f1': 0.8544554455445544, 'eval_accuracy': 0.9637898591262778, 'eval_runtime': 21.4457, 'eval_samples_per_second': 16.787, 'eval_steps_per_second': 1.072, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0932, 'grad_norm': 1.0977059602737427, 'learning_rate': 1.1451612903225806e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0394, 'grad_norm': 0.4885452389717102, 'learning_rate': 1.0645161290322582e-05, 'epoch': 3.23}\n",
      "{'loss': 0.0258, 'grad_norm': 1.8905702829360962, 'learning_rate': 9.838709677419354e-06, 'epoch': 3.36}\n",
      "{'loss': 0.051, 'grad_norm': 0.2755037546157837, 'learning_rate': 9.03225806451613e-06, 'epoch': 3.49}\n",
      "{'loss': 0.0307, 'grad_norm': 0.2997608780860901, 'learning_rate': 8.225806451612904e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0402, 'grad_norm': 0.2557797133922577, 'learning_rate': 7.419354838709678e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0511, 'grad_norm': 0.5929769277572632, 'learning_rate': 6.612903225806452e-06, 'epoch': 3.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb06409a0967438b87f4b42bc8512ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05541658401489258, 'eval_precision': 0.8242481203007519, 'eval_recall': 0.9255936675461741, 'eval_f1': 0.8719860800397713, 'eval_accuracy': 0.9677058434623405, 'eval_runtime': 28.0968, 'eval_samples_per_second': 12.813, 'eval_steps_per_second': 0.819, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0319, 'grad_norm': 0.18408159911632538, 'learning_rate': 5.8064516129032256e-06, 'epoch': 4.03}\n",
      "{'loss': 0.0268, 'grad_norm': 0.19207881391048431, 'learning_rate': 4.9999999999999996e-06, 'epoch': 4.17}\n",
      "{'loss': 0.0269, 'grad_norm': 0.7222564816474915, 'learning_rate': 4.1935483870967736e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0278, 'grad_norm': 0.5594026446342468, 'learning_rate': 3.3870967741935484e-06, 'epoch': 4.44}\n",
      "{'loss': 0.0281, 'grad_norm': 0.26258233189582825, 'learning_rate': 2.580645161290323e-06, 'epoch': 4.57}\n",
      "{'loss': 0.0253, 'grad_norm': 0.08018475025892258, 'learning_rate': 1.774193548387097e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0579, 'grad_norm': 0.5902414917945862, 'learning_rate': 9.67741935483871e-07, 'epoch': 4.84}\n",
      "{'loss': 0.0618, 'grad_norm': 0.1893954873085022, 'learning_rate': 1.6129032258064518e-07, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb7b06437694eb9aaf2e053297f3765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04932594299316406, 'eval_precision': 0.8147806004618937, 'eval_recall': 0.9308707124010555, 'eval_f1': 0.8689655172413793, 'eval_accuracy': 0.9666378477343234, 'eval_runtime': 29.2405, 'eval_samples_per_second': 12.312, 'eval_steps_per_second': 0.787, 'epoch': 5.0}\n",
      "{'train_runtime': 5659.7476, 'train_samples_per_second': 2.623, 'train_steps_per_second': 0.164, 'train_loss': 0.23603373824428486, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5f05315d448bf93e41e3a3631d266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run B train metrics: {'train_runtime': 5659.7476, 'train_samples_per_second': 2.623, 'train_steps_per_second': 0.164, 'total_flos': 929459193068352.0, 'train_loss': 0.23603373824428486, 'epoch': 5.0}\n",
      "Run B eval metrics: {'eval_loss': 0.05541658401489258, 'eval_precision': 0.8242481203007519, 'eval_recall': 0.9255936675461741, 'eval_f1': 0.8719860800397713, 'eval_accuracy': 0.9677058434623405, 'eval_runtime': 29.3153, 'eval_samples_per_second': 12.28, 'eval_steps_per_second': 0.785, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if \"class_weights_tensor\" not in globals():\n",
    "    train_label_counts = Counter()\n",
    "    for row in dataset_dict[\"train\"]:\n",
    "        train_label_counts.update(row[\"ner_tags\"])\n",
    "\n",
    "    total_tokens = sum(train_label_counts.get(label, 0) for label in labels)\n",
    "    num_classes = len(labels)\n",
    "    raw_weights = {\n",
    "        label: (total_tokens / (num_classes * train_label_counts.get(label, 1)))\n",
    "        if train_label_counts.get(label, 0) > 0 else 0.0\n",
    "        for label in labels\n",
    "    }\n",
    "    o_weight = raw_weights.get(\"O\", 1.0) or 1.0\n",
    "    normalized_weights = {\n",
    "        label: (weight / o_weight if weight > 0 else 0.0)\n",
    "        for label, weight in raw_weights.items()\n",
    "    }\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [normalized_weights[label] for label in labels],\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels_tensor = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights.to(logits.device),\n",
    "            ignore_index=-100,\n",
    "        )\n",
    "        loss = loss_fn(logits.view(-1, model.config.num_labels), labels_tensor.view(-1))\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "model_b = init_model()\n",
    "training_args_b = make_training_args(RUN_B_DIR)\n",
    "\n",
    "trainer_b = WeightedTrainer(\n",
    "    model=model_b,\n",
    "    args=training_args_b,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "train_result_b = trainer_b.train()\n",
    "eval_result_b = trainer_b.evaluate(tokenized_datasets[\"dev\"])\n",
    "print(\"Run B train metrics:\", train_result_b.metrics)\n",
    "print(\"Run B eval metrics:\", eval_result_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9395b1e776c34fb78246903b0f960902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run B - Weighted] Overall Precision: 0.8242\n",
      "[Run B - Weighted] Overall Recall:    0.9256\n",
      "[Run B - Weighted] Overall F1:        0.8720\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CONCEPT     0.8911    0.9783    0.9326        92\n",
      "  HADITH_REF     0.0000    0.0000    0.0000         0\n",
      "       PLACE     0.8281    0.9815    0.8983        54\n",
      "     SCHOLAR     0.8219    0.9211    0.8687      1749\n",
      "\n",
      "   micro avg     0.8242    0.9256    0.8720      1895\n",
      "   macro avg     0.6353    0.7202    0.6749      1895\n",
      "weighted avg     0.8255    0.9256    0.8726      1895\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m tokenizer.save_pretrained(\u001b[38;5;28mstr\u001b[39m(run_b_final_dir))\n\u001b[32m     11\u001b[39m (RUN_B_DIR / \u001b[33m\"\u001b[39m\u001b[33mdev_overall_metrics.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(\n\u001b[32m     12\u001b[39m     json.dumps(overall_b, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     13\u001b[39m     encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m (RUN_B_DIR / \u001b[33m\"\u001b[39m\u001b[33mdev_classification_report.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m     17\u001b[39m     encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss_curve_b.empty:\n\u001b[32m     20\u001b[39m     loss_curve_b.to_csv(RUN_B_DIR / \u001b[33m\"\u001b[39m\u001b[33mtraining_loss_curve.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:202\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    200\u001b[39m chunks = \u001b[38;5;28mself\u001b[39m.iterencode(o, _one_shot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     chunks = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type int32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "overall_b, report_b, pred_output_b = evaluate_and_print(\n",
    "    trainer_b, tokenized_datasets[\"dev\"], \"Run B - Weighted\"\n",
    ")\n",
    "loss_curve_b = plot_training_loss(trainer_b, \"Run B - Weighted\")\n",
    "\n",
    "run_b_final_dir = RUN_B_DIR / \"final_model\"\n",
    "run_b_final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_b.save_model(str(run_b_final_dir))\n",
    "tokenizer.save_pretrained(str(run_b_final_dir))\n",
    "\n",
    "(RUN_B_DIR / \"dev_overall_metrics.json\").write_text(\n",
    "    json.dumps(overall_b, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(RUN_B_DIR / \"dev_classification_report.json\").write_text(\n",
    "    json.dumps(report_b, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "if not loss_curve_b.empty:\n",
    "    loss_curve_b.to_csv(RUN_B_DIR / \"training_loss_curve.csv\", index=False)\n",
    "\n",
    "print(f\"Saved Run B model and metrics under: {RUN_B_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Per-Entity Metrics\n",
    "\n",
    "Use per-entity precision/recall/F1 with support counts in mind:\n",
    "- High-support classes (especially SCHOLAR) are expected to be more stable.\n",
    "- Very low-support classes (BOOK, HADITH_REF, some I- tags) can show volatile scores.\n",
    "- If weighted loss helps, you usually see recall gains on rare classes, sometimes with a precision tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison table saved to: C:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_ablation_comparison.csv\n",
      "Comparison summary saved to: C:\\Users\\diaab\\islamic-ner\\models\\islamic_ner_ablation_comparison.json\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Run A (standard)\": overall_a,\n",
    "        \"Run B (weighted)\": overall_b,\n",
    "    }\n",
    ")\n",
    "comparison_df\n",
    "\n",
    "comparison_csv_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison.csv\"\n",
    "comparison_json_path = ROOT / \"models\" / \"islamic_ner_ablation_comparison.json\"\n",
    "\n",
    "comparison_payload = {\n",
    "    \"model_name\": model_name,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"run_a_output_dir\": str(RUN_A_DIR),\n",
    "    \"run_b_output_dir\": str(RUN_B_DIR),\n",
    "    \"run_a_overall\": overall_a,\n",
    "    \"run_b_overall\": overall_b,\n",
    "    \"class_weights_normalized\": normalized_weights,\n",
    "}\n",
    "\n",
    "comparison_df.to_csv(comparison_csv_path)\n",
    "comparison_json_path.write_text(\n",
    "    json.dumps(comparison_payload, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Comparison table saved to:\", comparison_csv_path)\n",
    "print(\"Comparison summary saved to:\", comparison_json_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
