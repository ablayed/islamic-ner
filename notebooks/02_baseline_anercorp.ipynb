{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - ANERCorp Baseline with AraBERT\n",
        "\n",
        "This notebook is a sanity-check baseline for Arabic NER using **ANERCorp** and **AraBERT** (`aubmindlab/bert-base-arabertv02`).\n",
        "\n",
        "We use it to validate that our token-classification setup is healthy before adapting the pipeline to Islamic-domain NER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is ANERCorp and why use it?\n",
        "\n",
        "ANERCorp is a standard Arabic NER benchmark with BIO labels for `PER`, `ORG`, `LOC`, and `MISC`.\n",
        "\n",
        "We use it here because it is a fast sanity check for:\n",
        "- data parsing,\n",
        "- token/label alignment,\n",
        "- training/evaluation wiring.\n",
        "\n",
        "Typical AraBERT results are often around the **80-90 F1** range (depending on split/setup).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b170dcc1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\Users\\diaab\\islamic-ner\n",
            "ANERCorp dir: C:\\Users\\diaab\\islamic-ner\\data\\raw\\anercorp\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def ensure_package(import_name: str, pip_name: str | None = None) -> None:\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        return\n",
        "    except ImportError:\n",
        "        pkg = pip_name or import_name\n",
        "        print(f\"Installing missing package: {pkg} (kernel python: {sys.executable})\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "\n",
        "# Install into the active notebook kernel interpreter if missing.\n",
        "ensure_package(\"evaluate\")\n",
        "ensure_package(\"seqeval\")\n",
        "ensure_package(\"datasets\")\n",
        "ensure_package(\"transformers\")\n",
        "ensure_package(\"accelerate\")\n",
        "\n",
        "import random\n",
        "import re\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# Optional if environment is missing packages:\n",
        "# !pip install -q transformers datasets evaluate seqeval accelerate\n",
        "\n",
        "ROOT = Path.cwd().resolve()\n",
        "if not (ROOT / \"src\").exists():\n",
        "    ROOT = ROOT.parent.resolve()\n",
        "\n",
        "DATA_DIR = ROOT / \"data\" / \"raw\" / \"anercorp\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"aubmindlab/bert-base-arabertv02\"\n",
        "OUTPUT_DIR = ROOT / \"models\" / \"anercorp_baseline\"\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"Project root:\", ROOT)\n",
        "print(\"ANERCorp dir:\", DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ef2c86",
      "metadata": {},
      "source": [
        "## 1) Download ANERCorp (HF first, known-source fallback)\n",
        "\n",
        "We try HuggingFace first with `datasets.load_dataset(\"anercorp\")` and similar names.\n",
        "If that fails, we try known sources and parse CoNLL files (`word<TAB>tag`, blank line between sentences).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5951b5c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded from HuggingFace source: MagedSaeed/ANERcorp_experimental\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tokens', 'ner_tags'],\n",
            "        num_rows: 3972\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tokens', 'ner_tags'],\n",
            "        num_rows: 924\n",
            "    })\n",
            "})\n",
            "Split sizes: {'train': 3972, 'test': 924}\n"
          ]
        }
      ],
      "source": [
        "HF_DATASET_CANDIDATES = [\n",
        "    \"anercorp\",\n",
        "    \"ANERcorp\",\n",
        "    \"asas-ai/ANERCorp\",\n",
        "    \"MagedSaeed/ANERcorp_experimental\",\n",
        "]\n",
        "\n",
        "KNOWN_SOURCE_PAGES = [\n",
        "    \"https://camel.abudhabi.nyu.edu/anercorp/\",\n",
        "    \"https://docs.google.com/forms/d/e/1FAIpQLSc4lNiI8eqjrsVQ0GBhgkctLCdLzBGshbdVWUogci3-wqnfoQ/viewform?usp=sf_link\",\n",
        "]\n",
        "\n",
        "KNOWN_DIRECT_FILE_URLS = [\n",
        "    # Add direct mirrors if needed.\n",
        "]\n",
        "\n",
        "\n",
        "def normalize_tag(tag: str) -> str:\n",
        "    tag = str(tag).strip()\n",
        "    if tag in {\"\", \"0\"}:\n",
        "        return \"O\"\n",
        "    tag = tag.replace(\"B-PERS\", \"B-PER\").replace(\"I-PERS\", \"I-PER\")\n",
        "    return tag\n",
        "\n",
        "\n",
        "def parse_conll_text(conll_text: str) -> List[Dict[str, List[str]]]:\n",
        "    sentences: List[Dict[str, List[str]]] = []\n",
        "    tokens: List[str] = []\n",
        "    tags: List[str] = []\n",
        "\n",
        "    for raw_line in conll_text.splitlines():\n",
        "        line = raw_line.strip()\n",
        "        if not line:\n",
        "            if tokens:\n",
        "                sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
        "                tokens, tags = [], []\n",
        "            continue\n",
        "\n",
        "        parts = re.split(r\"\\s+\", line)\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        tokens.append(parts[0])\n",
        "        tags.append(normalize_tag(parts[-1]))\n",
        "\n",
        "    if tokens:\n",
        "        sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def parse_conll_file(path: Path) -> List[Dict[str, List[str]]]:\n",
        "    return parse_conll_text(path.read_text(encoding=\"utf-8\", errors=\"replace\"))\n",
        "\n",
        "\n",
        "def maybe_load_hf_sentence_level(split) -> List[Dict[str, List[str]]]:\n",
        "    records: List[Dict[str, List[str]]] = []\n",
        "    if len(split) == 0:\n",
        "        return records\n",
        "\n",
        "    cols = set(split.column_names)\n",
        "    row0 = split[0]\n",
        "\n",
        "    token_key = next((k for k in [\"tokens\", \"words\", \"word\"] if k in cols), None)\n",
        "    tag_key = next((k for k in [\"ner_tags\", \"tags\", \"tag\", \"labels\"] if k in cols), None)\n",
        "    if token_key is None or tag_key is None:\n",
        "        return records\n",
        "\n",
        "    # Sentence-level already.\n",
        "    if isinstance(row0[token_key], list):\n",
        "        for row in split:\n",
        "            tokens = [str(x) for x in row[token_key]]\n",
        "            tags = [normalize_tag(x) for x in row[tag_key]]\n",
        "            if tokens and len(tokens) == len(tags):\n",
        "                records.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
        "        return records\n",
        "\n",
        "    # Token-level with sentence id.\n",
        "    sent_id_key = next(\n",
        "        (k for k in [\"sentence_id\", \"sent_id\", \"sid\", \"sentence\", \"doc_id\"] if k in cols),\n",
        "        None,\n",
        "    )\n",
        "    if sent_id_key is not None:\n",
        "        grouped: Dict[str, Dict[str, List[str]]] = {}\n",
        "        order: List[str] = []\n",
        "        for row in split:\n",
        "            sid = str(row[sent_id_key])\n",
        "            if sid not in grouped:\n",
        "                grouped[sid] = {\"tokens\": [], \"ner_tags\": []}\n",
        "                order.append(sid)\n",
        "            grouped[sid][\"tokens\"].append(str(row[token_key]))\n",
        "            grouped[sid][\"ner_tags\"].append(normalize_tag(row[tag_key]))\n",
        "        for sid in order:\n",
        "            if grouped[sid][\"tokens\"]:\n",
        "                records.append(grouped[sid])\n",
        "        return records\n",
        "\n",
        "    # Token-level without boundaries -> unsupported.\n",
        "    return []\n",
        "\n",
        "\n",
        "def convert_hf_to_dataset_dict(raw) -> DatasetDict | None:\n",
        "    if isinstance(raw, DatasetDict):\n",
        "        converted: Dict[str, Dataset] = {}\n",
        "        for split_name in raw.keys():\n",
        "            recs = maybe_load_hf_sentence_level(raw[split_name])\n",
        "            if recs:\n",
        "                converted[split_name] = Dataset.from_list(recs)\n",
        "        return DatasetDict(converted) if converted else None\n",
        "\n",
        "    recs = maybe_load_hf_sentence_level(raw)\n",
        "    if not recs:\n",
        "        return None\n",
        "    return DatasetDict({\"train\": Dataset.from_list(recs)})\n",
        "\n",
        "\n",
        "def try_load_from_hf() -> Tuple[DatasetDict | None, str | None, List[str]]:\n",
        "    errors: List[str] = []\n",
        "    for dataset_name in HF_DATASET_CANDIDATES:\n",
        "        try:\n",
        "            raw = load_dataset(dataset_name)\n",
        "            ds = convert_hf_to_dataset_dict(raw)\n",
        "            if ds is not None:\n",
        "                return ds, dataset_name, errors\n",
        "            errors.append(f\"{dataset_name}: unsupported format\")\n",
        "        except Exception as exc:\n",
        "            errors.append(f\"{dataset_name}: {type(exc).__name__}: {exc}\")\n",
        "    return None, None, errors\n",
        "\n",
        "\n",
        "def download_file(url: str, destination: Path) -> bool:\n",
        "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=60) as response:\n",
        "            response.raise_for_status()\n",
        "            with destination.open(\"wb\") as out:\n",
        "                for chunk in response.iter_content(chunk_size=1024 * 64):\n",
        "                    if chunk:\n",
        "                        out.write(chunk)\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"[skip] {url} -> {exc}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def extract_links(html: str, base_url: str) -> List[str]:\n",
        "    hrefs = re.findall(r'href=[\"\\']([^\"\\']+)[\"\\']', html)\n",
        "    links = [urljoin(base_url, href) for href in hrefs]\n",
        "    return list(dict.fromkeys(links))\n",
        "\n",
        "\n",
        "def try_known_source_download() -> DatasetDict | None:\n",
        "    local_train = [\n",
        "        DATA_DIR / \"train.txt\",\n",
        "        DATA_DIR / \"anercorp_train.txt\",\n",
        "        DATA_DIR / \"train.conll\",\n",
        "    ]\n",
        "    local_test = [\n",
        "        DATA_DIR / \"test.txt\",\n",
        "        DATA_DIR / \"anercorp_test.txt\",\n",
        "        DATA_DIR / \"test.conll\",\n",
        "    ]\n",
        "\n",
        "    train_path = next((p for p in local_train if p.exists()), None)\n",
        "    test_path = next((p for p in local_test if p.exists()), None)\n",
        "    if train_path and test_path:\n",
        "        train_records = parse_conll_file(train_path)\n",
        "        test_records = parse_conll_file(test_path)\n",
        "        if train_records and test_records:\n",
        "            return DatasetDict({\n",
        "                \"train\": Dataset.from_list(train_records),\n",
        "                \"test\": Dataset.from_list(test_records),\n",
        "            })\n",
        "\n",
        "    downloaded: List[Path] = []\n",
        "\n",
        "    for url in KNOWN_DIRECT_FILE_URLS:\n",
        "        name = url.split(\"?\")[0].split(\"/\")[-1] or \"anercorp_file\"\n",
        "        out = DATA_DIR / name\n",
        "        if download_file(url, out):\n",
        "            downloaded.append(out)\n",
        "\n",
        "    for page_url in KNOWN_SOURCE_PAGES:\n",
        "        try:\n",
        "            html = requests.get(page_url, timeout=30).text\n",
        "        except Exception as exc:\n",
        "            print(f\"[skip-page] {page_url} -> {exc}\")\n",
        "            continue\n",
        "\n",
        "        for link in extract_links(html, page_url):\n",
        "            low = link.lower()\n",
        "            if not (low.endswith(\".txt\") or low.endswith(\".conll\") or low.endswith(\".zip\") or \"download\" in low):\n",
        "                continue\n",
        "            name = link.split(\"?\")[0].split(\"/\")[-1] or \"anercorp_download\"\n",
        "            out = DATA_DIR / name\n",
        "            if download_file(link, out):\n",
        "                downloaded.append(out)\n",
        "\n",
        "    train_records: List[Dict[str, List[str]]] = []\n",
        "    test_records: List[Dict[str, List[str]]] = []\n",
        "\n",
        "    for path in downloaded:\n",
        "        if path.suffix.lower() in {\".txt\", \".conll\"}:\n",
        "            records = parse_conll_file(path)\n",
        "            name = path.name.lower()\n",
        "            if \"train\" in name:\n",
        "                train_records.extend(records)\n",
        "            elif \"test\" in name:\n",
        "                test_records.extend(records)\n",
        "        elif path.suffix.lower() == \".zip\":\n",
        "            extract_dir = DATA_DIR / f\"{path.stem}_extracted\"\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "            try:\n",
        "                with zipfile.ZipFile(path, \"r\") as zf:\n",
        "                    zf.extractall(extract_dir)\n",
        "                for txt in extract_dir.rglob(\"*.txt\"):\n",
        "                    records = parse_conll_file(txt)\n",
        "                    name = txt.name.lower()\n",
        "                    if \"train\" in name:\n",
        "                        train_records.extend(records)\n",
        "                    elif \"test\" in name:\n",
        "                        test_records.extend(records)\n",
        "            except Exception as exc:\n",
        "                print(f\"[skip-zip] {path} -> {exc}\")\n",
        "\n",
        "    if train_records and test_records:\n",
        "        return DatasetDict({\n",
        "            \"train\": Dataset.from_list(train_records),\n",
        "            \"test\": Dataset.from_list(test_records),\n",
        "        })\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "dataset_dict, hf_source_name, hf_errors = try_load_from_hf()\n",
        "\n",
        "if dataset_dict is not None:\n",
        "    print(f\"Loaded from HuggingFace source: {hf_source_name}\")\n",
        "else:\n",
        "    print(\"HF loading failed, trying known-source fallback...\")\n",
        "    for err in hf_errors:\n",
        "        print(\" -\", err)\n",
        "    dataset_dict = try_known_source_download()\n",
        "\n",
        "if dataset_dict is None:\n",
        "    raise RuntimeError(\n",
        "        \"Could not load ANERCorp from HF or known-source fallback. \"\n",
        "        \"Place train/test CoNLL files in data/raw/anercorp/ and retry.\"\n",
        "    )\n",
        "\n",
        "print(dataset_dict)\n",
        "print(\"Split sizes:\", {k: len(v) for k, v in dataset_dict.items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97710c3a",
      "metadata": {},
      "source": [
        "## 2) Dataset exploration\n",
        "\n",
        "We inspect:\n",
        "- number of sentences and tokens,\n",
        "- entity type distribution,\n",
        "- random annotated sentence samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "909a1a4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using splits: {'train': 3972, 'test': 924}\n",
            "Sentences: 4896\n",
            "Tokens: 150072\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity_type</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PER</td>\n",
              "      <td>6421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ORG</td>\n",
              "      <td>3416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>5052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MISC</td>\n",
              "      <td>1662</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  entity_type  count\n",
              "0         PER   6421\n",
              "1         ORG   3416\n",
              "2         LOC   5052\n",
              "3        MISC   1662"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample annotated sentences:\n",
            "ÙˆÙÙŠ/O Ø¶ÙˆØ¡/O Ù…Ø¬Ù…Ù„/O Ø¢Ø±Ø§Ø¦Ù‡/O Ø­ÙˆÙ„Ù‡Ø§/O ØŒ/O Ù…Ù†/O Ø§Ù„ØµØ¹Ø¨/O Ø§Ø¹ØªØ¨Ø§Ø±Ù‡/O Ø¹Ø¯ÙˆØ§/O Ø§ÙŠØ¯ÙŠÙˆÙ„ÙˆØ¬ÙŠØ§/O Ù„Ù‡Ø§/O Ù…Ø«Ù„/O Ø§Ù„Ø¹Ù‚Ø§Ø¯/B-PER ØŒ/O ÙÙ‡Ùˆ/O ÙÙŠ/O Ø§Ù„ÙˆØ§Ù‚Ø¹/O ØµØ¯ÙŠÙ‚/O Ù„Ù‡Ø§/O ØŒ/O Ø£Ùˆ/O Ù†ØµÙŠØ±/O ØŒ/O ÙˆØ§Ù†/O Ø¬Ø§Ø²/O ÙÙŠ/O Ù‡Ø°Ø§/O Ø§Ù„Ø§Ø·Ø§Ø±/O Ø§Ø³ØªØ¯Ø¹Ø§Ø¡/O Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©/O Ø§Ù„ØªØ±Ø§Ø«ÙŠØ©/O Ø§Ù„Ù…Ø£Ø«ÙˆØ±Ø©/O :/O '/O\n",
            "--------------------------------------------------------------------------------\n",
            "ÙˆØ£Ø¶Ø§Ù/O Ø£Ù†/O Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª/O Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†ÙŠØ©/O Ù€/O Ø§Ù„Ù…ØµØ±ÙŠØ©/O ÙˆØ·ÙŠØ¯Ø©/O ÙˆÙ…ØªÙ…ÙŠØ²Ø©/O ÙÙŠ/O Ø¬Ù…ÙŠØ¹/O Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª/O ,/O ÙˆØ£Ø´Ø§Ø±/O Ø¥Ù„ÙŠ/O Ø£Ù†/O Ù‚Ù…ØªÙ‡/O Ù…Ø¹/O Ø§Ù„Ø±Ø¦ÙŠØ³/O Ù…Ø¨Ø§Ø±Ùƒ/B-PER ØªÙ†Ø§ÙˆÙ„Øª/O Ø§Ù„ØªØ¹Ø§ÙˆÙ†/O Ø¨ÙŠÙ†/O Ø§Ù„Ø¨Ù„Ø¯ÙŠÙ†/O Ø§Ù„Ø´Ù‚ÙŠÙ‚ÙŠÙ†/O ,/O ÙˆØ§Ø³ØªØ¹Ø±Ø§Ø¶/O Ù…Ø§/O ØªÙ…Ø±/O Ø¨Ù‡/O Ø§Ù„Ù…Ù†Ø·Ù‚Ø©/O Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©/O Ù…Ù†/O ØªØ·ÙˆØ±Ø§Øª/O Ø³ÙˆØ§Ø¡/O Ø¹Ù„ÙŠ/O\n",
            "--------------------------------------------------------------------------------\n",
            "ÙˆÙÙŠ/O ØªØ¨Ø±ÙŠØ±Ù‡/O Ù„Ù„Ø¹Ù…Ù„ÙŠØ©/O ;/O Ù‚Ø§Ù„/O Ø§Ù„Ø¬ÙŠØ´/O Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠ/O Ø¥Ù†Ù‡/O Ù†ÙØ°Ù‡Ø§/O Ø¨Ù‡Ø¯Ù/O Ù…Ù†Ø¹/O ØªÙ‡Ø±ÙŠØ¨/O Ø£Ø³Ù„Ø­Ø©/O Ù…Ù†/O Ø³ÙˆØ±ÙŠØ§/B-LOC ÙˆØ¥ÙŠØ±Ø§Ù†/B-LOC Ø¥Ù„ÙŠ/O Ø­Ø²Ø¨/B-ORG Ø§Ù„Ù„Ù‡/I-ORG ./O\n",
            "--------------------------------------------------------------------------------\n",
            "ÙˆÙ…Ù†/O Ø§Ù„Ø´ÙˆØ§Ù‡Ø¯/O Ø§Ù„ØªÙŠ/O Ù†Ø¹ØªÙ‚Ø¯/O Ø£Ù†/O Ø£ØµØ­Ø§Ø¨Ù‡Ø§/O Ù‚Ø¯/O ÙŠÙƒÙˆÙ†ÙˆÙ†/O Ø¹Ø±Ø¨Ø§Ù‹/O Ù‚Ø¨Ø±/O (/O Ù…Ø­Ù…ÙˆØ¯/B-PER Ø¨Ù†/I-PER Ù…Ø­Ù…Ø¯/I-PER Ø¨Ù†/I-PER Ø£Ø¨Ùˆ/I-PER Ø§Ù„Ø¨ØµÙ„/I-PER Ø§Ù„Ø¬ÙŠÙ„Ø§Ù†ÙŠ/I-PER Ø§Ù„Ù…ØªÙˆÙÙ‰/O ÙÙŠ/O Ø§Ù„Ø«Ø§Ù…Ù†/O Ù…Ù†/O Ø¬Ù…Ø§Ø¯/O Ø§Ù„Ø¢Ø®Ø±/O Ø³Ù†Ø©/O Ø§Ø«Ù†ÙŠÙ†/O ÙˆØ®Ù…Ø³ÙŠÙ†/O ÙˆØ³Ø¨Ø¹Ù…Ø§Ø¦Ø©/O (/O Ø§Ù„Ù…ÙˆØ§ÙÙ‚/O 1351Ù…/O )/O ÙˆÙ‡Ùˆ/O Ø´Ø§Ù‡Ø¯/O Ù…ÙƒØªÙ…Ù„/O\n",
            "--------------------------------------------------------------------------------\n",
            "ÙˆÙ‚Ø§Ù„/O ÙŠÙˆØ³Ù/B-PER \"/O Ù„Ø£Ù†/O Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª/O Ø§Ù„Ø¥ØµØ§Ø¨Ø©/O Ø¨Ù†ÙˆØ¨Ø§Øª/O Ù‚Ù„Ø¨ÙŠØ©/O Ù…Ø±ØªØ¨Ø·Ø©/O Ø¨Ø§Ù„ØªØ¯Ø®ÙŠÙ†/O ØªØªÙ‚Ù„Øµ/O Ø¨Ù‚ÙˆØ©/O Ø¨Ø¹Ø¯/O Ø§Ù„ØªÙˆÙ‚Ù/O Ø¹Ù†/O Ø§Ù„ØªØ¯Ø®ÙŠÙ†/O ,/O ÙØ¥Ù†/O Ø¬Ù‡ÙˆØ¯/O Ù‡ÙŠØ¦Ø§Øª/O Ø§Ù„ØµØ­Ø©/O Ø§Ù„Ø¹Ø§Ù…Ø©/O Ù„Ù…Ù†Ø¹/O Ø§Ù„Ù†Ø§Ø³/O Ù…Ù†/O Ø¨Ø¯Ø¡/O ØªÙ„Ùƒ/O Ø§Ù„Ø¹Ø§Ø¯Ø©/O ÙˆØ§Ù„ØªØ±ÙˆÙŠØ¬/O Ù„Ù„Ø¥Ù‚Ù„Ø§Ø¹/O Ø¹Ù†Ù‡Ø§/O Ø³ÙŠÙƒÙˆÙ†/O Ù„Ù‡Ø§/O Ø£Ø«Ø±/O ÙƒØ¨ÙŠØ±/O\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "if \"test\" not in dataset_dict:\n",
        "    split = dataset_dict[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
        "    dataset_dict = DatasetDict({\"train\": split[\"train\"], \"test\": split[\"test\"]})\n",
        "\n",
        "print(\"Using splits:\", {k: len(v) for k, v in dataset_dict.items()})\n",
        "\n",
        "all_rows = list(dataset_dict[\"train\"]) + list(dataset_dict[\"test\"])\n",
        "sentence_count = len(all_rows)\n",
        "token_count = sum(len(row[\"tokens\"]) for row in all_rows)\n",
        "\n",
        "print(\"Sentences:\", sentence_count)\n",
        "print(\"Tokens:\", token_count)\n",
        "\n",
        "entity_counter = Counter()\n",
        "for row in all_rows:\n",
        "    for tag in row[\"ner_tags\"]:\n",
        "        if tag == \"O\":\n",
        "            continue\n",
        "        ent = tag.split(\"-\", 1)[1] if \"-\" in tag else tag\n",
        "        entity_counter[ent] += 1\n",
        "\n",
        "dist_df = pd.DataFrame([\n",
        "    {\"entity_type\": ent, \"count\": entity_counter.get(ent, 0)}\n",
        "    for ent in [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "])\n",
        "display(dist_df)\n",
        "\n",
        "print(\"Sample annotated sentences:\")\n",
        "for row in random.sample(all_rows, k=min(5, len(all_rows))):\n",
        "    pairs = [f\"{tok}/{tag}\" for tok, tag in list(zip(row[\"tokens\"], row[\"ner_tags\"]))[:35]]\n",
        "    print(\" \".join(pairs))\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e34a226",
      "metadata": {},
      "source": [
        "## 3) Fine-tune AraBERT for token classification\n",
        "\n",
        "### Subword-label alignment (why needed)\n",
        "\n",
        "BERT tokenizers split some words into multiple subword pieces.\n",
        "NER labels are word-level, so we align as follows:\n",
        "- first subword gets the BIO label,\n",
        "- remaining subwords get `-100` so the loss ignores them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "54681eda",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels: ['O', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27127a47a0ef40e195058c07a5c59b69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenize + align labels:   0%|          | 0/3972 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9414a8a6870d4b15949cd42efcbe50fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenize + align labels:   0%|          | 0/924 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3972\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 924\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list = sorted({tag for row in dataset_dict[\"train\"] for tag in row[\"ner_tags\"]})\n",
        "if \"O\" in label_list:\n",
        "    label_list = [\"O\"] + [label for label in label_list if label != \"O\"]\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(\"Labels:\", label_list)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "    aligned = []\n",
        "    for i in range(len(examples[\"tokens\"])):\n",
        "        word_ids = tokenized.word_ids(batch_index=i)\n",
        "        word_tags = examples[\"ner_tags\"][i]\n",
        "\n",
        "        prev_word = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != prev_word:\n",
        "                label_ids.append(label2id[word_tags[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            prev_word = word_idx\n",
        "\n",
        "        aligned.append(label_ids)\n",
        "\n",
        "    tokenized[\"labels\"] = aligned\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_dict[\"train\"].column_names,\n",
        "    desc=\"Tokenize + align labels\",\n",
        ")\n",
        "\n",
        "tokenized_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "86e8dd0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e668b4210214992bd11f795c62342ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/747 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.388, 'grad_norm': 0.6761423349380493, 'learning_rate': 2.7991967871485944e-05, 'epoch': 0.2}\n",
            "{'loss': 0.1061, 'grad_norm': 1.1202919483184814, 'learning_rate': 2.598393574297189e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0756, 'grad_norm': 0.9610965847969055, 'learning_rate': 2.397590361445783e-05, 'epoch': 0.6}\n",
            "{'loss': 0.0682, 'grad_norm': 0.5251806974411011, 'learning_rate': 2.1967871485943775e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a39db79727b247ef8881341b6b620284",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14318469166755676, 'eval_precision': 0.8203728967712597, 'eval_recall': 0.7898423817863398, 'eval_f1': 0.8048182020968103, 'eval_accuracy': 0.966074571931509, 'eval_runtime': 43.4543, 'eval_samples_per_second': 21.264, 'eval_steps_per_second': 1.335, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0622, 'grad_norm': 0.596483051776886, 'learning_rate': 1.9959839357429718e-05, 'epoch': 1.0}\n",
            "{'loss': 0.0502, 'grad_norm': 0.7444311380386353, 'learning_rate': 1.7951807228915664e-05, 'epoch': 1.2}\n",
            "{'loss': 0.0473, 'grad_norm': 0.2677379548549652, 'learning_rate': 1.5943775100401607e-05, 'epoch': 1.41}\n",
            "{'loss': 0.0412, 'grad_norm': 0.6272027492523193, 'learning_rate': 1.3935742971887551e-05, 'epoch': 1.61}\n",
            "{'loss': 0.0432, 'grad_norm': 0.8829789161682129, 'learning_rate': 1.1927710843373494e-05, 'epoch': 1.81}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2e14155ad584a08bbcaba4e60d086fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1338927149772644, 'eval_precision': 0.8319215336602764, 'eval_recall': 0.8169877408056042, 'eval_f1': 0.8243870112657389, 'eval_accuracy': 0.9704352696431429, 'eval_runtime': 48.6385, 'eval_samples_per_second': 18.997, 'eval_steps_per_second': 1.192, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.043, 'grad_norm': 0.37518537044525146, 'learning_rate': 9.919678714859438e-06, 'epoch': 2.01}\n",
            "{'loss': 0.0283, 'grad_norm': 0.3259151577949524, 'learning_rate': 7.911646586345381e-06, 'epoch': 2.21}\n",
            "{'loss': 0.0339, 'grad_norm': 0.32918938994407654, 'learning_rate': 5.9036144578313255e-06, 'epoch': 2.41}\n",
            "{'loss': 0.0318, 'grad_norm': 0.11548250168561935, 'learning_rate': 3.895582329317269e-06, 'epoch': 2.61}\n",
            "{'loss': 0.0326, 'grad_norm': 0.7116721868515015, 'learning_rate': 1.887550200803213e-06, 'epoch': 2.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0667a66909b4aa98b9e5d24cae31391",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1441417932510376, 'eval_precision': 0.8406320541760722, 'eval_recall': 0.8152364273204904, 'eval_f1': 0.8277394976661481, 'eval_accuracy': 0.9709553528564571, 'eval_runtime': 47.2043, 'eval_samples_per_second': 19.574, 'eval_steps_per_second': 1.229, 'epoch': 3.0}\n",
            "{'train_runtime': 3590.3367, 'train_samples_per_second': 3.319, 'train_steps_per_second': 0.208, 'train_loss': 0.07206859151362735, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diaab\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92271f0d3c6e4f6780725131b8a18071",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.1441417932510376,\n",
              " 'eval_precision': 0.8406320541760722,\n",
              " 'eval_recall': 0.8152364273204904,\n",
              " 'eval_f1': 0.8277394976661481,\n",
              " 'eval_accuracy': 0.9709553528564571,\n",
              " 'eval_runtime': 47.0213,\n",
              " 'eval_samples_per_second': 19.651,\n",
              " 'eval_steps_per_second': 1.233,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seqeval_metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    pred_ids = np.argmax(logits, axis=2)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for pred_row, label_row in zip(pred_ids, labels):\n",
        "        row_true = []\n",
        "        row_pred = []\n",
        "        for pred_id, label_id in zip(pred_row, label_row):\n",
        "            if label_id == -100:\n",
        "                continue\n",
        "            row_true.append(id2label[int(label_id)])\n",
        "            row_pred.append(id2label[int(pred_id)])\n",
        "        y_true.append(row_true)\n",
        "        y_pred.append(row_pred)\n",
        "\n",
        "    scores = seqeval_metric.compute(predictions=y_pred, references=y_true)\n",
        "    return {\n",
        "        \"precision\": scores[\"overall_precision\"],\n",
        "        \"recall\": scores[\"overall_recall\"],\n",
        "        \"f1\": scores[\"overall_f1\"],\n",
        "        \"accuracy\": scores[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(ROOT / \"models\" / \"anercorp_baseline_runs\"),\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=50,\n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Evaluate with seqeval\n",
        "\n",
        "We print per-entity precision/recall/F1, overall F1, and the full classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9762fea2829b432d9a278252bb8aad09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Precision: 0.8406\n",
            "Overall Recall:    0.8152\n",
            "Overall F1:        0.8277\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.8881    0.9275    0.9074       676\n",
            "        MISC     0.6798    0.5679    0.6188       243\n",
            "         ORG     0.7596    0.7364    0.7478       459\n",
            "         PER     0.8815    0.8377    0.8591       906\n",
            "\n",
            "   micro avg     0.8406    0.8152    0.8277      2284\n",
            "   macro avg     0.8022    0.7674    0.7833      2284\n",
            "weighted avg     0.8375    0.8152    0.8255      2284\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.881533</td>\n",
              "      <td>0.837748</td>\n",
              "      <td>0.859083</td>\n",
              "      <td>906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.759551</td>\n",
              "      <td>0.736383</td>\n",
              "      <td>0.747788</td>\n",
              "      <td>459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>0.888102</td>\n",
              "      <td>0.927515</td>\n",
              "      <td>0.907381</td>\n",
              "      <td>676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.679803</td>\n",
              "      <td>0.567901</td>\n",
              "      <td>0.618834</td>\n",
              "      <td>243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  entity  precision    recall        f1  support\n",
              "0    PER   0.881533  0.837748  0.859083      906\n",
              "1    ORG   0.759551  0.736383  0.747788      459\n",
              "2    LOC   0.888102  0.927515  0.907381      676\n",
              "3   MISC   0.679803  0.567901  0.618834      243"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred_output = trainer.predict(tokenized_datasets[\"test\"])\n",
        "pred_ids = np.argmax(pred_output.predictions, axis=2)\n",
        "label_ids = pred_output.label_ids\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for pred_row, label_row in zip(pred_ids, label_ids):\n",
        "    row_true = []\n",
        "    row_pred = []\n",
        "    for p, l in zip(pred_row, label_row):\n",
        "        if l == -100:\n",
        "            continue\n",
        "        row_true.append(id2label[int(l)])\n",
        "        row_pred.append(id2label[int(p)])\n",
        "    y_true.append(row_true)\n",
        "    y_pred.append(row_pred)\n",
        "\n",
        "overall_p = precision_score(y_true, y_pred)\n",
        "overall_r = recall_score(y_true, y_pred)\n",
        "overall_f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Overall Precision: {overall_p:.4f}\")\n",
        "print(f\"Overall Recall:    {overall_r:.4f}\")\n",
        "print(f\"Overall F1:        {overall_f1:.4f}\")\n",
        "print()\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "report_dict = classification_report(y_true, y_pred, output_dict=True, digits=4)\n",
        "per_entity_rows = []\n",
        "for entity in [\"PER\", \"ORG\", \"LOC\", \"MISC\"]:\n",
        "    if entity in report_dict:\n",
        "        per_entity_rows.append({\n",
        "            \"entity\": entity,\n",
        "            \"precision\": report_dict[entity][\"precision\"],\n",
        "            \"recall\": report_dict[entity][\"recall\"],\n",
        "            \"f1\": report_dict[entity][\"f1-score\"],\n",
        "            \"support\": report_dict[entity][\"support\"],\n",
        "        })\n",
        "\n",
        "if per_entity_rows:\n",
        "    display(pd.DataFrame(per_entity_rows))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Save trained model\n",
        "\n",
        "This saves the sanity-check baseline model for quick reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved baseline model to: C:\\Users\\diaab\\islamic-ner\\models\\anercorp_baseline\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "trainer.save_model(str(OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "\n",
        "print(f\"Saved baseline model to: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What results tell us and next adaptation step\n",
        "\n",
        "If the setup is healthy, AraBERT on ANERCorp is commonly around **80-90 F1**.\n",
        "\n",
        "If lower than expected, check:\n",
        "- parsing quality,\n",
        "- label alignment,\n",
        "- train/test split consistency.\n",
        "\n",
        "Next we adapt this pipeline for Islamic NER by replacing labels/data and adding weak supervision from gazetteers + rules.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
