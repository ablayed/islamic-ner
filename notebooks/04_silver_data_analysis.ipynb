{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Silver Data Analysis (Data Quality Report)\n",
    "\n",
    "This notebook audits the generated silver NER dataset and produces a data quality report suitable for portfolio documentation.\n",
    "\n",
    "It covers:\n",
    "- split loading and summary,\n",
    "- label distribution and co-occurrence,\n",
    "- entity length behavior,\n",
    "- source-level differences (Sanadset vs hadith-json),\n",
    "- gazetteer coverage and long-tail analysis,\n",
    "- class imbalance flags and mitigation suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\diaab\\islamic-ner\n",
      "Silver dir: C:\\Users\\diaab\\islamic-ner\\data\\silver\n",
      "Splits:\n",
      "  - train: C:\\Users\\diaab\\islamic-ner\\data\\silver\\train.json\n",
      "  - dev: C:\\Users\\diaab\\islamic-ner\\data\\silver\\dev.json\n",
      "  - test_held_out: C:\\Users\\diaab\\islamic-ner\\data\\silver\\test_held_out.json\n",
      "MAX_RECORDS_PER_SPLIT = None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / 'data').exists() and (candidate / 'notebooks').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_project_root(Path.cwd().resolve())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.preprocessing.normalize import ArabicNormalizer\n",
    "\n",
    "SILVER_DIR = ROOT / 'data' / 'silver'\n",
    "SPLIT_PATHS = {\n",
    "    'train': SILVER_DIR / 'train.json',\n",
    "    'dev': SILVER_DIR / 'dev.json',\n",
    "    'test_held_out': SILVER_DIR / 'test_held_out.json',\n",
    "}\n",
    "GAZETTEER_PATHS = {\n",
    "    'SCHOLAR': ROOT / 'data' / 'gazetteers' / 'scholars.txt',\n",
    "    'BOOK': ROOT / 'data' / 'gazetteers' / 'books.txt',\n",
    "    'CONCEPT': ROOT / 'data' / 'gazetteers' / 'concepts.txt',\n",
    "    'PLACE': ROOT / 'data' / 'gazetteers' / 'places.txt',\n",
    "}\n",
    "ENTITY_TYPES = ['SCHOLAR', 'BOOK', 'CONCEPT', 'PLACE', 'HADITH_REF']\n",
    "\n",
    "# Set to an int (e.g. 50000) for fast exploratory runs; keep None for full analysis.\n",
    "MAX_RECORDS_PER_SPLIT = None\n",
    "PROGRESS_EVERY = 50_000\n",
    "\n",
    "normalizer = ArabicNormalizer()\n",
    "\n",
    "for split_name, path in SPLIT_PATHS.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Missing split file: {path}')\n",
    "\n",
    "print('Project root:', ROOT)\n",
    "print('Silver dir:', SILVER_DIR)\n",
    "print('Splits:')\n",
    "for split_name, path in SPLIT_PATHS.items():\n",
    "    print(f'  - {split_name}: {path}')\n",
    "print('MAX_RECORDS_PER_SPLIT =', MAX_RECORDS_PER_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Silver Splits\n",
    "\n",
    "This section streams `train/dev/test_held_out` and computes all downstream statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train -> train.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36miter_json_array\u001b[39m\u001b[34m(path, chunk_size)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     obj, end = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diaab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_name, split_path \u001b[38;5;129;01min\u001b[39;00m SPLIT_PATHS.items():\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mProcessing split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_json_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mMAX_RECORDS_PER_SPLIT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_RECORDS_PER_SPLIT\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36miter_json_array\u001b[39m\u001b[34m(path, chunk_size)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     chunk = \u001b[43mhandle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:319\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def iter_json_array(path: Path, chunk_size: int = 1 << 16):\n",
    "    \"\"\"Yield objects from a top-level JSON array (or JSONL fallback) without loading full file.\"\"\"\n",
    "    decoder = json.JSONDecoder()\n",
    "\n",
    "    with path.open('r', encoding='utf-8') as handle:\n",
    "        buffer = ''\n",
    "\n",
    "        # Prime buffer until we have a non-whitespace character.\n",
    "        while True:\n",
    "            chunk = handle.read(chunk_size)\n",
    "            if not chunk:\n",
    "                return\n",
    "            buffer += chunk\n",
    "            pos0 = 0\n",
    "            while pos0 < len(buffer) and buffer[pos0].isspace():\n",
    "                pos0 += 1\n",
    "            if pos0 < len(buffer):\n",
    "                break\n",
    "\n",
    "        # Fallback: support JSONL files by parsing one JSON object per non-empty line.\n",
    "        if buffer[pos0] != '[':\n",
    "            handle.seek(0)\n",
    "            for line_no, line in enumerate(handle, start=1):\n",
    "                text = line.strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                try:\n",
    "                    yield decoder.decode(text)\n",
    "                except json.JSONDecodeError as exc:\n",
    "                    raise ValueError(f'Invalid JSONL record at {path}:{line_no}: {exc}') from exc\n",
    "            return\n",
    "\n",
    "        pos = pos0 + 1\n",
    "\n",
    "        while True:\n",
    "            # Skip separators between array elements.\n",
    "            while True:\n",
    "                if pos >= len(buffer):\n",
    "                    chunk = handle.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        return\n",
    "                    buffer += chunk\n",
    "                    continue\n",
    "\n",
    "                ch = buffer[pos]\n",
    "                if ch.isspace() or ch == ',':\n",
    "                    pos += 1\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            if ch == ']':\n",
    "                return\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    obj, end = decoder.raw_decode(buffer, pos)\n",
    "                    yield obj\n",
    "                    pos = end\n",
    "\n",
    "                    # Trim consumed buffer periodically.\n",
    "                    if pos > 1_000_000:\n",
    "                        buffer = buffer[pos:]\n",
    "                        pos = 0\n",
    "                    break\n",
    "                except json.JSONDecodeError as exc:\n",
    "                    chunk = handle.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        context = buffer[max(0, pos - 30): min(len(buffer), pos + 60)]\n",
    "                        raise ValueError(\n",
    "                            f'Malformed JSON in {path} near char {pos}: {exc.msg}. Context: {context!r}'\n",
    "                        ) from exc\n",
    "                    buffer += chunk\n",
    "\n",
    "\n",
    "def extract_entities(tokens: List[str], tags: List[str]) -> List[Dict]:\n",
    "    entities = []\n",
    "    n = min(len(tokens), len(tags))\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        label = tags[i]\n",
    "        if label == 'O' or '-' not in label:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        prefix, entity_type = label.split('-', 1)\n",
    "        if prefix not in {'B', 'I'}:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Treat malformed starting I-* as entity start for robustness.\n",
    "        start = i\n",
    "        i += 1\n",
    "        while i < n and tags[i] == f'I-{entity_type}':\n",
    "            i += 1\n",
    "\n",
    "        end = i\n",
    "        entities.append(\n",
    "            {\n",
    "                'type': entity_type,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'length_tokens': end - start,\n",
    "                'text': ' '.join(tokens[start:end]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def load_gazetteer_entries(path: Path) -> List[Dict]:\n",
    "    entries = []\n",
    "    if not path.exists():\n",
    "        return entries\n",
    "\n",
    "    for raw in path.read_text(encoding='utf-8').splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        variants = [v.strip() for v in line.split('|') if v.strip()]\n",
    "        if not variants:\n",
    "            continue\n",
    "\n",
    "        variants_norm = {normalizer.normalize(v) for v in variants if normalizer.normalize(v)}\n",
    "        if not variants_norm:\n",
    "            continue\n",
    "\n",
    "        entries.append(\n",
    "            {\n",
    "                'canonical': variants[0],\n",
    "                'variants': variants,\n",
    "                'variants_norm': variants_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "split_sentence_counts = Counter()\n",
    "source_sentence_counts = Counter()\n",
    "source_entity_counts = defaultdict(Counter)\n",
    "source_entity_sum = Counter()\n",
    "source_entity_n = Counter()\n",
    "\n",
    "token_label_counts = Counter()\n",
    "entity_count_by_type = Counter({entity_type: 0 for entity_type in ENTITY_TYPES})\n",
    "\n",
    "cooccurrence = Counter()\n",
    "\n",
    "entity_length_sum = Counter()\n",
    "entity_length_n = Counter()\n",
    "entity_length_dist = defaultdict(Counter)\n",
    "\n",
    "observed_entities_by_type = defaultdict(set)\n",
    "entity_text_counter_by_type = defaultdict(Counter)\n",
    "representative_surface = defaultdict(dict)\n",
    "\n",
    "for split_name, split_path in SPLIT_PATHS.items():\n",
    "    print(f'Processing split: {split_name} -> {split_path.name}')\n",
    "\n",
    "    for i, record in enumerate(iter_json_array(split_path), start=1):\n",
    "        if MAX_RECORDS_PER_SPLIT is not None and i > MAX_RECORDS_PER_SPLIT:\n",
    "            break\n",
    "\n",
    "        tokens = record.get('tokens') or []\n",
    "        tags = record.get('ner_tags') or []\n",
    "        if not isinstance(tokens, list) or not isinstance(tags, list):\n",
    "            continue\n",
    "\n",
    "        n = min(len(tokens), len(tags))\n",
    "        tokens = tokens[:n]\n",
    "        tags = tags[:n]\n",
    "\n",
    "        source = str(record.get('source', 'unknown'))\n",
    "\n",
    "        split_sentence_counts[split_name] += 1\n",
    "        source_sentence_counts[source] += 1\n",
    "        token_label_counts.update(tags)\n",
    "\n",
    "        entities = extract_entities(tokens, tags)\n",
    "        source_entity_sum[source] += len(entities)\n",
    "        source_entity_n[source] += 1\n",
    "\n",
    "        sentence_types = sorted({e['type'] for e in entities if e['type'] in ENTITY_TYPES})\n",
    "        for entity_type in sentence_types:\n",
    "            cooccurrence[(entity_type, entity_type)] += 1\n",
    "        for left, right in combinations(sentence_types, 2):\n",
    "            cooccurrence[(left, right)] += 1\n",
    "            cooccurrence[(right, left)] += 1\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_type = entity['type']\n",
    "            if entity_type not in ENTITY_TYPES:\n",
    "                continue\n",
    "\n",
    "            entity_count_by_type[entity_type] += 1\n",
    "            source_entity_counts[source][entity_type] += 1\n",
    "\n",
    "            length_tokens = int(entity['length_tokens'])\n",
    "            entity_length_sum[entity_type] += length_tokens\n",
    "            entity_length_n[entity_type] += 1\n",
    "            entity_length_dist[entity_type][length_tokens] += 1\n",
    "\n",
    "            normalized_entity_text = normalizer.normalize(entity['text'])\n",
    "            if not normalized_entity_text:\n",
    "                continue\n",
    "\n",
    "            observed_entities_by_type[entity_type].add(normalized_entity_text)\n",
    "            entity_text_counter_by_type[entity_type][normalized_entity_text] += 1\n",
    "            representative_surface[entity_type].setdefault(normalized_entity_text, entity['text'])\n",
    "\n",
    "        if i % PROGRESS_EVERY == 0:\n",
    "            print(f'  {split_name}: processed {i:,} records...')\n",
    "\n",
    "\n",
    "entity_counts_series = pd.Series({entity_type: entity_count_by_type[entity_type] for entity_type in ENTITY_TYPES})\n",
    "\n",
    "split_counts_df = pd.DataFrame(\n",
    "    {'split': list(split_sentence_counts.keys()), 'sentences': list(split_sentence_counts.values())}\n",
    ").sort_values('split')\n",
    "\n",
    "total_tokens = int(sum(token_label_counts.values()))\n",
    "o_tokens = int(token_label_counts.get('O', 0))\n",
    "entity_tokens = int(total_tokens - o_tokens)\n",
    "\n",
    "token_balance_df = pd.DataFrame(\n",
    "    [\n",
    "        {'group': 'O', 'tokens': o_tokens, 'pct': (o_tokens / total_tokens * 100) if total_tokens else 0.0},\n",
    "        {'group': 'Entity', 'tokens': entity_tokens, 'pct': (entity_tokens / total_tokens * 100) if total_tokens else 0.0},\n",
    "    ]\n",
    ")\n",
    "\n",
    "label_distribution_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'label': label,\n",
    "            'count': count,\n",
    "            'pct': (count / total_tokens * 100) if total_tokens else 0.0,\n",
    "        }\n",
    "        for label, count in token_label_counts.items()\n",
    "    ]\n",
    ").sort_values('count', ascending=False)\n",
    "\n",
    "cooccur_df = pd.DataFrame(0, index=ENTITY_TYPES, columns=ENTITY_TYPES, dtype=int)\n",
    "for (left, right), value in cooccurrence.items():\n",
    "    if left in cooccur_df.index and right in cooccur_df.columns:\n",
    "        cooccur_df.loc[left, right] = int(value)\n",
    "\n",
    "source_entity_df = pd.DataFrame(index=sorted(source_sentence_counts.keys()), columns=ENTITY_TYPES).fillna(0)\n",
    "for source in source_entity_df.index:\n",
    "    for entity_type in ENTITY_TYPES:\n",
    "        source_entity_df.loc[source, entity_type] = int(source_entity_counts[source][entity_type])\n",
    "source_entity_df = source_entity_df.astype(int)\n",
    "\n",
    "source_summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'source': source,\n",
    "            'sentences': int(source_sentence_counts[source]),\n",
    "            'entities_total': int(source_entity_sum[source]),\n",
    "            'avg_entities_per_sentence': (\n",
    "                source_entity_sum[source] / source_entity_n[source] if source_entity_n[source] else 0.0\n",
    "            ),\n",
    "        }\n",
    "        for source in sorted(source_sentence_counts.keys())\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_length_avg_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'entity_type': entity_type,\n",
    "            'avg_length_tokens': (\n",
    "                entity_length_sum[entity_type] / entity_length_n[entity_type] if entity_length_n[entity_type] else 0.0\n",
    "            ),\n",
    "            'entity_count': int(entity_length_n[entity_type]),\n",
    "        }\n",
    "        for entity_type in ENTITY_TYPES\n",
    "    ]\n",
    ")\n",
    "\n",
    "gazetteer_entries = {\n",
    "    entity_type: load_gazetteer_entries(path)\n",
    "    for entity_type, path in GAZETTEER_PATHS.items()\n",
    "}\n",
    "\n",
    "gazetteer_coverage_rows = []\n",
    "for entity_type in ['SCHOLAR', 'BOOK', 'CONCEPT', 'PLACE']:\n",
    "    entries = gazetteer_entries.get(entity_type, [])\n",
    "    observed = observed_entities_by_type[entity_type]\n",
    "\n",
    "    covered = 0\n",
    "    for entry in entries:\n",
    "        if entry['variants_norm'] & observed:\n",
    "            covered += 1\n",
    "\n",
    "    total_entries = len(entries)\n",
    "    gazetteer_coverage_rows.append(\n",
    "        {\n",
    "            'entity_type': entity_type,\n",
    "            'gazetteer_entries': total_entries,\n",
    "            'covered_entries': covered,\n",
    "            'coverage_pct': (covered / total_entries * 100) if total_entries else 0.0,\n",
    "            'unique_entities_observed_in_data': len(observed),\n",
    "        }\n",
    "    )\n",
    "\n",
    "gazetteer_coverage_df = pd.DataFrame(gazetteer_coverage_rows)\n",
    "\n",
    "top20_by_type = {}\n",
    "singleton_rows = []\n",
    "for entity_type in ENTITY_TYPES:\n",
    "    counter = entity_text_counter_by_type[entity_type]\n",
    "    representative = representative_surface[entity_type]\n",
    "\n",
    "    top_rows = []\n",
    "    for rank, (normalized_text, count) in enumerate(counter.most_common(20), start=1):\n",
    "        top_rows.append(\n",
    "            {\n",
    "                'rank': rank,\n",
    "                'entity_normalized': normalized_text,\n",
    "                'example_surface': representative.get(normalized_text, normalized_text),\n",
    "                'count': int(count),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    top20_by_type[entity_type] = pd.DataFrame(top_rows)\n",
    "\n",
    "    unique_entities = len(counter)\n",
    "    singleton_count = sum(1 for value in counter.values() if value == 1)\n",
    "    singleton_rows.append(\n",
    "        {\n",
    "            'entity_type': entity_type,\n",
    "            'unique_entities': unique_entities,\n",
    "            'singletons': singleton_count,\n",
    "            'singleton_pct': (singleton_count / unique_entities * 100) if unique_entities else 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "singleton_df = pd.DataFrame(singleton_rows)\n",
    "\n",
    "total_entities = int(sum(entity_counts_series.values))\n",
    "entity_share_df = pd.DataFrame(\n",
    "    {\n",
    "        'entity_type': ENTITY_TYPES,\n",
    "        'count': [int(entity_counts_series[entity_type]) for entity_type in ENTITY_TYPES],\n",
    "    }\n",
    ")\n",
    "entity_share_df['pct'] = np.where(total_entities > 0, entity_share_df['count'] / total_entities * 100, 0.0)\n",
    "\n",
    "scholar_share = float(entity_share_df.loc[entity_share_df['entity_type'] == 'SCHOLAR', 'pct'].iloc[0])\n",
    "book_share = float(entity_share_df.loc[entity_share_df['entity_type'] == 'BOOK', 'pct'].iloc[0])\n",
    "imbalance_flag = scholar_share >= 80.0 and book_share <= 2.0\n",
    "\n",
    "print()\n",
    "print('Completed streaming analysis.')\n",
    "print(f'Total sentences analyzed: {sum(split_sentence_counts.values()):,}')\n",
    "print(f'Total tokens analyzed: {total_tokens:,}')\n",
    "print(f'Total entities analyzed: {total_entities:,}')\n",
    "\n",
    "display(split_counts_df)\n",
    "display(source_summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Label Distribution Analysis\n",
    "\n",
    "- Entity type frequencies\n",
    "- `O` vs entity token share\n",
    "- Entity type co-occurrence matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(entity_share_df.sort_values('count', ascending=False))\n",
    "display(token_balance_df)\n",
    "display(label_distribution_df.head(20))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "entity_share_df.set_index('entity_type')['count'].reindex(ENTITY_TYPES).plot(kind='bar', ax=ax, color='#2E86AB')\n",
    "ax.set_title('Entity Type Frequencies (Span Counts)')\n",
    "ax.set_xlabel('Entity type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.bar(token_balance_df['group'], token_balance_df['pct'], color=['#9E9E9E', '#3F8EFC'])\n",
    "ax.set_title('Token Share: O vs Entity')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.set_ylim(0, 100)\n",
    "for i, value in enumerate(token_balance_df['pct']):\n",
    "    ax.text(i, value + 1, f'{value:.2f}%', ha='center', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "display(cooccur_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(cooccur_df.values, cmap='YlGnBu')\n",
    "ax.set_xticks(range(len(ENTITY_TYPES)))\n",
    "ax.set_xticklabels(ENTITY_TYPES, rotation=45, ha='right')\n",
    "ax.set_yticks(range(len(ENTITY_TYPES)))\n",
    "ax.set_yticklabels(ENTITY_TYPES)\n",
    "ax.set_title('Entity Type Co-occurrence (Sentence-level)')\n",
    "\n",
    "for i in range(len(ENTITY_TYPES)):\n",
    "    for j in range(len(ENTITY_TYPES)):\n",
    "        ax.text(j, i, str(int(cooccur_df.iloc[i, j])), ha='center', va='center', fontsize=8)\n",
    "\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Entity Length Analysis\n",
    "\n",
    "- Average token length per type\n",
    "- Distribution of lengths per entity type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(entity_length_avg_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "entity_length_avg_df.set_index('entity_type')['avg_length_tokens'].reindex(ENTITY_TYPES).plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    color='#E07A5F',\n",
    ")\n",
    "ax.set_title('Average Entity Length by Type (tokens)')\n",
    "ax.set_xlabel('Entity type')\n",
    "ax.set_ylabel('Average length')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def bucket_lengths(counter: Counter, max_bucket: int = 8) -> Dict[str, int]:\n",
    "    buckets = Counter()\n",
    "    for length, count in counter.items():\n",
    "        key = str(length) if length <= max_bucket else f'{max_bucket + 1}+'\n",
    "        buckets[key] += count\n",
    "    return dict(buckets)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, entity_type in enumerate(ENTITY_TYPES):\n",
    "    ax = axes[idx]\n",
    "    buckets = bucket_lengths(entity_length_dist[entity_type], max_bucket=8)\n",
    "\n",
    "    labels = [str(i) for i in range(1, 9)] + ['9+']\n",
    "    values = [buckets.get(label, 0) for label in labels]\n",
    "\n",
    "    ax.bar(labels, values, color='#81B29A')\n",
    "    ax.set_title(entity_type)\n",
    "    ax.set_xlabel('Entity length (tokens)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Entity Length Distributions by Type', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Source Comparison\n",
    "\n",
    "Compare entity profiles between `sanadset` and `hadith_json` sourced sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(source_summary_df)\n",
    "display(source_entity_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "source_entity_df.reindex(columns=ENTITY_TYPES).plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('Entity Counts by Source')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Entity count')\n",
    "ax.legend(title='Entity type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "source_entity_rate_df = source_entity_df.div(source_entity_df.sum(axis=1), axis=0).fillna(0) * 100\n",
    "display(source_entity_rate_df)\n",
    "\n",
    "if {'sanadset', 'hadith_json'}.issubset(set(source_entity_df.index)):\n",
    "    sanadset_row = source_entity_df.loc['sanadset']\n",
    "    hadith_row = source_entity_df.loc['hadith_json']\n",
    "\n",
    "    sanadset_scholar_per_sentence = sanadset_row['SCHOLAR'] / max(source_summary_df.loc[source_summary_df['source'] == 'sanadset', 'sentences'].iloc[0], 1)\n",
    "    hadith_scholar_per_sentence = hadith_row['SCHOLAR'] / max(source_summary_df.loc[source_summary_df['source'] == 'hadith_json', 'sentences'].iloc[0], 1)\n",
    "\n",
    "    print('SCHOLAR entities per sentence:')\n",
    "    print(f\"  sanadset: {sanadset_scholar_per_sentence:.3f}\")\n",
    "    print(f\"  hadith_json: {hadith_scholar_per_sentence:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Coverage Analysis\n",
    "\n",
    "- Gazetteer coverage in actual silver entities\n",
    "- Top 20 entities per type\n",
    "- Singleton analysis (entities seen once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gazetteer_coverage_df)\n",
    "display(singleton_df)\n",
    "\n",
    "for entity_type in ENTITY_TYPES:\n",
    "    print()\n",
    "    print('=' * 80)\n",
    "    print(f'Top 20 entities: {entity_type}')\n",
    "    if top20_by_type[entity_type].empty:\n",
    "        print('No entities observed.')\n",
    "    else:\n",
    "        display(top20_by_type[entity_type])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Potential Issues and Mitigation\n",
    "\n",
    "Flags imbalance risks and proposes practical mitigation options for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(entity_share_df)\n",
    "\n",
    "print('Class imbalance checks:')\n",
    "print(f\"- SCHOLAR share: {scholar_share:.2f}%\")\n",
    "print(f\"- BOOK share: {book_share:.2f}%\")\n",
    "print(f\"- Imbalance flag (SCHOLAR >= 80% and BOOK <= 2%): {imbalance_flag}\")\n",
    "\n",
    "rare_type_rows = entity_share_df[entity_share_df['pct'] < 5.0].copy()\n",
    "if rare_type_rows.empty:\n",
    "    print()\n",
    "    print('No entity type under 5%.')\n",
    "else:\n",
    "    print()\n",
    "    print('Entity types under 5% of all entities:')\n",
    "    display(rare_type_rows.sort_values('pct'))\n",
    "\n",
    "print()\n",
    "print('Suggested mitigation actions:')\n",
    "mitigations = []\n",
    "\n",
    "if imbalance_flag:\n",
    "    mitigations.append('Oversample sentences containing BOOK entities during training batches.')\n",
    "    mitigations.append('Use class-weighted loss or focal loss to reduce SCHOLAR dominance.')\n",
    "    mitigations.append('Track per-class F1 and accept lower BOOK F1 only with explicit reporting.')\n",
    "\n",
    "if not rare_type_rows.empty:\n",
    "    mitigations.append('Create targeted mini-corpora for low-frequency classes (BOOK/PLACE/HADITH_REF).')\n",
    "\n",
    "if singleton_df['singleton_pct'].max() > 60:\n",
    "    mitigations.append('High singleton tail detected: prefer subword-aware models and contextual augmentation.')\n",
    "\n",
    "if not mitigations:\n",
    "    mitigations.append('Current class balance is acceptable; proceed with standard stratified training.')\n",
    "\n",
    "for idx, item in enumerate(mitigations, start=1):\n",
    "    print(f'{idx}. {item}')\n",
    "\n",
    "print()\n",
    "print('Data quality report cell completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
