{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Sanadset Exploration\n",
    "\n",
    "This notebook explores the Sanadset tagged hadith corpus.\n",
    "\n",
    "It covers:\n",
    "- dataset loading and schema checks,\n",
    "- `<NAR>` narrator extraction and frequency analysis,\n",
    "- `<SANAD>` vs `<MATN>` split statistics,\n",
    "- quality checks for malformed tags and gaps,\n",
    "- fuzzy coverage comparison against `hadith-json` (`bukhari.json`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / 'data').exists() and (candidate / 'notebooks').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_project_root(Path.cwd().resolve())\n",
    "DATA_DIR = ROOT / 'data' / 'raw'\n",
    "SANADSET_PATH = DATA_DIR / 'sanadset' / 'sanadset.csv'\n",
    "BUKHARI_PATH = DATA_DIR / 'hadith_json' / 'bukhari.json'\n",
    "SANADSET_NROWS = 50_000\n",
    "\n",
    "NAR_PATTERN = re.compile(r'<NAR>(.*?)</NAR>', re.IGNORECASE | re.DOTALL)\n",
    "SANAD_PATTERN = re.compile(r'<SANAD>(.*?)</SANAD>', re.IGNORECASE | re.DOTALL)\n",
    "MATN_PATTERN = re.compile(r'<MATN>(.*?)</MATN>', re.IGNORECASE | re.DOTALL)\n",
    "NAR_TOKEN_PATTERN = re.compile(r'</?NAR>', re.IGNORECASE)\n",
    "TAG_STRIP_PATTERN = re.compile(r'</?(?:NAR|SANAD|MATN)>', re.IGNORECASE)\n",
    "\n",
    "ARABIC_DIACRITICS = re.compile(r'[ؗ-ًؚ-ْٰـ]')\n",
    "NON_WORD_PATTERN = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "\n",
    "\n",
    "SUMMARY = {\n",
    "    'rows_loaded': 0,\n",
    "    'text_column': '',\n",
    "    'id_column': '',\n",
    "    'nar_hit_rate': 0.0,\n",
    "    'total_narrator_mentions': 0,\n",
    "    'unique_narrators': 0,\n",
    "    'top_narrator': '',\n",
    "    'top_narrator_frequency': 0,\n",
    "    'avg_sanad_words': 0.0,\n",
    "    'avg_matn_words': 0.0,\n",
    "    'pct_with_both_tags': 0.0,\n",
    "    'both_tags_count': 0,\n",
    "    'malformed_nar_rows': 0,\n",
    "    'nested_nar_rows': 0,\n",
    "    'long_narrator_names_count': 0,\n",
    "    'zero_narrator_rows': 0,\n",
    "    'zero_narrator_pct': 0.0,\n",
    "    'bukhari_sample_size': 0,\n",
    "    'bukhari_coverage_threshold': 0.0,\n",
    "    'bukhari_coverage_count': 0,\n",
    "    'bukhari_coverage_pct': 0.0,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def strip_known_tags(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    stripped = TAG_STRIP_PATTERN.sub(' ', text)\n",
    "    stripped = re.sub(r'<[^>]+>', ' ', stripped)\n",
    "    return normalize_spaces(stripped)\n",
    "\n",
    "\n",
    "def infer_tagged_text_column(df: pd.DataFrame) -> str:\n",
    "    scores = []\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        if not (pd.api.types.is_string_dtype(series) or series.dtype == object):\n",
    "            continue\n",
    "        sample = series.dropna().astype(str).head(500)\n",
    "        if sample.empty:\n",
    "            continue\n",
    "        hit_rate = sample.str.contains(r'</?(?:NAR|SANAD|MATN)>', regex=True).mean()\n",
    "        if hit_rate > 0:\n",
    "            scores.append((hit_rate, col))\n",
    "\n",
    "    if scores:\n",
    "        scores.sort(reverse=True)\n",
    "        return scores[0][1]\n",
    "\n",
    "    preferred = ['tagged_text', 'text', 'hadith_text', 'hadith', 'sentence', 'content']\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for key in preferred:\n",
    "        if key in lower_map:\n",
    "            return lower_map[key]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        if pd.api.types.is_string_dtype(series) or series.dtype == object:\n",
    "            return col\n",
    "\n",
    "    raise ValueError('Could not infer a text column in Sanadset CSV.')\n",
    "\n",
    "\n",
    "def detect_nar_text_column(df: pd.DataFrame, sample_size: int = 3000) -> tuple[str, pd.DataFrame]:\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        if not (pd.api.types.is_string_dtype(series) or series.dtype == object):\n",
    "            continue\n",
    "\n",
    "        sample = series.dropna().astype(str).head(sample_size)\n",
    "        if sample.empty:\n",
    "            continue\n",
    "\n",
    "        nar_hit_rate = sample.str.contains(r'<\\s*NAR\\s*>', regex=True, case=False).mean()\n",
    "        any_tag_hit_rate = sample.str.contains(r'</?(?:NAR|SANAD|MATN)>', regex=True, case=False).mean()\n",
    "        candidates.append(\n",
    "            {\n",
    "                'column': col,\n",
    "                'sample_rows': len(sample),\n",
    "                'nar_hit_rate': float(nar_hit_rate),\n",
    "                'any_tag_hit_rate': float(any_tag_hit_rate),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    candidates_df = pd.DataFrame(candidates)\n",
    "    if not candidates_df.empty:\n",
    "        candidates_df = candidates_df.sort_values(\n",
    "            ['nar_hit_rate', 'any_tag_hit_rate', 'sample_rows'], ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        if candidates_df.iloc[0]['nar_hit_rate'] > 0:\n",
    "            return str(candidates_df.iloc[0]['column']), candidates_df\n",
    "\n",
    "    return infer_tagged_text_column(df), candidates_df\n",
    "\n",
    "\n",
    "def infer_id_column(df: pd.DataFrame) -> str:\n",
    "    preferred = ['id', 'hadith_id', 'hadithid', 'hadith_no', 'index']\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for key in preferred:\n",
    "        if key in lower_map:\n",
    "            return lower_map[key]\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_narrators(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    narrators = []\n",
    "    for match in NAR_PATTERN.findall(text):\n",
    "        candidate = strip_known_tags(match)\n",
    "        if candidate:\n",
    "            narrators.append(candidate)\n",
    "    return narrators\n",
    "\n",
    "\n",
    "def extract_tag_content(text: str, pattern) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    parts = [strip_known_tags(part) for part in pattern.findall(text)]\n",
    "    parts = [part for part in parts if part]\n",
    "    return normalize_spaces(' '.join(parts))\n",
    "\n",
    "\n",
    "def highlight_narrators_in_sanad(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    sanad_parts = SANAD_PATTERN.findall(text)\n",
    "    if not sanad_parts:\n",
    "        return ''\n",
    "    combined = ' '.join(sanad_parts)\n",
    "\n",
    "    def _repl(match):\n",
    "        inner = normalize_spaces(strip_known_tags(match.group(1)))\n",
    "        return f'[[{inner}]]' if inner else ''\n",
    "\n",
    "    highlighted = re.sub(r'<NAR>(.*?)</NAR>', _repl, combined, flags=re.IGNORECASE | re.DOTALL)\n",
    "    highlighted = TAG_STRIP_PATTERN.sub(' ', highlighted)\n",
    "    highlighted = re.sub(r'<[^>]+>', ' ', highlighted)\n",
    "    return normalize_spaces(highlighted)\n",
    "\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0\n",
    "    return len(re.findall(r'\\S+', text))\n",
    "\n",
    "\n",
    "def nar_tag_quality_flags(text: str) -> tuple[bool, bool]:\n",
    "    if not isinstance(text, str):\n",
    "        return False, False\n",
    "    depth = 0\n",
    "    malformed = False\n",
    "    nested = False\n",
    "\n",
    "    for token in NAR_TOKEN_PATTERN.findall(text):\n",
    "        if token.lower() == '<nar>':\n",
    "            if depth > 0:\n",
    "                nested = True\n",
    "            depth += 1\n",
    "        else:\n",
    "            if depth == 0:\n",
    "                malformed = True\n",
    "            else:\n",
    "                depth -= 1\n",
    "\n",
    "    if depth != 0:\n",
    "        malformed = True\n",
    "\n",
    "    return malformed, nested\n",
    "\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    cleaned = strip_known_tags(text)\n",
    "    cleaned = ARABIC_DIACRITICS.sub('', cleaned)\n",
    "    cleaned = cleaned.replace('?', '?').replace('?', '?').replace('?', '?')\n",
    "    cleaned = cleaned.replace('?', '?').replace('?', '?')\n",
    "    cleaned = NON_WORD_PATTERN.sub(' ', cleaned)\n",
    "    return normalize_spaces(cleaned)\n",
    "\n",
    "\n",
    "def shorten(text: str, limit: int = 170) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    if len(text) <= limit:\n",
    "        return text\n",
    "    return text[:limit].rstrip() + '...'\n",
    "\n",
    "\n",
    "if not SANADSET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'{SANADSET_PATH} not found. Run `python scripts/download_sanadset.py` first '\n",
    "        'or manually place the CSV at this path.'\n",
    "    )\n",
    "\n",
    "print('Project root:', ROOT)\n",
    "print('Sanadset path:', SANADSET_PATH)\n",
    "print('Bukhari path:', BUKHARI_PATH)\n",
    "print('Exploration nrows:', SANADSET_NROWS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Load Sanadset CSV and inspect structure\n",
    "\n",
    "This section loads `data/raw/sanadset/sanadset.csv` using `nrows=50000` for fast exploration,\n",
    "then reports:\n",
    "- total loaded row count,\n",
    "- all column names and dtypes,\n",
    "- sampled rows,\n",
    "- auto-detected tagged-text column based on `<NAR>` hit rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['utf-8', 'utf-8-sig', 'cp1256', 'windows-1256']\n",
    "sanad_df = None\n",
    "last_error = None\n",
    "\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        sanad_df = pd.read_csv(SANADSET_PATH, encoding=enc, nrows=SANADSET_NROWS)\n",
    "        print(f'Loaded Sanadset with encoding: {enc} (nrows={SANADSET_NROWS:,})')\n",
    "        break\n",
    "    except UnicodeDecodeError as exc:\n",
    "        last_error = exc\n",
    "\n",
    "if sanad_df is None:\n",
    "    raise RuntimeError(f'Failed to load CSV with tried encodings: {encodings}. Last error: {last_error}')\n",
    "\n",
    "if sanad_df.empty:\n",
    "    raise ValueError('Sanadset CSV loaded but is empty.')\n",
    "\n",
    "sanad_df = sanad_df.reset_index(drop=True)\n",
    "\n",
    "print(f'Total rows loaded for exploration: {len(sanad_df):,}')\n",
    "print()\n",
    "print('All CSV columns:')\n",
    "for idx, col in enumerate(sanad_df.columns, start=1):\n",
    "    print(f'{idx:02d}. {col}')\n",
    "\n",
    "dtype_df = pd.DataFrame({'column': sanad_df.columns, 'dtype': sanad_df.dtypes.astype(str)})\n",
    "display(dtype_df)\n",
    "\n",
    "text_preview_cols = list(sanad_df.columns[: min(6, len(sanad_df.columns))])\n",
    "print()\n",
    "print('Sample rows (first 3, first columns):')\n",
    "display(sanad_df[text_preview_cols].head(3))\n",
    "\n",
    "tagged_col, nar_candidates_df = detect_nar_text_column(sanad_df)\n",
    "id_col = infer_id_column(sanad_df)\n",
    "if not id_col:\n",
    "    sanad_df = sanad_df.reset_index().rename(columns={'index': 'row_id'})\n",
    "    id_col = 'row_id'\n",
    "\n",
    "print()\n",
    "print(f'Tagged text column selected (based on <NAR> hit rate): {tagged_col}')\n",
    "print(f'ID column selected: {id_col}')\n",
    "\n",
    "if nar_candidates_df.empty:\n",
    "    print('No string/object candidates were available for <NAR> detection. Used fallback text-column inference.')\n",
    "    selected_hit_rate = 0.0\n",
    "else:\n",
    "    display(nar_candidates_df.head(12))\n",
    "    hit_row = nar_candidates_df[nar_candidates_df['column'] == tagged_col]\n",
    "    selected_hit_rate = float(hit_row.iloc[0]['nar_hit_rate']) if not hit_row.empty else 0.0\n",
    "    print(f\"Selected column '<{tagged_col}>' <NAR> hit rate: {selected_hit_rate * 100:.2f}%\")\n",
    "\n",
    "nar_rows = sanad_df[sanad_df[tagged_col].astype(str).str.contains(r'<\\s*NAR\\s*>', regex=True, case=False)]\n",
    "if nar_rows.empty:\n",
    "    print(f\"No rows with explicit <NAR> tags found in selected column '{tagged_col}'.\")\n",
    "    display(sanad_df[[id_col, tagged_col]].head(5))\n",
    "else:\n",
    "    print()\n",
    "    print('Sample rows containing <NAR> tags (first 5):')\n",
    "    display(nar_rows[[id_col, tagged_col]].head(5))\n",
    "\n",
    "sample_n = min(10, len(sanad_df))\n",
    "sample_df = sanad_df[[id_col, tagged_col]].sample(sample_n, random_state=42)\n",
    "print()\n",
    "print(f'Random sample ({sample_n} rows) from selected text column:')\n",
    "display(sample_df)\n",
    "\n",
    "SUMMARY['rows_loaded'] = int(len(sanad_df))\n",
    "SUMMARY['text_column'] = tagged_col\n",
    "SUMMARY['id_column'] = id_col\n",
    "SUMMARY['nar_hit_rate'] = float(selected_hit_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Parse `<NAR>` tags and narrator statistics\n",
    "\n",
    "We extract all narrators from `<NAR>...</NAR>` tags, then compute:\n",
    "- unique narrator count,\n",
    "- top 50 most frequent narrators,\n",
    "- histogram of narrators-per-hadith.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanad_df['narrators'] = sanad_df[tagged_col].astype(str).apply(extract_narrators)\n",
    "sanad_df['narrator_count'] = sanad_df['narrators'].apply(len)\n",
    "\n",
    "all_narrators = [name for names in sanad_df['narrators'] for name in names]\n",
    "narrator_freq = Counter(all_narrators)\n",
    "\n",
    "total_mentions = len(all_narrators)\n",
    "unique_narrators = len(narrator_freq)\n",
    "print(f'Total narrator mentions: {total_mentions:,}')\n",
    "print(f'Unique narrators: {unique_narrators:,}')\n",
    "\n",
    "top50_df = pd.DataFrame(narrator_freq.most_common(50), columns=['narrator', 'frequency'])\n",
    "print('Top 50 narrators by frequency:')\n",
    "display(top50_df)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "max_count = int(sanad_df['narrator_count'].max()) if len(sanad_df) else 0\n",
    "bins = range(0, max_count + 2)\n",
    "plt.hist(sanad_df['narrator_count'], bins=bins, edgecolor='black', alpha=0.8)\n",
    "plt.title('Distribution of Narrators per Hadith')\n",
    "plt.xlabel('Narrator count in hadith')\n",
    "plt.ylabel('Number of hadiths')\n",
    "plt.xticks(range(0, min(max_count + 1, 25)))\n",
    "plt.grid(axis='y', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "SUMMARY['total_narrator_mentions'] = int(total_mentions)\n",
    "SUMMARY['unique_narrators'] = int(unique_narrators)\n",
    "if narrator_freq:\n",
    "    top_name, top_freq = narrator_freq.most_common(1)[0]\n",
    "    SUMMARY['top_narrator'] = top_name\n",
    "    SUMMARY['top_narrator_frequency'] = int(top_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Analyze `<SANAD>` vs `<MATN>` split\n",
    "\n",
    "This section computes isnad/matn lengths (word-based), reports coverage of both tags,\n",
    "and shows sample isnad snippets with narrators highlighted as `[[name]]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanad_df['sanad_text'] = sanad_df[tagged_col].astype(str).apply(lambda x: extract_tag_content(x, SANAD_PATTERN))\n",
    "sanad_df['matn_text'] = sanad_df[tagged_col].astype(str).apply(lambda x: extract_tag_content(x, MATN_PATTERN))\n",
    "\n",
    "sanad_df['sanad_words'] = sanad_df['sanad_text'].apply(word_count)\n",
    "sanad_df['matn_words'] = sanad_df['matn_text'].apply(word_count)\n",
    "\n",
    "has_sanad = sanad_df['sanad_text'].str.len() > 0\n",
    "has_matn = sanad_df['matn_text'].str.len() > 0\n",
    "has_both = has_sanad & has_matn\n",
    "\n",
    "avg_isnad_words = sanad_df.loc[has_sanad, 'sanad_words'].mean() if has_sanad.any() else 0.0\n",
    "avg_matn_words = sanad_df.loc[has_matn, 'matn_words'].mean() if has_matn.any() else 0.0\n",
    "both_count = int(has_both.sum())\n",
    "pct_both = (has_both.mean() * 100) if len(sanad_df) else 0.0\n",
    "\n",
    "print(f'Average <SANAD> length (words): {avg_isnad_words:.2f}')\n",
    "print(f'Average <MATN> length (words): {avg_matn_words:.2f}')\n",
    "print(f'Rows with both <SANAD> and <MATN>: {both_count:,}/{len(sanad_df):,} ({pct_both:.2f}%)')\n",
    "\n",
    "sanad_df['sanad_highlighted'] = sanad_df[tagged_col].astype(str).apply(highlight_narrators_in_sanad)\n",
    "isnad_sample = sanad_df.loc[sanad_df['sanad_highlighted'].str.len() > 0, [id_col, 'sanad_highlighted']]\n",
    "\n",
    "if isnad_sample.empty:\n",
    "    print('No <SANAD> sections found for highlighting.')\n",
    "else:\n",
    "    print('Sample highlighted sanad snippets (10 rows max):')\n",
    "    display(isnad_sample.sample(min(10, len(isnad_sample)), random_state=42))\n",
    "\n",
    "SUMMARY['avg_sanad_words'] = float(avg_isnad_words)\n",
    "SUMMARY['avg_matn_words'] = float(avg_matn_words)\n",
    "SUMMARY['pct_with_both_tags'] = float(pct_both)\n",
    "SUMMARY['both_tags_count'] = int(both_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Quality checks\n",
    "\n",
    "Checks performed:\n",
    "- malformed or nested `<NAR>` tags,\n",
    "- narrators with very long names (>6 words),\n",
    "- hadiths with zero narrator tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = sanad_df[tagged_col].astype(str).apply(nar_tag_quality_flags)\n",
    "sanad_df['nar_malformed'] = flags.apply(lambda x: x[0])\n",
    "sanad_df['nar_nested'] = flags.apply(lambda x: x[1])\n",
    "\n",
    "malformed_count = int(sanad_df['nar_malformed'].sum())\n",
    "nested_count = int(sanad_df['nar_nested'].sum())\n",
    "\n",
    "print(f'Rows with malformed <NAR> tags: {malformed_count:,}')\n",
    "print(f'Rows with nested <NAR> tags: {nested_count:,}')\n",
    "\n",
    "issue_df = sanad_df[sanad_df['nar_malformed'] | sanad_df['nar_nested']].copy()\n",
    "if not issue_df.empty:\n",
    "    display(issue_df[[id_col, 'nar_malformed', 'nar_nested', tagged_col]].head(20))\n",
    "\n",
    "long_narrators = []\n",
    "for narrator, freq in narrator_freq.items():\n",
    "    words = word_count(narrator)\n",
    "    if words > 6:\n",
    "        long_narrators.append((narrator, words, freq))\n",
    "\n",
    "long_narrators_df = pd.DataFrame(long_narrators, columns=['narrator', 'word_count', 'frequency'])\n",
    "if long_narrators_df.empty:\n",
    "    print('No narrator names with >6 words were found.')\n",
    "else:\n",
    "    long_narrators_df = long_narrators_df.sort_values(['frequency', 'word_count'], ascending=False)\n",
    "    print(f'Narrators with very long names (>6 words): {len(long_narrators_df):,}')\n",
    "    display(long_narrators_df.head(50))\n",
    "\n",
    "zero_narr_df = sanad_df[sanad_df['narrator_count'] == 0].copy()\n",
    "zero_count = len(zero_narr_df)\n",
    "zero_pct = (zero_count / len(sanad_df) * 100) if len(sanad_df) else 0.0\n",
    "print(f'Rows with 0 narrators tagged: {zero_count:,} ({zero_pct:.2f}%)')\n",
    "if not zero_narr_df.empty:\n",
    "    display(zero_narr_df[[id_col, tagged_col]].head(20))\n",
    "\n",
    "SUMMARY['malformed_nar_rows'] = malformed_count\n",
    "SUMMARY['nested_nar_rows'] = nested_count\n",
    "SUMMARY['long_narrator_names_count'] = int(len(long_narrators_df))\n",
    "SUMMARY['zero_narrator_rows'] = int(zero_count)\n",
    "SUMMARY['zero_narrator_pct'] = float(zero_pct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Compare with hadith-json (`bukhari.json`) using fuzzy matching\n",
    "\n",
    "Process:\n",
    "- sample 100 random hadiths from `bukhari.json`,\n",
    "- normalize text and compare against Sanadset hadith text,\n",
    "- estimate coverage using a fuzzy score threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hadith_records(payload):\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    if not isinstance(payload, dict):\n",
    "        return []\n",
    "\n",
    "    candidates = []\n",
    "    for key in ['hadiths', 'data', 'items', 'results']:\n",
    "        value = payload.get(key)\n",
    "        if isinstance(value, list):\n",
    "            candidates.append(value)\n",
    "        elif isinstance(value, dict):\n",
    "            nested = value.get('hadiths')\n",
    "            if isinstance(nested, list):\n",
    "                candidates.append(nested)\n",
    "\n",
    "    if candidates:\n",
    "        return max(candidates, key=len)\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_hadith_text(record):\n",
    "    text_keys = ['hadithArabic', 'arabic', 'arab', 'text', 'hadith', 'matn']\n",
    "    for key in text_keys:\n",
    "        value = record.get(key)\n",
    "        if isinstance(value, str) and value.strip():\n",
    "            return value.strip()\n",
    "\n",
    "    for value in record.values():\n",
    "        if isinstance(value, str) and len(value.split()) >= 5:\n",
    "            return value.strip()\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_hadith_id(record, fallback):\n",
    "    id_keys = ['hadithnumber', 'hadithNumber', 'id', 'arabicnumber', 'number']\n",
    "    for key in id_keys:\n",
    "        value = record.get(key)\n",
    "        if isinstance(value, (str, int)):\n",
    "            return str(value)\n",
    "    return str(fallback)\n",
    "\n",
    "\n",
    "if not BUKHARI_PATH.exists():\n",
    "    print(f'{BUKHARI_PATH} not found. Run `python scripts/download_data.py` and rerun this section.')\n",
    "else:\n",
    "    with BUKHARI_PATH.open('r', encoding='utf-8') as handle:\n",
    "        bukhari_payload = json.load(handle)\n",
    "\n",
    "    raw_records = extract_hadith_records(bukhari_payload)\n",
    "    bukhari_rows = []\n",
    "    for idx, rec in enumerate(raw_records):\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "        text = extract_hadith_text(rec)\n",
    "        if not text:\n",
    "            continue\n",
    "        bukhari_rows.append({'bukhari_id': extract_hadith_id(rec, idx), 'text': text})\n",
    "\n",
    "    bukhari_df = pd.DataFrame(bukhari_rows)\n",
    "    if bukhari_df.empty:\n",
    "        print('Could not extract hadith texts from bukhari.json schema.')\n",
    "    else:\n",
    "        sample_size = min(100, len(bukhari_df))\n",
    "        bukhari_sample = bukhari_df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f'Sampled {sample_size} hadiths from bukhari.json')\n",
    "\n",
    "        sanad_plain = sanad_df[tagged_col].astype(str).apply(strip_known_tags)\n",
    "        sanad_norm = sanad_plain.apply(normalize_for_match)\n",
    "        sanad_lengths = sanad_norm.apply(len)\n",
    "\n",
    "        token_index = defaultdict(set)\n",
    "        for i, text in enumerate(sanad_norm):\n",
    "            tokens = {tok for tok in text.split() if len(tok) >= 4}\n",
    "            for tok in tokens:\n",
    "                token_index[tok].add(i)\n",
    "\n",
    "        def candidate_pool(query_norm):\n",
    "            tokens = sorted({tok for tok in query_norm.split() if len(tok) >= 4}, key=len, reverse=True)\n",
    "            candidates = set()\n",
    "            for tok in tokens[:12]:\n",
    "                candidates.update(token_index.get(tok, set()))\n",
    "                if len(candidates) > 4000:\n",
    "                    break\n",
    "            return candidates\n",
    "\n",
    "        threshold = 0.60\n",
    "        results = []\n",
    "\n",
    "        for _, row in bukhari_sample.iterrows():\n",
    "            query_raw = row['text']\n",
    "            query_norm = normalize_for_match(query_raw)\n",
    "            if not query_norm:\n",
    "                continue\n",
    "\n",
    "            candidates = candidate_pool(query_norm)\n",
    "            if not candidates:\n",
    "                continue\n",
    "\n",
    "            if len(candidates) > 1200:\n",
    "                candidates = sorted(candidates, key=lambda i: abs(sanad_lengths.iloc[i] - len(query_norm)))[:1200]\n",
    "\n",
    "            best_idx = None\n",
    "            best_score = 0.0\n",
    "            for cand_idx in candidates:\n",
    "                score = SequenceMatcher(None, query_norm, sanad_norm.iloc[cand_idx]).ratio()\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = cand_idx\n",
    "\n",
    "            if best_idx is None:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    'bukhari_id': row['bukhari_id'],\n",
    "                    'score': round(best_score, 4),\n",
    "                    'matched_sanadset_id': sanad_df.iloc[best_idx][id_col],\n",
    "                    'bukhari_text_sample': shorten(query_raw),\n",
    "                    'sanadset_text_sample': shorten(sanad_plain.iloc[best_idx]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if not results:\n",
    "            print('No fuzzy matches were produced. Check text normalization or data availability.')\n",
    "        else:\n",
    "            matches_df = pd.DataFrame(results).sort_values('score', ascending=False)\n",
    "            covered = int((matches_df['score'] >= threshold).sum())\n",
    "            coverage_pct = (covered / sample_size) * 100 if sample_size else 0.0\n",
    "            print(f'Coverage at threshold >= {threshold:.2f}: {covered}/{sample_size} ({coverage_pct:.2f}%)')\n",
    "            display(matches_df.head(25))\n",
    "\n",
    "            SUMMARY['bukhari_sample_size'] = int(sample_size)\n",
    "            SUMMARY['bukhari_coverage_threshold'] = float(threshold)\n",
    "            SUMMARY['bukhari_coverage_count'] = int(covered)\n",
    "            SUMMARY['bukhari_coverage_pct'] = float(coverage_pct)\n",
    "\n",
    "print()\n",
    "print('=' * 84)\n",
    "print('FINAL SUMMARY - SANADSET EXPLORATION (nrows=50,000)')\n",
    "print('=' * 84)\n",
    "print(f\"Rows loaded: {SUMMARY['rows_loaded']:,}\")\n",
    "print(f\"Detected text column with <NAR>: {SUMMARY['text_column']} (hit rate: {SUMMARY['nar_hit_rate'] * 100:.2f}%)\")\n",
    "print(f\"ID column: {SUMMARY['id_column']}\")\n",
    "print('-' * 84)\n",
    "print(f\"Narrator mentions: {SUMMARY['total_narrator_mentions']:,}\")\n",
    "print(f\"Unique narrators: {SUMMARY['unique_narrators']:,}\")\n",
    "if SUMMARY['top_narrator']:\n",
    "    print(f\"Top narrator: {SUMMARY['top_narrator']} ({SUMMARY['top_narrator_frequency']:,})\")\n",
    "print('-' * 84)\n",
    "print(f\"Average <SANAD> length (words): {SUMMARY['avg_sanad_words']:.2f}\")\n",
    "print(f\"Average <MATN> length (words): {SUMMARY['avg_matn_words']:.2f}\")\n",
    "print(\n",
    "    f\"Rows with both <SANAD> and <MATN>: {SUMMARY['both_tags_count']:,}/{SUMMARY['rows_loaded']:,} \"\n",
    "    f\"({SUMMARY['pct_with_both_tags']:.2f}%)\"\n",
    ")\n",
    "print('-' * 84)\n",
    "print(f\"Malformed <NAR> rows: {SUMMARY['malformed_nar_rows']:,}\")\n",
    "print(f\"Nested <NAR> rows: {SUMMARY['nested_nar_rows']:,}\")\n",
    "print(f\"Long narrator names (>6 words): {SUMMARY['long_narrator_names_count']:,}\")\n",
    "print(f\"Zero-narrator rows: {SUMMARY['zero_narrator_rows']:,} ({SUMMARY['zero_narrator_pct']:.2f}%)\")\n",
    "print('-' * 84)\n",
    "if SUMMARY['bukhari_sample_size'] > 0:\n",
    "    print(\n",
    "        f\"Bukhari fuzzy coverage @ >= {SUMMARY['bukhari_coverage_threshold']:.2f}: \"\n",
    "        f\"{SUMMARY['bukhari_coverage_count']}/{SUMMARY['bukhari_sample_size']} \"\n",
    "        f\"({SUMMARY['bukhari_coverage_pct']:.2f}%)\"\n",
    "    )\n",
    "else:\n",
    "    print('Bukhari fuzzy coverage: not computed (missing file or no extracted matches).')\n",
    "print('=' * 84)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
